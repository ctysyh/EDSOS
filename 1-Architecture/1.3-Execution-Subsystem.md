# **1.3 Execution-Subsystem**

## **1.3.1 设计愿景与核心原则**
- 执行单元的重新定义：从“线程”到“TS 节点”
- 核心设计哲学
  - 无隐式共享
  - 结构决定可见性
  - 调度器驱动
  - 去中心化与局部自组织
  - 硬件协同与创造性利用
- 与传统执行模型的本质区别

---

## **1.3.2 Tree-Stacked（TS）进程模型**

### **1.3.2.1 TS 作为统一执行原语**

在 EDSOS 中，“进程”不再是一个扁平的地址空间加一个执行流的集合，而是一棵 **动态演化的 Tree-Stacked（TS）结构**。这棵 TS 是进程在运行时的完整结构化表达，其拓扑编码了控制流、数据流、作用域、生命周期与局部性等全部执行语义。

#### **1.3.2.1.1 进程即 TS：从抽象到实例**

每个 EDSOS 进程实例在创建时，由 EDSOS Loader 构造一棵初始 TS。这棵 TS 的根节点具有特殊语义：它承载了传统操作系统中进程控制块（PCB）的全部元信息以及 EDSOS 特有的补充元信息，包括：

- 进程 ID；
- Capability 授权集合（可访问的文件路径前缀、服务访问权限等）；
- 调度策略；
- 其他。

除根节点外，TS 中的所有其他节点均为**普通执行单元**，无全局 ID，仅通过父子指针在树中定位。这种设计消除了传统进程模型中“线程 ID”“栈指针”“堆段”等离散概念，将它们统一为 TS 节点的结构属性。实际上，这样的组织形式类似于 Linux 中的 VMA 结构，但是含义内容、拓扑结构、可执行操作等均截然不同。

#### **1.3.2.1.2 节点：执行与内存的统一体**

TS 节点是 EDSOS 中最小的可调度、可执行、可共享的原子单元。每个节点在虚拟地址空间中占据**连续的若干虚拟页（VPs）**，包含三个逻辑段：

- **元数据段**：节点类型、大小、祖先路径哈希等（由 EDSOS Loader 生成）；
- **指令段**：一段符合 EDSOS Standard ABI 的代码流；
- **数据段**：局部变量、栈帧、或逃逸数据的存储空间。

节点的语义取决于其内容与上下文：

- 若包含有效指令段，则可被调度执行，行为类似**协程或函数调用帧**；
- 若仅含数据段（无主要逻辑），则退化为**生命周期逃逸的堆内存块**；
- 若为根节点，则兼具**进程元数据容器**与**Capability 策略锚点**功能。

这种统一性使得 EDSOS 能在**同一抽象下支持从微秒级函数调用到退化模式的大型线程**的全谱系执行模式。

#### **1.3.2.1.3 节点状态机和节点操作**

节点使用 5 态状态机：

| 状态 | 含义 |
|------|--------|
| `ready` | 就绪等待调度 |
| `running` | 正在执行 |
| `block` | 阻塞 |
| `zombie` | 临终 |
| `error` | 错误 |

其中，10 种操作可以改变节点状态或节点内容：

| 操作 | 触发者 | 目标状态 | 是否需中间状态 | 并行影响 | 关键协同 |
|------|--------|----------|----------------|----------|----------|
| `push` | 父节点 / 编译器桩 | → ready | 否 | 无（叶子创建） | 内存子系统（VP 分配） |
| `active` | 调度器 | ready → running | 否 | 核心级调度 | TLB 预热、祖链重建 |
| `wait` | 节点自身 / 资源管理器 | running → block | 否 | 无 | 事件通知机制 |
| `lift` | 节点自身 / 协议 | 结构变更 | 存在但无需显式标识 | 需 pause 子孙 | DPU、祖链更新 |
| `merge` | 内存管理器 | 结构变更 | 存在但无需显式标识 | 需 pause 自身 | 虚拟重映射 |
| `divide` | 内存管理器 | 结构变更 | 存在但无需显式标识 | 需 pause 自身 | 数据迁移 |
| `pause` | 调度器 / 中断 | running → ready | 否 | 核心级 | TLB 刷新、上下文保存 |
| `finish` | 节点自身 / 回调 | → zombie | 否 | 无 | 子树清理 |
| `pop` | 父节点 | zombie → 回收 | 否 | 无 | 引用计数 |
| `warn` | 安全监控 / Fault Handler | → error | 否 | 全局终止 | 安全审计 |

**所有操作均由 EDSOS Scheduler 执行**，详见 §1.3.3 节点调度器架构。

#### **1.3.2.1.4 TS 的构建：EDSOS Loader 的角色**

一棵 TS 的初始结构在进程加载阶段由 **EDSOS Loader** 构建，过程如下：

- **解析 ELF 与 Manifest**：
  - 提取入口函数、依赖库、Capability 声明；
  - 检查 `syscall` 调用，匹配合法的 syscall 桩函数内联展开为对应的 x64 指令，遇到非法调用按策略拒绝加载（默认）或退化替换（兼容模式下开启弱安全选项）。

- **分配虚拟地址与元数据**：
  - 请求内存子系统为每个节点分配连续 GVA；
  - 生成 `.edsos_node_meta`，记录 `vbase`、`size`、`ancestor_path_hint`。

- **搭建初始树形拓扑**：  
  - 创建根节点，注入 Capability 与系统资源视图；
  - 将主入口函数作为根的直接子节点；
  - 静态依赖（如全局变量、init 函数）按作用域关系组织为子树。

最终，Loader 输出一棵**结构完整、作用域明确、可直接调度**的 TS，作为进程的运行时起点。

#### **1.3.2.1.5 与传统进程模型的映射**

| 传统概念 | EDSOS 对应 | 说明 |
|--------|-----------|------|
| 进程地址空间 | 整棵 TS 的 GVA 范围 | 作用域受结构限制 |
| 线程 | 可并行调度的 TS 节点 | 支持父子并行 |
| 栈帧 | 正常节点的数据段 | 大小受 TLB 约束 |
| 堆内存 | 无指令段/弱指令段的退化节点的数据段 | 生命周期由引用计数管理 |
| PCB | TS 根节点 | 含 Capability 与调度元数据 |

值得注意的是，EDSOS **在用户 TS 中不进入内核态**。系统服务（如文件、网络）同样以 TS 形式存在，通过结构共享与事件同步协作。TS 中可能包含一个 **KPN（Kernel Proxy Node）** 作为与系统服务通信的代理，详见 §1.3.7 系统调用模型。

---

### **1.3.2.2 Meta-TS：整个系统的结构化表示**

EDSOS 摒弃了传统操作系统中“内核管理进程”的主从模型，转而采用一种 **全结构化、去中心化的宇宙观**：整个系统——包括用户应用、文件服务、网络协议栈、设备驱动乃至安全策略模块——共同构成一棵动态演化的 **Meta-TS（Meta-Tree-Stacked）**。

Meta-TS 并非物理存在的单一数据结构，而是 **所有 TS 实例在逻辑上的并集**。每个 TS 子树代表一个自治的执行上下文，彼此通过显式结构操作（如 CTRN 共享、事件同步）协作，而非依赖中心化的内核仲裁。

#### **1.3.2.2.1 所有服务即 TS：对等性与模块化**

在 EDSOS 中，不直接存在特权层级的“内核进程”。取而代之的是：

- **用户 TS**：如 `/user_a/chrome_instance_01`，代表一个浏览器实例；
- **系统服务 TS**：如 `/net/sockets`、`/fs/ext4_main`、`/auth/cap_validator`，提供网络与文件 I/O 服务乃至 Capability 审批；
- **驱动 TS**：如 `/drivers/net/e1000`、`/drivers/storage/nvme_ctrl`，直接操作硬件。

这些 TS 在结构上完全对等：
- 均由节点构成，遵循相同的 TS 公理；
- 均可被调度器调度；
- 均通过 CTRN 和 EDSOS Event 与其他 TS 通信。

差异仅体现在 **Capability 授权** 与 **硬件访问权限** 上，而非执行模型本身。这种设计天然支持**微内核的安全边界**，同时通过结构共享避免了传统微内核的 IPC 性能瓶颈。

#### **1.3.2.2.2 狭义内核：作为“硅中介层”**

传统意义上的“内核”（内存映射、TLB 管理、上下文切换、中断处理）在 EDSOS 中 **不表现为任何 TS 节点**，而是以 **“硅中介层”（Silicon Interposer）** 的形式存在：

- 它是裸金属之上的最小运行时；
- 提供调度器、中断与错误响应、内存子系统中的分配和映射等基础原语；
- 所有 TS 通过 **加载期 ABI 内联** 直连该中介层；
- 通信“布线”通过 **CTRN、链式消息总线、EDSOS Event** 实现。

这种设计消除了“陷入内核”的概念：系统调用要么由调度器本地处理（隐式的安全特权切换），要么通过异步委托到服务 TS（不在当前 TS 范围内执行）。**内核不再是执行实体，而是协作基础设施**。

#### **1.3.2.2.3 进程生命周期：TS 根节点的创建与销毁**

在 EDSOS 中，“创建一个进程”等价于 **构造一棵新的 TS 并将其根节点注入 Meta-TS**。

- **创建流程**：
  1. 父 TS 调用 `edsos_spawn(manifest_gva)`；
  2. 调度器触发 EDSOS Loader，解析 ELF 与 Capability 清单；
  3. Loader 构建初始 TS 结构；
  4. 新 TS 的根节点被 `push` 为父节点的子节点（或全局命名空间下的独立子树）；
  5. 根节点被标记为 `ready`，等待调度。

- **销毁流程**：
  1. 用户调用 `exit()` 或异常终止；
  2. 调度器递归 `finish` 整棵 TS 子树；
  3. 所有子孙节点进入 `zombie` 状态；
  4. 父节点（或全局回收器）执行 `pop`，释放内存；
  5. 引用计数归零后，CTRN 自动回收，无内存泄漏。

整个过程 **无需全局进程表**，生命周期完全由 TS 结构与引用计数隐式维护。

#### **1.3.2.2.4 进程标识与命名：根节点即身份**

EDSOS 使用 64 位整数 PID 作为进程的全局唯一标识。PID 保存在根节点中，可通过全局注册表注册人类可读名（如 `/user/chrome_instance_01`）。

TS 运行的兼容模式和 Capability 授权标识在加载期写入根节点。

根节点一旦创建即不可修改，直到 TS 的生命周期结束。

#### **1.3.2.2.5 IPC 的结构化实现：CTRN 作为联结**

进程间通信（IPC）在 EDSOS 中不是“消息传递”，而是通过 **CTRN（Cross TS Referring Node）** 显式联结。这实质上是结构化的共享内存，通过作用域和引用计数确保安全性。

CTRN 是一个退化 TS 节点，在每个引用的 TS 中各自遵守其内的结构约束；CTRN 不会因为被两个 TS 同时引用而造成两个 TS 之间的边界破坏，即其中任一 TS 不可访问 CTRN 在另一 TS 中的直系祖节点。

CTRN **只负责数据共享**，**不负责并发控制**。各个 TS 同时读写的竞态由 EDSOS Event 或其他机制手动解决。

工作流程如下：
- **创建**：Node A `push(CTRN_X)` → CTRN_X 成为 A 的子节点。
- **注册**：CTRN_X 注册到全局命名表（DHT）。
- **lift**：A `lift(CTRN_X)` → CTRN_X 成为 A 的父节点 → A 可访问。
- **跨 TS 连接**：
   - Node B `lookup("CTRN_X")` → 获得节点起始 VA。
   - B `push(CTRN_X_VA, 'as_ctrn')` → `lift(CTRN_X)` → CTRN_X 成为 B 的祖先。
- **断开**：`pop` → refcnt--，refcnt=0 时释放。

---

## **1.3.3 节点调度器架构**

### **1.3.3.1 Per-Core 分布式调度器**

EDSOS 调度器（Scheduler）并非传统意义上的“内核线程”或“系统进程”，而是一套 **分布于每个 CPU 核心、由内联指令段与本地数据结构共同构成的运行时契约**。它没有全局实例，不占用任何 TS 节点，也不表现为可被调度的执行实体；其存在完全体现为 **每个 Core 上一组协同工作的软硬件机制**。

#### **1.3.3.1.1 调度器的本质：内联指令 + 本地数据结构**

EDSOS Scheduler 由两部分组成：

- **内联指令段（Trampoline Stubs）**：  
  在进程加载阶段，EDSOS Loader 将调度器接口（如 `edsos_wait`、`edsos_push`、`edsos_yield`）**静态内联展开为只读机器指令**，嵌入用户代码的函数入口/出口处。这些 stub 直接操作当前 Core 的调度器数据结构，**不由进程代码显式syscall特权切换（ring0/ring3）**，也**无法被用户代码篡改或绕过**。

- **Per-Core 数据结构**：  
  每个 CPU Core 维护一组独立的调度状态，包括：
  - **Ready Forest**：就绪节点的 MPSC 队列；
  - **Wait Graph**：事件名到阻塞节点列表的映射；
  - **PCID 与 TLB 上下文缓存**；
  - **负载统计与迁移状态**（如 `ready_queue_length`、`subtree_home_core`）。

调度器的“执行”即这些 stub 对本地数据结构的原子操作，其行为由硬件内存模型与调度器设计共同保证一致性。

#### **1.3.3.1.2 完全去中心化：无全局协调**

EDSOS 调度器严格遵循 **Per-Core 独立原则**：

- 不存在全局就绪队列、全局调度器线程或中心仲裁者；
- 所有调度决策（除跨核负载均衡外）均在本地完成；
- 节点创建（`push`）、阻塞（`wait`）、唤醒（`signal`）等操作仅影响当前 Core 的数据结构；
- 跨核协作通过 **链式消息总线** 异步完成，不阻塞本地调度。

这种设计使得系统天然具备 **水平扩展能力**：增加 CPU Core 即线性提升调度吞吐，无锁竞争瓶颈。

#### **1.3.3.1.3 核心职责：调度器做什么**

EDSOS Scheduler 的职责被严格限定在 **执行流控制与结构维护** 范围内，具体包括：

- **就绪结构维护**：  
  管理 `Ready Linked List`，支持 FIFO、紧急插入、依赖感知插入等多种策略，并实现调度决策前置，入队时即确定顺序，调度执行仅为 O(1) 切换；
- **事件驱动同步**：  
  维护 `Wait Graph`，实现 `wait(event)` 与 `signal(event)` 的 O(1) 阻塞/唤醒；
- **节点激活与上下文切换**：  
  执行 `active` 操作：重建祖先链、预热 TLB、切换 CR3、跳转指令入口；
- **结构操作执行**：  
  原子执行 `push`/`pop`/`lift`/`merge`/`divide` 等节点操作，确保 TS 结构良定义；
- **链式消息总线端点**：  
  作为消息总线的一个节点，收发跨 Core 事件（如负载 steal 请求、跨核 signal）；
- **硬件状态协同**：  
  管理 PCID 分配、TLB 刷新、RDTSC 时间戳采样等，隐藏硬件复杂性。

值得注意的是，**调度器不包含任何业务逻辑**：它不解析文件路径、不验证 Capability、不管理全局资源。这些职责被委托给对应的系统服务 TS（如 VFS、Auth），通过 KPN 异步协作完成（见 §1.3.7 系统调用模型）。

#### **1.3.3.1.4 调度器与 TS 的关系**

调度器与 TS 是 **正交但紧密协作** 的两个维度：

- TS 定义了“**什么可以运行**”（作用域、结构、生命周期）；
- 调度器决定了“**何时运行、在哪运行**”（执行顺序、Core 亲和、TLB 上下文）。

调度器通过遍历 TS 的父子指针重建祖先链，通过 TLB 预热确保作用域访问安全，通过 `pause`/`active` 保障结构变更的原子可见性。**TS 是语义，调度器是执行引擎**。

---

### **1.3.3.2 调度触发机制**

EDSOS 调度器的执行由两类正交机制触发：**协作式调度（Cooperative Scheduling）** 与 **抢占式调度（Preemptive Scheduling）**。二者均通过 **内联指令段** 实现，但触发时机、调用上下文与职责范围截然不同。所有调度路径均**隐式包含链式消息总线的消息处理逻辑**，以维持去中心化系统的稳定性。

#### **1.3.3.2.1 协作式调度：加载器内联的桩函数**

协作式调度由 **EDSOS Loader 在进程加载期静态内联** 到用户代码中，分为两类桩函数：

##### **(1) 节点边界桩（Entry/Exit Trampolines）**

- **触发时机**：仅在节点**开始执行前**（entry）与**结束执行后**（exit）自动调用，用户代码无法显式调用或跳过。
- **功能**：
  - **Entry 桩**（节点即将运行）：
    - 递归预热 TLB：依次 `touch` 当前节点及其祖先链的 hot pages；
    - 设置 CPU 时间配额（通过 TSC deadline）；
    - 恢复寄存器上下文（若为 resume）；
    - **调用 `check()` 处理链式消息总线的入站消息**。
  - **Exit 桩**（节点即将结束）：
    - 保存寄存器状态（若需 resume）；
    - 通知调度器当前节点已结束；
    - **触发调度主逻辑**：
      ```asm
      ; 1. 从 Ready Forest 头部弹出 next_node
      ; 2. 验证其祖链合法性
      ; 3. 切换 CR3（PCID）
      ; 4. jmp next_node.entry
      ```
    - **调用 `check()` 处理链式消息总线的入站消息**。

> 此类桩函数是调度器“执行切换”的核心路径，确保每次上下文切换都伴随 TLB 预热与消息处理。

##### **(2) 状态操作桩（NOI：常态开放接口）**

- **触发时机**：可在节点执行流中**任意位置显式调用**，如 `edsos_wait()`、`edsos_push()`、`edsos_lift()` 等。
- **本质**：这些是 **Normal Open Interface (NOI)**，构成第一类系统调用（见 §1.3.7 系统调用模型）。
- **功能**：
  - 执行对应节点操作（如将当前节点加入 Wait Graph、分配新子节点等）；
  - 若操作影响结构（如 `lift`），则递归 `pause` 所有受影响子孙；
  - 若涉及跨 PM（如远程 `lift`），则向目标 PM 的**代理子节点**发送结构变更消息；
  - **调用 `check()` 处理链式消息总线的入站消息**。

> NOI 的关键特性是 **零特权切换、O(1) 延迟、原子语义**。它们将调度器暴露为“可编程的运行时服务”，而非黑盒内核。

#### **1.3.3.2.2 抢占式调度：硬件中断驱动**

当协作式调度不足以保证公平性或实时性时，EDSOS 依赖硬件中断强制介入：

- **触发源**：
  - **本地定时器中断**：CPU 时间片耗尽；
  - **IPI（Inter-Processor Interrupt）**：来自其他 Core 的负载 steal 请求、跨核 `signal` 唤醒；
  - **TLB 一致性中断**：远程 TLB 刷新请求；
  - **安全异常中断**：Page Fault 验证失败、元数据损坏等。
  
- **中断处理流程**：
  1. 中断向量表跳转至调度器中断处理 stub；
  2. 保存当前 `running` 节点的完整上下文；
  3. 根据中断类型执行相应操作：
     - **时间片耗尽** → `pause` 当前节点，将其重新加入 Ready Forest 尾部；
     - **IPI 唤醒** → 将指定节点从 Wait Graph 移至 Ready Forest；
     - **安全异常** → `warn` 当前节点，触发子树终止；
  4. **调用 `check()` 处理链式消息总线的入站消息**；
  5. 触发调度主逻辑（同 Exit 桩），切换至下一就绪节点。

> 抢占式路径确保系统在恶意或阻塞节点存在时仍具备**强活性（liveness）与安全性**。

#### **1.3.3.2.3 链式消息总线的协同要求**

所有调度触发路径（无论协作或抢占）均**强制执行 `check()`**，原因在于：

- 链式消息总线采用 **无中心、无确认、SPSC 环形缓冲区** 设计；
- 消息可靠性依赖于**接收方稳定、频繁地消费消息**；
- 若某 Core 长时间不调用 `check()`，其入站队列将溢出，导致系统失活。

因此，EDSOS 将消息处理**深度耦合到调度路径**中，确保即使在高负载下，跨核协调（如负载均衡、事件唤醒）仍能及时完成。

--- 

### **1.3.3.3 就绪结构：Distributed Ready Forest**

EDSOS 调度器的核心就绪管理结构称为 **Distributed Ready Forest**，它并非单一数据结构，而是 **所有 CPU Core 上独立维护的就绪链表（Ready Linked List）的逻辑并集**。每个 Core 的就绪链表完全自治，支持灵活的插入策略与结构亲和优化，共同构成一个去中心化、高可扩展的任务分发网络。

#### **1.3.3.3.1 Per-Core Ready Linked List**

每个 CPU Core 维护一个 **Ready Linked List**。由于同一 CPU Core 天然串行，节点的插入和弹出无需锁或原子操作。

##### **(1) 链表节点**
- 每个链表项为指向 TS 节点的指针（LVA）；
- 此外还可选通过一个附加数据指针（LVA）附加元数据：`priority_hint`、`scheduling_policy` 等。

##### **(2) 多策略插入位置**
调度器支持三类插入语义，由调用上下文或元数据决定：

- **尾部插入（Tail Insertion）**：  
  默认策略，保证 FIFO 公平性。

- **头部插入（Head Insertion）**：  
  用于高优先级系统服务节点（如中断回调、驱动完成处理），确保低延迟响应。

- **智能顺位插入（Smart Positioning）**：  
  由以下信息驱动：
  - **编译器扩展属性**：如 `__edsos_depends_on(node_X)` 声明依赖链；
  - **Loader 解析的启发式元数据**：如热点函数、I/O 绑定提示；
  - **运行时优化器反馈**：基于历史执行间隔、缓存命中率的 ML 模型输出。  
  调度器根据提示信息将节点插入到**依赖节点之后、同类亲和节点附近**的位置，提升 TLB 与缓存重用率。

##### **(3) “基地 Core”绑定的隐式性**
- 新节点由当前 Core 的调度器 `push` 创建，**自动加入当前 Core 的 Ready Linked List**；
- 由此整个子树的生长都在当前 Core 执行，具有良好的局部性；
- **无需显式绑定操作**，迁移（migration）才是显式行为（见 §1.3.8 负载均衡）。

#### **1.3.3.3.2 Per-Core Wait Graph**

Wait Graph 是调度器维护的 **事件到阻塞节点的映射结构**，用于实现 `wait(event)` / `signal(event)` 同步原语。其设计兼顾性能、内存效率与命名空间隔离。
Wait Graph 包含三个 Per-Core 组件：Block Slot Table、Local Event Name Hash Map、Global Event Name Hash Map。

##### **(1) 阻塞节点槽位表（Block Slot Table）**：

  - 类型：动态数组 `BlockSlot slots[MAX_SLOTS]`，初始分配 `N₀` 个槽位（如 64）；
  - 每个 `BlockSlot` 包含：
    - `Node* blocked_nodes[MAX_NODES_PER_SLOT]`：FIFO 队列，初始长度 `M₀`（如 8）；
    - `uint32_t count`：当前阻塞节点数；
    - `bool is_global`：标记该槽位是否对应全局事件（用于区分命名空间）。
  - 槽位复用与生命周期：
    - 槽位（ID）可复用，生命周期由引用计数管理。
  - 动态伸缩策略：
    - 若某槽位 `count == MAX_NODES_PER_SLOT`，则分配新缓冲区（如 2× 扩容），迁移指针；
    - 若连续 `T` 调度周期内 `count < M₀/4`，则缩容；
    - 若所有槽位使用率 > 90%，则 `MAX_SLOTS` 阶段式扩展（如 +64）；
    - 若长期空闲槽位 > 50%，则收缩 `MAX_SLOTS`。

##### **(2) 本地事件名映射表（Local Event Name Hash Map）**：

  - 类型：哈希表 `local_event_name → local_event_id`；
    - `local_event_name`：进程内事件名（如 `"mutex_lock_0x1234"`）；
    - `local_event_id`：分配自 **本地 ID 空间**（如 0 ~ N₀−1）；
    - 名称在当前 CPU Core 唯一，无需全局注册；
  - 初始化：映射在首次 `wait()` 时创建。
  - 生命周期：随首个 `wait()` 创建，随最后一个阻塞者离开且无 pending signal 时回收。

##### **(3) 全局事件名映射表（Global Event Name Hash Map）**：

  - 类型：哈希表 `global_event_name → local_event_id`；
  - `global_event_name`：全局唯一名（如 `"io_complete_disk0_blk123"`）；
  - `local_event_id`：分配自 **全局 ID 映射空间**（与本地 ID 不重合，如高位设 1）；
  - 初始化：首次 `lookup()` 时查询全局事件注册表（DHT），成功后缓存映射。
  - 生命周期：随首个 `wait()` 创建，随最后一个阻塞者离开且无 pending signal 时回收。

> 两个映射表的 `local_event_id` 值域互斥（如本地 ID < 2³¹，全局 ID ≥ 2³¹），确保无冲突。

---

### **1.3.3.4 链式消息总线**

链式消息总线是 EDSOS 节点调度器实现 **去中心化跨核协调** 的核心通信基础设施，无锁、无中心，依赖局部结构与定期消费运行。

#### **1.3.3.4.1 链式可扩展的拓扑结构**

链式消息总线在逻辑上是一张 **有向图**，实现为每个节点维护一组 **数量等于直连通道数** 的 SPSC（Single-Producer Single-Consumer）无锁队列作为消息缓冲区，节点自己作为消费者、**被直连的上一节点** 作为生产者。

每个节点持有数量等同于直连通道数的 **第 i 个方向的下一节点消息缓冲区** 引用，作为 *直连* 的实现。当发送消息时，**直接写入下一个节点的消息缓冲区**。

#### **1.3.3.4.2 消息的数据结构**

消息有固定格式的消息头和动态的消息负载；发送消息时，写入缓冲区的数据是 **完整的消息头结构体**，其中包含包含标签、传播方向、TTL 等内容，以及对应的消息负载指针和长度。

#### **1.3.3.4.3 总线调用**

每个节点必须频繁调用 `check()` 函数以及时处理自己的缓冲区中的消息。这个函数内会遍历自己的所有缓冲区，根据消息标签调用回调表来处理，并根据 TTL 决定是将消息继续传播到下一个节点还是释放，传播方向规定了具体应该传播给哪个节点。

---

## **1.3.4 节点操作与生命周期**

### **1.3.4.1 原子操作语义**

#### **1.3.4.1.1 `push`：原子创建，无中间态**

- **语义**：创建新叶子节点，挂载为当前节点的子节点。
- **触发者**：当前 running 节点（如函数调用、线程 spawn）。
- **原子性保证**：
  - 内存子系统必须 **一次性分配所有 VPs**；
  - 若 VP 不足 → 返回错误，不创建部分节点；
  - 跨机 push：目标 PM 完成 VP 分配 + 祖先链验证后，才返回成功。
- **并行性**：多个核心可并发 push 不同父节点的子节点，无冲突。
- **无需中间态**。

#### **1.3.4.1.2 `active`：调度器驱动的上下文激活**

- **语义**：将 ready 节点调度到 CPU 核心执行。
- **关键点**：
  - **不限于叶子节点**：父子可并行（如协程 + 主线程）；
  - **调度器责任**：
    1. 选择 ready 节点；
    2. 重建其祖先链（通过父指针遍历）；
    3. **增量 TLB 预热**（L1: 自身；L2: 祖先）；
    4. 加载寄存器上下文；
    5. 切换到 running。
- **失败处理**：
  - TLB 预热失败（罕见）→ 标记 error，尝试下一节点；
  - 无 ready 节点 → 进入 idle。
- **并行模型**：同片/跨片核心调度逻辑一致，仅 TLB 预热延迟不同。

#### **1.3.4.1.3 `wait`：阻塞与事件驱动唤醒**

- **语义**：节点主动放弃 CPU，等待某事件（I/O 完成、锁释放等）。
- **实现机制**：
  - 节点调用 `edsos_wait(event_id)`；
  - EDSOS 将节点加入 **event_id 的等待队列**；
  - 状态转为 block；
  - 当事件发生，**事件管理器**遍历队列，将节点状态转为 ready。
- **关键要求**：
  - **唤醒必须精准**：避免虚假唤醒（spurious wakeup）；
  - **无竞态**：wait 与 event_signal 需原子协调（通过内存子系统事件总线）。
- **并行性**：多个节点可 wait 同一事件，无冲突。

#### **1.3.4.1.4 `lift`：结构变更，需 pause**

- **语义**：将节点挂载到更高祖先下，改变其作用域。
- **无需中间状态**，但需 **pause 所有子孙节点**。
- **详细流程**：
  1. 节点发起 lift(target_ancestor_gva)；
  2. EDSOS 验证 target_ancestor 在 TS 树中（通过 GVA 遍历）；
  3. **暂停所有子孙节点**（递推调用 `pause`）；
  4. 原子更新父指针；
  5. 子孙节点状态仍为 ready，但下次 active 时将使用新祖链。
- **为什么无需 lift_pending**？
  - 因为 **作用域变更对子孙不可见，直到它们被重新调度**；
  - TLB 是 per-core 的，旧核心的 TLB 仍有效，直到被刷新；
  - **安全由调度器保证**：子孙下次运行时，TLB 预热基于新父链。
- **跨机 lift**：DPU 传输新挂载点，目标 PM 执行相同 pause + 更新；通过代理根节点（缓存本机子树的根节点之上的信息，接收其他 PM 发来的作用到本机子树根节点的直系祖节点链的结构变更消息）和代理子节点（缓存本机父树的从一个节点开始的子树在其他 PM 的信息，接收其他 PM 发来的从子树向上作用到这个节点的结构变更消息）来在每个 PM 中缓存信息，减少频繁跨机访问开销。

#### **1.3.4.1.5 `merge`：扩容操作，需 pause**

- **语义**：将自身与一个“扩展子节点”合并，扩大虚拟地址范围。
- **流程**：
  1. 先 `push` 一个扩展子节点（仅数据段）；
  2. 调用 `merge(child_gva)`；
  3. **pause 自身**（若 running）；
  4. 内存子系统 **虚拟重映射**：将 child 的 VPs 合并到父的地址空间；
  5. 更新 `vbase/size`；
  6. 子节点标记为 zombie；
  7. 自身状态仍为 ready（若原为 ready）或 running（下次调度恢复）。
- **关键**：merge 期间，**自身不能执行**，故需 pause。
- **无需 merge_pending**：因操作在 pause 保护下原子完成。

---

#### **1.3.4.1.6 `divide`：分裂操作，需 pause**

- **语义**：将部分数据分裂为新子节点，用于回收或 Lift。
- **流程**：
  1. **pause 自身**；
  2. 内存子系统分配新 VPs；
  3. 迁移数据；
  4. 创建新子节点（状态 ready）；
  5. 更新自身 `vbase/size`；
  6. 恢复状态。
- **典型场景**：冷数据回收 → divide → lift 到全局缓存池。

---

#### **1.3.4.1.7 `pause`：中断驱动的上下文冻结**

- **语义**：将 running 节点暂停，保存上下文，回到 ready。
- **触发场景**：
  - 定时器中断（时间片到期）；
  - TLB 一致性中断（跨核 TLB 刷新）；
  - 调度器主动抢占；
  - lift/merge/divide 需要暂停。
- **操作**：
  - 保存寄存器；
  - 状态 → ready；
  - **不清空 TLB**（下次 active 可复用，除非祖链变更）。

---

#### **1.3.4.1.8 `finish`：正常退出**

- **语义**：节点执行完毕，进入 zombie。
- **触发**：`return` 或 `edsos_exit()`。
- **行为**：
  - 状态 → zombie；
  - 若有子节点，**递归 finish**（TS 子树语义）；
  - 父节点可 later `pop`。

---

#### **1.3.4.1.9 `pop`：父节点回收子节点**

- **语义**：父节点显式回收已 zombie 的子节点。
- **原子性**：检查子节点状态 == zombie → 释放内存 → 删除父指针。
- **跨机 pop**：发送 DPU unref，目标 PM 回收。

---

#### **1.3.4.1.10 `warn`：安全异常**

- **语义**：强制进入 error 状态，终止子树。
- **触发**：
  - Page Fault 验证失败（访问非法地址）；
  - 元数据 magic 错误；
  - DPU 消息签名无效。
- **行为**：
  - 状态 → error；
  - **递归 error 所有子孙**；
  - 上报安全日志；
  - 父节点可选择 ignore 或 terminate。

---

### 1.3.4.2 安全异常处理：`warn` 与子树终止
<TODO>

---

## **1.3.5 硬件协同执行模型**

### **1.3.5.1 TLB 驱动的作用域访问控制**

在 EDSOS 中，内存安全与作用域隔离并非通过传统的、运行时的软件检查来实现，而是通过精心设计 **PTE（Page Table Entry）的权限位** 与 **MMU TLB 的层次化缓存行为**，将硬件的地址转换机制直接转化为作用域访问控制的执行引擎。其核心在于构建一种特定的 PTE 与 TLB 状态，使得 **合法的访问路径必然命中且通常高效，而非法的访问路径必然触发可被操作系统捕获的异常**。

#### **1.3.5.1.1 页表项（PTE）的二元状态设计**

EDSOS 的页表项被赋予两种截然不同的状态，这种状态直接映射了TS节点的作用域语义：

- **“作用域内”页（In-Scope Pages）**：
  - **权限**：PTE 被设置为 **可读/可写（或可执行）**。
  - **映射**：这些页对应于 **当前正在运行的TS节点自身** 及其 **所有直系祖先节点** 的数据段和指令段。
  - **目的**：允许节点无缝、高效地访问其作用域内的所有数据。

- **“作用域外”页（Out-of-Scope Pages）**：
  - **权限**：PTE 被显式地标记为 **“不存在”（Present = 0）** 或 **“无访问权限”（如 User=0, R/W=0）**。
  - **映射**：这些页对应于系统中**除当前节点及其祖先之外的所有其他内存**，包括兄弟节点、子孙节点、其他TS的节点等。
  - **目的**：任何对这些页的访问尝试，都将 **必然触发 Page Fault 异常**，从而被操作系统内核（Fault Handler）捕获并处理。

> 通过这种二元化的 PTE 状态设计，页表本身即成为作用域边界的 **静态、权威定义**。硬件 MMU 在进行地址转换时，自动遵循此定义。

#### **1.3.5.1.2 TLB 层次结构与作用域语义的精准对齐**

EDSOS 利用现代CPU的 **两级TLB（L1和L2）** 的不同特性，与TS树的层次结构进行精准对齐，以实现性能与安全的统一：

- **L1 TLB：当前节点的“安全缓存”**
  - **容量**：L1 TLB 容量较小（通常仅能容纳数十个条目）。
  - **映射内容**：在节点被调度执行前，其 **自身** 数据和指令所在内存页的所有条目会被 **主动预热** 至 L1 TLB。
  - **效果**：节点访问自身数据（如局部变量、函数代码）时，**100% L1 TLB命中**。这不仅带来了极致的性能（纳秒级访问）；更重要的是由于 L1 TLB 中 **只包含自身页**，任何试图通过 L1 TLB 访问非自身数据的行为在物理上就不可能发生，构成了第一道安全屏障。

- **L2 TLB：祖先链的“作用域缓存”**
  - **容量**：L2 TLB 容量远大于 L1（通常可容纳数百至上千个条目）。
  - **映射内容**：在节点被调度执行前，其 **所有直系祖先节点** 的“作用域内”页的翻译条目会被 **主动预热** 至 L2 TLB。
  - **效果**：当节点访问祖先数据（如全局配置、父上下文）时，会发生 **L1 TLB Miss**，但会紧接着在 **L2 TLB Hit**。整个过程 **不会触发 Page Walk**，保证了访问的合法性与高效性。L2 TLB 的大容量使其通常足以容纳整条祖先链，从而将作用域安全边界自然地扩展到继承链。
  - **路径可靠性**：虽然 L2 TLB Hit 之后硬件会自动将这个 PTE 缓存填入 L1 TLB，导致一个在 L1 TLB 中的节点内部页被挤出，但这个内部页的 PTE 通常也会在 L2 TLB 中缓存，再次访问这个内部页时仍会在 L2 TLB Hit。

#### **1.3.5.1.3 异常路径：Page Fault 作为安全哨兵**

- 当一个访问请求既未命中L1 TLB，也未命中L2 TLB时，硬件MMU将启动 **Page Walk** 过程，遍历页表以查找对应的 PTE：
  - **合法但未预热的冷数据**：
    - 如果该地址属于“作用域内”页（PTE 有效），Page Walk 会成功找到 PTE，并将新条目填入 TLB。这是一个 **容错路径**，虽然会带来性能抖动，但访问是被允许的。
  - **非法的越界访问**：
    - 如果该地址属于“作用域外”页（PTE 被标记为无效），Page Walk 会发现一个无效的 PTE，硬件将立即 **触发 Page Fault 异常**。
    - **OS行为**：EDSOS 的 Fault Handler 会捕获此异常，检查故障地址。由于页表已明确标记了作用域边界，Handler 可以 **瞬间判定此次访问为非法**，并立即对当前 TS 节点（及其子树）执行 `warn` 操作，将其状态置为 `error` 并触发全局终止。这个过程开销极低，因为判断逻辑极其简单。

> 因此，**Page Fault 在 EDSOS 中不再是模糊的“内存错误”，而是清晰、精准的“作用域越界”信号**。

#### **1.3.5.1.4 边界情况与兼容性：节点过大的优雅退化**

上述理想模型依赖于一个关键前提：**单个TS节点的大小 + 其整条祖先链的大小，必须能被 L1 + L2 TLB 的总容量所容纳**。EDSOS 对此有明确的工程约束和优雅的退化策略：

- **约束**：EDSOS 开发者手册会强烈建议原生代码控制单个节点的大小，确保其在绝大多数场景下能满足TLB容量要求；EDSOS 标准的编译器扩展也会遵循这一点。
- **退化场景**：
  - **节点自身过大**：如果单个节点的页数超过了 L1 TLB 容量，其部分页将无法被预热到 L1。访问这些“冷”页时会发生 **L1 Miss + L2 Hit**（如果祖先链不长，L2仍有空间）或 **L1/L2 Miss + Page Walk**。性能会下降，但只要 PTE 状态正确就可以成功访问，**安全性不受影响**。
  - **祖先链过长**：如果祖先链总大小超过了 L2 TLB 容量，部分祖先页无法被预热。访问这些页同样会触发 Page Walk。为了保证 **功能正确性**，这些页的 PTE **必须保持有效**，以避免合法的祖先访问被误判为非法。此时，性能会退化到与传统操作系统相当的水平（依赖 Page Walk 和 DRAM 访问），**失去确定性高性能的优势，但依然保持内存安全**。

> 这种设计确保了 EDSOS 模型的 **鲁棒性**：在满足约束的理想情况下，它能提供硬件级的、确定性的高性能与安全；在其他情况下，它也能优雅地退化，保证功能正确和基本安全。

---

### 1.3.5.2 PCID 作为祖链指纹
<TODO>
- TLB 刷新最小化
- 结构变更的缓存一致性

### **1.3.5.3 调度预热机制**

在 EDSOS 的硬件协同执行模型中，调度器不仅是执行流的仲裁者，更是 TLB 状态的主动管理者。每一次上下文切换都伴随着一次精心编排的 **TLB 预热（TLB Preheating）** 过程，目标是在新节点开始执行前都尽可能达到上一节的理想情况。

#### **1.3.5.3.1 预热目标的确定：基于最小公共祖先（LCA）的增量计算**

调度器并非每次都从零开始遍历整棵 TS 树来确定预热范围，而是通过 **增量比较** 新旧执行节点的祖先链，找出 **最小公共祖先（Lowest Common Ancestor, LCA）**，从而仅对发生变化的部分进行 TLB 状态更新。

- **RNDAC 优化机制**：每个 CPU 核心的调度器维护一个轻量级缓存 **Running Node Direct Ancestors Cache (RNDAC)**。它是一个有序数组，按从 **根到当前运行节点** 的顺序，存储当前正在执行节点的 **所有直系祖先节点指针**（包括自身）。
  - **缓存信息**：当一个节点 `N_old` 被调度出时，调度器的 RNDAC 即缓存了 `N_old` 的完整祖先链。
  - **快速 LCA 查找**：当调度器决定切换到新节点 `N_new` 时，它从 `N_new` 开始向上递归其父节点。对于每个访问到的祖先 `A`，调度器检查 `A` 是否存在于当前的 RNDAC 中。
    - 第一个命中的 `A` 即为 `N_old` 与 `N_new` 的 **LCA**。
    - 由于 RNDAC 是有序的，可以使用哈希表或直接线性扫描（因链通常不长）实现快速查找。(<TODO>分析、研究这里最高效的匹配算法)
  - **增量更新**：一旦找到 LCA，调度器即可：
    - 保留 RNDAC 中从根到 LCA 的部分；
    - 将 `N_new` 从 LCA 的子节点开始，一直到 `N_new` 自身的路径，**替换原有项** 或 **追加到 RNDAC 末尾**。
    - 此过程仅需 O(D) 时间，其中 D 是 `N_new` 到 LCA 的距离。由于调度器会进行亲和性调度，因此 D 通常很小。
  -  通过 RNDAC，调度器能以极低的开销确定预热范围：**需要预热的页面 = (N_new 到 LCA 的路径上的所有节点) + (N_new 自身)**。

> 此外，当直系祖链上发生了节点结构变化或拓扑变化（如一个节点被 lift 到了直系祖链范围内），调度器会主动触发 pause 以使得受影响的节点可以重新构建执行环境，并且会主动修改 RNDAC 的对应部分（因为此时的 RNDAC 正是对应于当前的 Running 节点，可以直接确定需要修改的项）。

#### **1.3.5.3.2 页表项（PTE）的同步更新**

- 在预热 TLB 之前，调度器必须确保页表的状态与新的作用域边界完全一致：
  - **冻结旧作用域**：确保所有无关 PTE 处于无效（Present=0）；
  - **激活新作用域**：遍历 `N_new` 及其到根的完整祖先链，将这些节点所映射的所有页的 PTE **设置为有效（Present=1, 权限正确）**。
- 此步骤确保了：**任何在预热后发生的内存访问，其合法性完全由页表状态决定**，为后续的 TLB 预热和安全异常处理奠定了基础。

#### **1.3.5.3.3 预热的顺序与策略**

- 预热顺序直接影响 TLB 的缓存局部性。EDSOS 采用 **“从根到叶”** 的顺序进行预热：
  - **从 LCA 的子节点开始，向下至 `N_new`**，依次预热 LCA 之后路径上的每一个节点。
- 现代 CPU 的 TLB 通常采用伪 LRU 等替换策略。后预热的条目在 TLB 中“更新鲜”，更不易被替换。程序最频繁访问的是**当前节点自身**的数据，其次是**直接父节点**。将它们最后预热，能最大程度保证它们在容量有限的 L1 TLB 中驻留，从而获得最佳性能。

#### **1.3.5.3.4 预热的实现方式：异步、非阻塞的硬件触发**

- 调度器对需要预热的每个页面，**执行一次虚拟地址的读取操作**（例如，读取该页的第一个字节）。
- 此读取会触发 MMU 的地址转换。由于 PTE 已被设为有效，硬件会成功完成 Page Walk（如果 TLB 中尚无条目），并将新的翻译条目 **安装到 L1/L2 TLB 中**。
- 调度器**不需要、也不应该等待**该页的数据从内存（或缓存）中实际加载完成。
- **优化**：可以通过预取（prefetch）指令来进一步提示硬件，并且利用硬件的自动 prefetch 来减少需要显式触发的读指令量。

---

## **1.3.6 同步机制**

### **1.3.6.1 EDSOS Event：执行顺序屏障**

在 EDSOS 中，**EDSOS Event** 并非传统操作系统中的内核对象（如 Windows 的 Event HANDLE 或 Linux 的 futex），而是一种**纯粹的概念性执行顺序屏障**。其核心设计目标是将并发同步从“共享状态上的竞争”转变为“由调度器驱动的、显式的执行流编排”。这一机制完全契合 EDSOS “调度器驱动”和“无隐式共享”的核心原则。

#### **1.3.6.1.1 本质：无实体的调度器提示信息**

EDSOS Event 的本质可以概括为以下几点：

- **无实体性**：Event 本身**不占用任何内存**，没有内核数据结构，也没有用户态的句柄（HANDLE）。它仅仅是一个由**名称**（`event_name`）唯一标识的逻辑概念。
- **存在性由调度器维护**：一个 Event 仅在至少有一个节点在等待它时，才在**某个调度器的 Wait Graph 中拥有一个逻辑槽位**。一旦所有等待者都被唤醒且没有新的等待者，该槽位即被回收，Event 在系统中“消失”。
- **作用是屏障**：Event 的唯一作用是作为一个**执行顺序的屏障**。调用 `wait(event_name)` 的节点会主动放弃 CPU，并将自己注册到该 Event 的等待队列中，其执行流在此处暂停。只有当另一个执行上下文调用 `signal(event_name)` 时，调度器才会将等待者从屏障后释放，使其重新进入就绪状态。
- **生命周期自动管理**：Event 的生命周期完全由其使用情况动态决定，无需显式的创建（`CreateEvent`）或销毁（`CloseHandle`）操作，极大地简化了编程模型并避免了资源泄漏。

#### **1.3.6.1.2 同核同步：O(1) 的调度器内操作**

当 `wait` 和 `signal` 操作发生在**同一个 CPU 核心**上时，EDSOS Event 的处理是极其高效的，其开销几乎为零。

- **核心数据结构**：每个 Per-Core 调度器维护一个 **Wait Graph**（详见 §1.3.3.3 就绪结构：Distributed Ready Forest）。

- **`wait` 操作流程**：
  - 节点执行 `edsos_wait(event_name)`。
  - 调度器桩函数将当前节点的状态从 `running` 改为 `block`。
  - 调度器在本地 Wait Graph 中查找或创建对应 `event_name` 的槽位，并将该节点加入其 FIFO 队列。
  - 调度器执行调度主逻辑，从 Ready Linked List 中选择下一个就绪节点执行。

- **`signal` 操作流程**：
  - 节点执行 `edsos_signal(event_name)`。
  - 调度器桩函数在本地 Wait Graph 中查找 `event_name` 对应的队列。
  - 如果队列非空，调度器将队列头部（或全部，取决于语义）的节点 **逐个插入本地 Ready Linked List**，并将其状态改为 `ready`。

> 这种同核处理模式确保了在最常见的同步场景下，EDSOS 能提供确定性的、极低延迟的同步性能。

#### **1.3.6.1.3 跨核同步：协商式唤醒（Negotiated Wakeup）**

当 `signal` 操作发生在与 `wait` 节点**不同的 CPU 核心**上时，情况变得复杂。EDSOS 采用一种**去中心化的协商式唤醒**机制来解决此问题，避免了全局锁或集中式事件管理器。

- **QUERY/RESPONSE/GRANT 三步协商流程**：
  - **QUERY（查询）**：Giver Core 的调度器通过 **链式消息总线** 广播一条 `QUERY(event_name)` 消息。
  - **RESPONSE（响应）**：每个收到 `QUERY` 消息的调度器会检查自己的本地 Wait Graph。如果发现有该 Event 的等待者，它会回复一条 `RESPONSE` 消息。该消息包含：
    - 本 Core 的 ID。
    - 一个用于排序的时间戳（通常基于 `RDTSC`），以近似实现跨核的 FIFO 公平性。
    - 等待者队列的长度等元数据。
  - **GRANT（授权）**：Giver Core 收集所有 `RESPONSE` 后，根据预定义策略（如选择时间戳最早的核心以保证公平性）**选出一个获胜者**（Winner Core），然后向获胜者发送 `GRANT(event_name)` 消息。
  - **本地唤醒**：Winner Core 收到 `GRANT` 消息后，在其**本地**执行与同核 `signal` 完全相同的操作：将等待者从 Wait Graph 移至 Ready Linked List。

- **关键优势**：
  - **去中心化**：没有单点瓶颈。
  - **按需通信**：只有存在等待者的 Core 才会参与协商，通信开销与实际需求成正比。
  - **近似 FIFO**：通过 `(core_id, rdtsc_tsc)` 对进行全序排序，可以在跨核场景下提供可接受的公平性保证。
  - **与调度器深度集成**：整个协商过程通过链式消息总线异步完成，并由调度器的 `check()` 机制定期处理（详见 §1.3.3.4 链式消息总线），不会阻塞正常的调度流程。

---

### 1.3.6.2 标准同步接口的实现
<TODO>
- Mutex / RWLock / Semaphore / Condition Variable 等
- 用户态状态 + Event 同步分离

---

## **1.3.7 系统调用模型**

### 1.3.7.1 第一类 syscall 架构
- 调度器本地处理型（零特权切换）
  - `wait`, `push`, `lift`, `yield` 等

### 1.3.7.2 第二类 syscall 架构

- 第二类：KPN 中转型（异步委托）
  - 文件、网络、权限等服务请求
- KPN（Kernel Proxy Node）机制
  - CTRN 形式的 capability 载体
  - 参数共享 + Event 同步 + 无拷贝

### 1.3.7.3 ABI 内联与安全固化
- 加载期注入只读指令段
- 运行时无鉴权，安全由结构保证

### 1.3.7.4 兼容模式支持（POSIX / Win32 / Native）

---

## **1.3.8 负载均衡**

### **1.3.8.1 负载均衡的策略与触发**

#### **1.3.8.1.1 Work-Stealing++**

EDSOS 的负载均衡机制称为 **Work-Stealing++**，它是对传统 Work-Stealing 的深度扩展。
核心思想是：**均衡的目标不是“每个 Core 的就绪节点数相等”，而是“最大化结构亲和、最小化上下文切换与跨 Core 通信”**。
该机制完全去中心化，依赖链式消息总线实现 Core 间协商，以 **TS 子树为迁移单位**，并引入 **代理节点缓存** 降低调度开销。

#### **1.3.8.1.2 触发跨 Core 调度的典型场景**

调度器持续监控本地负载状态，以下情况将触发 Work-Stealing++：

- **子树规模失衡**：  
  某 TS 子树节点数超过阈值（如 1024），导致单 Core 调度延迟上升；
- **TS 分散度过高**：  
  同一 TS 的子树分布在 ≥3 个 Core 上，引发频繁跨 Core `signal` 与 TLB 冷启动；
- **异常负载波动**：  
  通过链式消息总线或 IPI 收到邻居 Core 的 **过载告警**（如队列长度 > HIGH_WATERMARK）；
- **性能敏感 TS 争用**：  
  多个高优先级 TS（如云游戏主逻辑、实时音视频）共享同一 Core，导致调度抖动。

> 注意：EDSOS **不追求绝对负载均匀**。若一个 Core 运行 10 个低优先级 TS，另一个运行 1 个高优先级 TS，系统可能认为这是“良好均衡”。

---

### **1.3.8.3 去中心化协商：基于链式消息总线的请求-响应**

Work-Stealing++ 采用 **主动协商模型**，无全局协调者。

#### **1.3.8.2.1 触发主动请求的情况**

- **过载 Core**：  
  当 `ready_queue_length > HIGH_WATERMARK`，广播 `TASK_GIVE_REQUEST` 消息，附带：
  - 可迁移子树列表（按亲和性排序）；
  - 性能敏感度标签。

- **空闲 Core**：  
  当 `ready_queue_length < LOW_WATERMARK` 且未收到 `TASK_GIVE_REQUEST` 消息，广播 `TASK_GET_REQUEST` 消息，附带：
  - 自身空闲容量；
  - 优先提供的 TS 类型。

#### **1.3.8.2.2 响应和迁移触发**

- **主动响应**：
  收到 `STEAL_REQUEST` 后，如果认为是自己可以接收的，回复 `STEAL_OFFER`，包含：
  - 自身空闲容量；
  - 自身当前调度的 TS 数量；
  - 其他用于说明“为什么选我”的信息。

- **决策与迁移**：  
  Giver 收集所有 `STEAL_OFFER`，按以下策略选择 Taker：
  1. 优先选择 **同 NUMA 域** 的 Core；
  2. 若为性能敏感 TS，选择 **当前 TS 数量少且无其他关键 TS** 的 Core；
  3. 否则选择 **空闲容量最大** 的 Core。  
  确定后，发送 `MIGRATE_SUBTREE(subtree_root_gva, target_core)` 指令。

> 整个过程通过链式消息总线异步完成，**不阻塞本地调度**。

---

### **1.3.8.4 子树级迁移**

#### **1.3.8.3.1 迁移保持结构完整性**

迁移单位必须是 **完整的 TS 子树**（以某节点为根的全部后代），不允许发生“断枝”（子孙节点转移出去而侄甥节点保留）的情况，原因如下：

- **TLB 亲和性**：子树共享祖先链，迁移后可在目标 Core 预热完整 TLB 上下文；
- **事件局部性**：若子树内多个节点等待同一事件，拆分将导致跨 Core 唤醒开销。

#### **1.3.8.3.2 迁移流程**

1. Giver 调度器 `pause` 整棵子树；
2. 通过链式消息总线传输子树的所有节点指针（LVA）；
3. 更新子树中所有节点的 `subtree_home_core`；
4. Taker 将子树加入自己的 Ready Linked List。

> **数据零拷贝**：节点本身不需复制，仅调度上下文迁移。

---

### **1.3.8.5 代理节点机制：减少跨 Core 通信**

为降低同一 TS 分散在多 Core 时的协调开销，EDSOS 引入两类 **代理节点缓存**：

#### **1.3.8.4.1 代理根节点（Proxy Root）**

当一个 Core 上运行的子树的上游位于其他 Core，缓存子树的祖先链（直至 TS 根）。

- 用于 TLB 预热、Capability 验证；
- 接收来自上游节点的结构变更消息。

#### **1.3.8.4.2 代理子节点（Proxy Child）**

当一个 Core 上运行的 TS 将一个子树拆分给其他 Core，缓存该 TS 在其他 Core 上的子树信息。

- 用于跨 Core `signal` 路由；
- 接收来自下游节点的结构变更消息。

> 代理机制不仅用于跨 PM，也用于同一 CPU 的不同 Core，避免每次祖先链预热和事件触发都需全局查询。

---

### **1.3.8.6 TS 感知的均衡策略**

EDSOS 将 TS 分为两类，采用差异化调度策略：

| TS 类型 | 示例 | 调度策略 |
|--------|------|----------|
| **性能敏感型** | 云游戏主逻辑、实时控制 | 尽量独占 Core，避免与其他 TS 共享 |
| **性能不敏感型** | Capability 鉴权、注册表服务 | 多个可共用同一 Core，主动承担上下文切换开销 |

调度器通过节点中的 `perf_sensitivity` 标签识别类型，并在 Work-Stealing++ 决策中加权。

---

## **1.3.9 工程特性与范式总结**
- 性能：百万级节点、百纳秒调度、零拷贝 IPC
- 安全：作用域隔离、Capability 固化、无用户态竞态
- 可扩展：同核 → 多核 → 跨 CPU → 跨 PM 无缝扩展
- 调试性：全状态可见（Wait Graph、CTRN 拓扑、消息流）
- 范式创新：
  - 并发 = 执行顺序的显式编排
  - 进程 = 动态演化的 TS 实例
  - 操作系统 = 超TS 结构的活体系统
