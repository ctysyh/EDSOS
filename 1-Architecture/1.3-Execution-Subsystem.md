# **1.3 Execution-Subsystem**

## **1.3.1 设计愿景与核心原则**
- 执行单元的重新定义：从“线程”到“TS 节点”
- 核心设计哲学
  - 无隐式共享
  - 结构决定可见性
  - 调度器驱动
  - 去中心化与局部自组织
  - 硬件协同与创造性利用
- 与传统执行模型的本质区别

---

## **1.3.2 Tree-Stacked（TS）进程模型**

### **1.3.2.1 TS 作为统一执行原语**

在 EDSOS 中，“进程”不再是一个扁平的地址空间加一个执行流的集合，而是一棵 **动态演化的 Tree-Stacked（TS）结构**。这棵 TS 是进程在运行时的完整结构化表达，其拓扑编码了控制流、数据流、作用域、生命周期与局部性等全部执行语义。

#### **1.3.2.1.1 进程即 TS：从抽象到实例**

每个 EDSOS 进程实例在创建时，由 EDSOS Loader 构造一棵初始 TS。这棵 TS 的根节点具有特殊语义：它承载了传统操作系统中进程控制块（PCB）的全部元信息以及 EDSOS 特有的补充元信息，包括：

- 进程 ID；
- Capability 授权集合（可访问的文件路径前缀、服务访问权限等）；
- 调度策略；
- 其他。

除根节点外，TS 中的所有其他节点均为**普通执行单元**，无全局 ID，仅通过父子指针在树中定位。这种设计消除了传统进程模型中“线程 ID”“栈指针”“堆段”等离散概念，将它们统一为 TS 节点的结构属性。实际上，这样的组织形式类似于 Linux 中的 VMA 结构，但是含义内容、拓扑结构、可执行操作等均截然不同。

#### **1.3.2.1.2 节点：执行与内存的统一体**

TS 节点是 EDSOS 中最小的可调度、可执行、可共享的原子单元。每个节点在虚拟地址空间中占据 **连续的若干虚拟页（VPs）**，包含三个逻辑段：

- **元数据段**：节点类型、大小、祖先路径哈希等（由 EDSOS Loader 生成）；
- **指令段**：一段符合 EDSOS Standard ABI 的代码流；
- **数据段**：局部变量、栈帧、或逃逸数据的存储空间。

节点的语义取决于其内容与上下文：

- 若包含有效指令段，则可被调度执行，行为类似 **协程或函数调用帧**；
- 若仅含数据段（无指令段），则成为 **退化节点**，主要承担数据缓存和共享功能；
- 若为根节点，则兼具 **进程元数据容器** 与 **Capability 策略锚点** 功能。

这种统一性使得 EDSOS 能在 **同一抽象下支持从微秒级函数调用到退化模式的大型线程** 的全谱系执行模式。

#### **1.3.2.1.3 节点状态机和节点操作**

节点使用 5 态状态机：

| 状态 | 含义 |
|------|--------|
| `ready` | 就绪等待调度 |
| `running` | 正在执行 |
| `block` | 阻塞 |
| `zombie` | 临终 |
| `error` | 错误 |

有 10 种操作可以改变节点状态或节点内容：

| 操作 | 含义 |
|------|--------|
| `push` | 原叶子节点压入新叶子节点作为自己的子节点 |
| `active` | 调度器将 `ready` 节点调度执行进入 `running` 状态 |
| `wait` | `running` 节点进入等待某个 EDSOS Event 的 `block` 状态 |
| `lift` | 节点改变自身和一个子节点之间的拓扑关系，使之成为自己的父节点 |
| `merge` | 节点将自己和一个子节点合并 |
| `divide` | 节点将自己的一部分切割成为新节点作为自己的子节点 |
| `pause` | `running` 节点暂停执行，调度器安排下一个就绪节点 |
| `finish` | `running` 节点执行流结束或退出 |
| `pop` | 叶子节点被自己的父节点释放 |
| `warn` | 任何节点被触发安全警告进入 `error` 状态 |

所有操作均由 EDSOS Scheduler 执行，详见 §1.3.3 节点调度器架构。

#### **1.3.2.1.4 TS 的构建：EDSOS Loader 的角色**

一棵 TS 的初始结构在进程加载阶段由 **EDSOS Loader** 构建，过程如下：

- **解析 ELF 与 Manifest**：
  - 提取入口函数、依赖库、Capability 声明；
  - 检查 `syscall` 调用，匹配合法的 syscall 桩函数内联展开为对应的 x64 指令，遇到非法调用按策略拒绝加载（默认）或退化替换（兼容模式下开启弱安全选项）。

- **分配虚拟地址与元数据**：
  - 请求内存子系统为每个节点分配连续 LVA；
  - 生成 `.edsos_node_meta`，记录 `vbase`、`size`、`ancestor_path_hint`。

- **搭建初始树形拓扑**：  
  - 创建根节点，注入 Capability 与系统资源视图；
  - 将主入口函数作为根的直接子节点；
  - 静态依赖（如全局变量、init 函数）按作用域关系组织为子树。

最终，Loader 输出一棵**结构完整、作用域明确、可直接调度**的 TS，作为进程的运行时起点。

#### **1.3.2.1.5 与传统进程模型的映射**

| 传统概念 | EDSOS 对应 | 说明 |
|--------|-----------|------|
| 进程地址空间 | 整棵 TS | 作用域受结构限制 |
| 线程 | 可并行调度的 TS 节点 | 支持父子并行 |
| 栈帧 | 正常节点的数据段 | 大小受 TLB 约束 |
| 堆内存 | 无指令段/弱指令段的退化节点的数据段 | 生命周期由引用计数管理 |
| PCB | TS 根节点 | 含 Capability 与调度元数据 |

值得注意的是，EDSOS **在用户 TS 中不进入内核态**。系统服务（如文件、网络）同样以 TS 形式存在，通过结构共享与事件同步协作。TS 中可能包含一个 **KPN（Kernel Proxy Node）** 作为与系统服务通信的代理，详见 §1.3.7 系统调用模型。

---

### **1.3.2.2 Meta-TS：整个系统的结构化表示**

EDSOS 摒弃了传统操作系统中“内核管理进程”的主从模型，转而采用一种 **全结构化、去中心化的宇宙观**：整个系统——包括用户应用、文件服务、网络协议栈、设备驱动乃至安全策略模块——共同构成一棵动态演化的 **Meta-TS（Meta-Tree-Stacked）**。

Meta-TS 并非物理存在的单一数据结构，而是 **所有 TS 实例在逻辑上的并集**。每个 TS 子树代表一个自治的执行上下文，彼此通过显式结构操作（如 CTRN 共享、事件同步）协作，而非依赖中心化的内核仲裁。

#### **1.3.2.2.1 所有服务即 TS：对等性与模块化**

在 EDSOS 中，不直接存在特权层级的“内核进程”。取而代之的是：

- **用户 TS**：如 `/user_a/chrome_instance_01`，代表一个浏览器实例；
- **系统服务 TS**：如 `/net/sockets`、`/fs/ext4_main`、`/auth/cap_validator`，提供网络与文件 I/O 服务乃至 Capability 审批；
- **驱动 TS**：如 `/drivers/net/e1000`、`/drivers/storage/nvme_ctrl`，直接操作硬件。

这些 TS 在结构上完全对等：
- 均由节点构成，遵循相同的 TS 公理；
- 均可被调度器调度；
- 均通过 CTRN 和 EDSOS Event 与其他 TS 通信。

差异仅体现在 **Capability 授权** 与 **硬件访问权限** 上，而非执行模型本身。这种设计天然支持**微内核的安全边界**，同时通过结构共享避免了传统微内核的 IPC 性能瓶颈。

#### **1.3.2.2.2 狭义内核：作为“硅中介层”**

传统意义上的“内核”（内存映射、TLB 管理、上下文切换、中断处理）在 EDSOS 中 **不表现为任何 TS 节点**，而是以 *硅中介层（Silicon Interposer）* 的形式存在：

- 它直接运行在裸金属上；
- 提供调度器、中断与错误响应、内存子系统中的分配和映射等基础原语；
- 所有 TS 通过 **加载期 ABI 内联** 直连该中介层；
- 通信“布线”通过 **CTRN、链式消息总线、EDSOS Event** 实现。

本文档描述的所有 **逻辑操作** 都属于 *硅中介层* 的范畴，包括且不限于调度器代码和 *散落* 的其他内核逻辑代码。

这种设计消除了“陷入内核”的概念：系统调用要么由调度器本地处理（隐式的安全特权切换），要么通过异步委托到服务 TS（不在当前 TS 范围内执行）。

#### **1.3.2.2.3 进程生命周期：TS 根节点的创建与销毁**

在 EDSOS 中，“创建一个进程”等价于 **构造一棵新的 TS 并将其根节点注入 Meta-TS**。

- **创建流程**：
  1. 父 TS 调用 `edsos_spawn(manifest_gva)`；
  2. 调度器触发 EDSOS Loader，解析 ELF 与 Capability 清单；
  3. Loader 构建初始 TS 结构；
  4. 新 TS 的根节点被 `push` 为父节点的子节点（或全局命名空间下的独立子树）；
  5. 根节点被标记为 `ready`，等待调度。

- **销毁流程**：
  1. 用户调用 `exit()` 或异常终止；
  2. 调度器递归 `finish` 整棵 TS 子树；
  3. 所有子孙节点进入 `zombie` 状态；
  4. 父节点（或全局回收器）执行 `pop`，释放内存；
  5. 引用计数归零后，CTRN 自动回收，无内存泄漏。

整个过程 **无需全局进程表**，生命周期完全由 TS 结构与引用计数隐式维护。

#### **1.3.2.2.4 进程标识与命名：根节点即身份**

EDSOS 使用 64 位整数 PID 作为进程的全局唯一标识。PID 保存在根节点中，可通过全局注册表注册人类可读名（如 `/user/chrome_instance_01`）。

TS 运行的兼容模式和 Capability 授权标识在加载期写入根节点。

根节点一旦创建即不可修改，直到 TS 的生命周期结束。

#### **1.3.2.2.5 IPC 的结构化实现：CTRN 作为联结**

##### **1.3.2.2.5.1 CTRN 的定义**

进程间通信（IPC）在 EDSOS 中不是“消息传递”，而是通过 **CTRN（Cross TS Referring Node）** 显式联结。这实质上是结构化的共享内存，通过作用域和引用计数确保安全性。

CTRN *通常* 是一个退化 TS 节点，在每个引用的 TS 中各自遵守其内的结构约束；CTRN 不会因为被两个 TS 同时引用而造成两个 TS 之间的边界破坏，即其中任一 TS 不可访问 CTRN 在另一 TS 中的直系祖节点。

CTRN **只负责数据共享**，**不负责并发控制**。各个 TS 同时读写的竞态由 EDSOS Event 或其他机制手动解决。

含有指令段的特殊 CTRN 主要包括 KPN 及其安全子节点，详见 §1.3.7.2.2 利用 KPN 实现高效信道。

##### **1.3.2.2.5.2 CTRN 的工作流程**

- **创建**：Node A `push(CTRN_X)` → CTRN_X 成为 A 的子节点。
- **注册**：CTRN_X 注册到全局命名表（DHT）。
- **lift**：A `lift(CTRN_X)` → CTRN_X 成为 A 的父节点 → A 可访问。
- **跨 TS 连接**：
   - Node B `lookup("CTRN_X")` → 获得节点起始 VA。
   - B `push(CTRN_X_VA, 'as_ctrn')` → `lift(CTRN_X)` → CTRN_X 成为 B 的祖先。
- **断开**：`pop` → refcnt--，refcnt=0 时释放。

---

## **1.3.3 节点调度器架构**

### **1.3.3.1 Per-Core 分布式调度器**

EDSOS 调度器（Scheduler）并非传统意义上的“内核线程”或“系统进程”，而是一套 **分布于每个 CPU 核心、由内联指令段与本地数据结构共同构成的运行时契约**。它没有全局实例，不占用任何 TS 节点，也不表现为可被调度的执行实体；其存在完全体现为 **每个 Core 上一组协同工作的软硬件机制**。

#### **1.3.3.1.1 调度器的本质：内联指令 + 本地数据结构**

EDSOS Scheduler 由两部分组成：

- **内联指令段（Trampoline Stubs）**：  
  在进程加载阶段，EDSOS Loader 将调度器接口（如 `edsos_wait`、`edsos_push`、`edsos_yield`）**静态内联展开为只读机器指令**，嵌入用户代码的函数入口/出口处。这些 stub 直接操作当前 Core 的调度器数据结构，**不由进程代码显式syscall特权切换（ring0/ring3）**，也**无法被用户代码篡改或绕过**。

- **Per-Core 数据结构**：  
  每个 CPU Core 维护一组独立的调度状态，包括：
  - **Ready Forest**：就绪节点的 MPSC 队列；
  - **Wait Graph**：事件名到阻塞节点列表的映射；
  - **PCID 与 TLB 上下文缓存**；
  - **负载统计与迁移状态**（如 `ready_queue_length`、`subtree_home_core`）。

调度器的“执行”即这些 stub 对本地数据结构的原子操作，其行为由硬件内存模型与调度器设计共同保证一致性。

#### **1.3.3.1.2 完全去中心化：无全局协调**

EDSOS 调度器严格遵循 **Per-Core 独立原则**：

- 不存在全局就绪队列、全局调度器线程或中心仲裁者；
- 所有调度决策（除跨核负载均衡外）均在本地完成；
- 节点创建（`push`）、阻塞（`wait`）、唤醒（`signal`）等操作仅影响当前 Core 的数据结构；
- 跨核协作通过 **链式消息总线** 异步完成，不阻塞本地调度。

这种设计使得系统天然具备 **水平扩展能力**：增加 CPU Core 即线性提升调度吞吐，无锁竞争瓶颈。

#### **1.3.3.1.3 核心职责：调度器做什么**

EDSOS Scheduler 的职责被严格限定在 **执行流控制与结构维护** 范围内，具体包括：

- **就绪结构维护**：  
  管理 `Ready Linked List`，支持 FIFO、紧急插入、依赖感知插入等多种策略，并实现调度决策前置，入队时即确定顺序，调度执行仅为 O(1) 切换；
- **事件驱动同步**：  
  维护 `Wait Graph`，实现 `wait(event)` 与 `signal(event)` 的 O(1) 阻塞/唤醒；
- **节点激活与上下文切换**：  
  执行 `active` 操作：重建祖先链、预热 TLB、切换 CR3、跳转指令入口；
- **结构操作执行**：  
  原子执行 `push`/`pop`/`lift`/`merge`/`divide` 等节点操作，确保 TS 结构良定义；
- **链式消息总线端点**：  
  作为消息总线的一个节点，收发跨 Core 事件（如负载 steal 请求、跨核 signal）；
- **硬件状态协同**：  
  管理 PCID 分配、TLB 刷新、RDTSC 时间戳采样等，隐藏硬件复杂性。

值得注意的是，**调度器不包含任何业务逻辑**：它不解析文件路径、不验证 Capability、不管理全局资源。这些职责被委托给对应的系统服务 TS（如 VFS、Auth），通过 KPN 异步协作完成（见 §1.3.7 系统调用模型）。

#### **1.3.3.1.4 调度器与 TS 的关系**

调度器与 TS 是 **正交但紧密协作** 的两个维度：

- TS 定义了“**什么可以运行**”（作用域、结构、生命周期）；
- 调度器决定了“**何时运行、在哪运行**”（执行顺序、Core 亲和、TLB 上下文）。

调度器通过遍历 TS 的父子指针重建祖先链，通过 TLB 预热确保作用域访问安全，通过 `pause`/`active` 保障结构变更的原子可见性。**TS 是语义，调度器是执行引擎**。

---

### **1.3.3.2 调度触发机制**

EDSOS 调度器的执行由两类正交机制触发：**协作式调度（Cooperative Scheduling）** 与 **抢占式调度（Preemptive Scheduling）**。二者均通过 **内联指令段** 实现，但触发时机、调用上下文与职责范围截然不同。所有调度路径均**隐式包含链式消息总线的消息处理逻辑**，以维持去中心化系统的稳定性。所有这些接口统称为 EDSOS Scheduler 的**Normal Open Interface (NOI)**。

#### **1.3.3.2.1 协作式调度：加载器内联的桩函数**

协作式调度由 **EDSOS Loader 在进程加载期静态内联** 到用户代码中，分为两类桩函数：

##### **1.3.3.2.1.1 节点边界桩（Entry/Exit Trampolines）**

- **触发时机**：仅在节点**开始执行前**（entry）与**结束执行后**（exit）自动调用，用户代码无法显式调用或跳过。
- **功能**：
  - **Entry 桩**（节点即将运行）：
    - 递归预热 TLB：依次 `touch` 当前节点及其祖先链的 hot pages；
    - 设置 CPU 时间配额（通过 TSC deadline）；
    - 恢复寄存器上下文（若为 resume）；
    - **调用 `check()` 处理链式消息总线的入站消息**。
  - **Exit 桩**（节点即将结束）：
    - 保存寄存器状态（若需 resume）；
    - 通知调度器当前节点已结束；
    - **触发调度主逻辑**：
      ```asm
      ; 1. 从 Ready Forest 头部弹出 next_node
      ; 2. 验证其祖链合法性
      ; 3. 切换 CR3（PCID）
      ; 4. jmp next_node.entry
      ```
    - **调用 `check()` 处理链式消息总线的入站消息**。

> 此类桩函数是调度器“执行切换”的核心路径，确保每次上下文切换都伴随 TLB 预热与消息处理。

##### **1.3.3.2.1.2 状态操作桩**

- **触发时机**：可在节点执行流中**任意位置显式调用**，如 `edsos_wait()`、`edsos_push()`、`edsos_lift()` 等。
- 默认的隐式行为：**调用 `check()` 处理链式消息总线的入站消息**。

> 这类接口即 *NOI 中可被用户代码直接调用的接口*，详见 §1.3.7.1 第一类 syscall。

#### **1.3.3.2.2 抢占式调度：硬件中断驱动**

当协作式调度不足以保证公平性或实时性时，EDSOS 依赖硬件中断强制介入：

- **触发源**：
  - **本地定时器中断**：CPU 时间片耗尽；
  - **IPI（Inter-Processor Interrupt）**：来自其他 Core 的负载 steal 请求、跨核 `signal` 唤醒；
  - **TLB 一致性中断**：远程 TLB 刷新请求；
  - **安全异常中断**：Page Fault 验证失败、元数据损坏等。
  
- **中断处理流程**：
  1. 中断向量表跳转至调度器中断处理 stub；
  2. 保存当前 `running` 节点的完整上下文；
  3. 根据中断类型执行相应操作：
     - **时间片耗尽** → `pause` 当前节点，将其重新加入 Ready Forest 尾部；
     - **IPI 唤醒** → 将指定节点从 Wait Graph 移至 Ready Forest；
     - **安全异常** → `warn` 当前节点，触发子树终止；
  4. **调用 `check()` 处理链式消息总线的入站消息**；
  5. 触发调度主逻辑（同 Exit 桩），切换至下一就绪节点。

> 抢占式路径确保系统在恶意或阻塞节点存在时仍具备**强活性（liveness）与安全性**。

#### **1.3.3.2.3 链式消息总线的协同要求**

所有调度触发路径（无论协作或抢占）均**强制执行 `check()`**，原因在于：

- 链式消息总线采用 **无中心、无确认、SPSC 环形缓冲区** 设计；
- 消息可靠性依赖于**接收方稳定、频繁地消费消息**；
- 若某 Core 长时间不调用 `check()`，其入站队列将溢出，导致系统失活。

因此，EDSOS 将消息处理**深度耦合到调度路径**中，确保即使在高负载下，跨核协调（如负载均衡、事件唤醒）仍能及时完成。

--- 

### **1.3.3.3 就绪结构：Distributed Ready Forest**

EDSOS 调度器的核心就绪管理结构称为 **Distributed Ready Forest**，它并非单一数据结构，而是 **所有 CPU Core 上独立维护的就绪链表（Ready Linked List）的逻辑并集**。每个 Core 的就绪链表完全自治，支持灵活的插入策略与结构亲和优化，共同构成一个去中心化、高可扩展的任务分发网络。

#### **1.3.3.3.1 Per-Core Ready Linked List**

每个 CPU Core 维护一个 **Ready Linked List**。由于同一 CPU Core 天然串行，节点的插入和弹出无需锁或原子操作。

##### **(1) 链表节点**
- 每个链表项为指向 TS 节点的指针（LVA）；
- 此外还可选通过一个附加数据指针（LVA）附加元数据：`priority_hint`、`scheduling_policy` 等。

##### **(2) 多策略插入位置**
调度器支持三类插入语义，由调用上下文或元数据决定：

- **尾部插入（Tail Insertion）**：  
  默认策略，保证 FIFO 公平性。

- **头部插入（Head Insertion）**：  
  用于高优先级系统服务节点（如中断回调、驱动完成处理），确保低延迟响应。

- **智能顺位插入（Smart Positioning）**：  
  由以下信息驱动：
  - **编译器扩展属性**：如 `__edsos_depends_on(node_X)` 声明依赖链；
  - **Loader 解析的启发式元数据**：如热点函数、I/O 绑定提示；
  - **运行时优化器反馈**：基于历史执行间隔、缓存命中率的 ML 模型输出。  
  调度器根据提示信息将节点插入到**依赖节点之后、同类亲和节点附近**的位置，提升 TLB 与缓存重用率。

##### **(3) “基地 Core”绑定的隐式性**
- 新节点由当前 Core 的调度器 `push` 创建，**自动加入当前 Core 的 Ready Linked List**；
- 由此整个子树的生长都在当前 Core 执行，具有良好的局部性；
- **无需显式绑定操作**，迁移（migration）才是显式行为（见 §1.3.8 负载均衡）。

#### **1.3.3.3.2 Per-Core Wait Graph**

Wait Graph 是调度器维护的 **事件到阻塞节点的映射结构**，用于实现 `wait(event)` / `signal(event)` 同步原语。其设计兼顾性能、内存效率与命名空间隔离。
Wait Graph 包含三个 Per-Core 组件：Block Slot Table、Local Event Name Hash Map、Global Event Name Hash Map。

##### **(1) 阻塞节点槽位表（Block Slot Table）**：

  - 类型：动态数组 `BlockSlot slots[MAX_SLOTS]`，初始分配 `N₀` 个槽位（如 64）；
  - 每个 `BlockSlot` 包含：
    - `Node* blocked_nodes[MAX_NODES_PER_SLOT]`：FIFO 队列，初始长度 `M₀`（如 8）；
    - `uint32_t count`：当前阻塞节点数；
    - `bool is_global`：标记该槽位是否对应全局事件（用于区分命名空间）。
  - 槽位复用与生命周期：
    - 槽位（ID）可复用，生命周期由引用计数管理。
  - 动态伸缩策略：
    - 若某槽位 `count == MAX_NODES_PER_SLOT`，则分配新缓冲区（如 2× 扩容），迁移指针；
    - 若连续 `T` 调度周期内 `count < M₀/4`，则缩容；
    - 若所有槽位使用率 > 90%，则 `MAX_SLOTS` 阶段式扩展（如 +64）；
    - 若长期空闲槽位 > 50%，则收缩 `MAX_SLOTS`。

##### **(2) 本地事件名映射表（Local Event Name Hash Map）**：

  - 类型：哈希表 `local_event_name → local_event_id`；
    - `local_event_name`：进程内事件名（如 `"mutex_lock_0x1234"`）；
    - `local_event_id`：分配自 **本地 ID 空间**（如 0 ~ N₀−1）；
    - 名称在当前 CPU Core 唯一，无需全局注册；
  - 初始化：映射在首次 `wait()` 时创建。
  - 生命周期：随首个 `wait()` 创建，随最后一个阻塞者离开且无 pending signal 时回收。

##### **(3) 全局事件名映射表（Global Event Name Hash Map）**：

  - 类型：哈希表 `global_event_name → local_event_id`；
  - `global_event_name`：全局唯一名（如 `"io_complete_disk0_blk123"`）；
  - `local_event_id`：分配自 **全局 ID 映射空间**（与本地 ID 不重合，如高位设 1）；
  - 初始化：首次 `lookup()` 时查询全局事件注册表（DHT），成功后缓存映射。
  - 生命周期：随首个 `wait()` 创建，随最后一个阻塞者离开且无 pending signal 时回收。

> 两个映射表的 `local_event_id` 值域互斥（如本地 ID < 2³¹，全局 ID ≥ 2³¹），确保无冲突。

---

### **1.3.3.4 链式消息总线**

链式消息总线是 EDSOS 节点调度器实现 **去中心化跨核协调** 的核心通信基础设施，无锁、无中心，依赖局部结构与定期消费运行。

#### **1.3.3.4.1 链式可扩展的拓扑结构**

链式消息总线在逻辑上是一张 **有向图**，实现为每个节点维护一组 **数量等于直连通道数** 的 SPSC（Single-Producer Single-Consumer）无锁队列作为消息缓冲区，节点自己作为消费者、**被直连的上一节点** 作为生产者。

每个节点持有数量等同于直连通道数的 **第 i 个方向的下一节点消息缓冲区** 引用，作为 *直连* 的实现。当发送消息时，**直接写入下一个节点的消息缓冲区**。

#### **1.3.3.4.2 消息的数据结构**

消息有固定格式的消息头和动态的消息负载；发送消息时，写入缓冲区的数据是 **完整的消息头结构体**，其中包含包含标签、传播方向、TTL 等内容，以及对应的消息负载指针和长度。

#### **1.3.3.4.3 总线调用**

每个节点必须频繁调用 `check()` 函数以及时处理自己的缓冲区中的消息。这个函数内会遍历自己的所有缓冲区，根据消息标签调用回调表来处理，并根据 TTL 决定是将消息继续传播到下一个节点还是释放，传播方向规定了具体应该传播给哪个节点。

---

### **1.3.3.5 面向调度器的事件闭包**

**面向调度器的事件闭包**（Scheduler-Driven Event Closure, SDEC） 是 EDSOS 节点调度器提供的一种**轻量级、受控的事件响应机制**。它允许具有特定权限的节点注册一段**静态声明的指令片段**，当关联事件被触发时，调度器**直接执行该指令片段**，而无需唤醒并完整调度一个独立节点。SDEC 旨在优化高频、简单的事件处理响应，同时严格维持 EDSOS 的作用域安全并遵守最小权限原则。

#### **SDEC 的详细定义**

SDEC 是由**具有注册权限的节点主动创建并绑定到自身**的回调实体，其核心特征如下：

- **触发方式**：与一个 EDSOS Event 绑定，当该 Event 被 `signal` 时由调度器自动执行。
- **参数与返回值**：
  - **无返回值**：SDEC 无“返回”操作。
  - **可接受参数**：SDEC 可接受事件触发时携带的附加信息作为输入参数。
- **不可嵌套等待其他 Event**。
- **生命周期**：与绑定节点一致；当绑定节点 `finish` 时其所有 SDEC 自动释放。

#### **SDEC 作为结构体实例解析和执行**

SDEC 以结构体实例的形式存在，其关键字段包括：

| 字段 | 说明 |
|------|------|
| `type_tag` | 枚举值 `NODE_TYPE_CLOSURE`，用于与普通节点区分 |
| `if_param` | 布尔值，表示是否需要参数 |
| `host_node` | 指向绑定节点的指针 |
| `code_start`, `code_length` | 指令段起始偏移与长度（字节） |
| `data_start`, `data_length` | 可访问数据段起始偏移与长度（字节） |

SDEC 使用一个临时的小运行空间，排布方式为小端序、数据段之后紧接指令段，注册时指令段的地址均为相对地址，加载执行时替换为绝对地址。

该 Closure Descriptor 的指针被存储在 **Wait Graph 中对应 Event 的唤醒槽位**（Wake Slot） 中，且相对于所有完整节点靠前排列。当 Event 被触发时，调度器执行以下流程：

1.  确认指针引用的类型为 `NODE_TYPE_CLOSURE`。
2.  **验证内存访问合法性**：
    - 检查 `code_length` 和 `data_length` 超过上限；
    - 将指令段中的相对地址替换为 `data_start` 为起始的绝对地址并同时检查其在 `data_length` + `code_length` 范围内。
3.  若验证通过，调度器**直接跳转执行** `[code_start, code_start + code_length)` 范围内的指令，使用事件附加信息作为参数。
4.  执行完成后，调度器继续处理该 Event 的其他监听者。

#### **SDEC 的安全约束**

1.  **注册权限限制**：仅当节点的 KPN 中包含 `CAP_REGISTER_CLOSURE` Capability 时，方可调用 `edsos_register_closure(...)` 注册 SDEC。普通节点默认无此权限。单个节点可注册的 SDEC 数量存在上限。
2.  **禁止事件嵌套等待和自触发**：
    - SDEC 内不可 `wait` 任何事件；
    - 节点不可 `signal` 自己注册的 SDEC 所监听的事件。
3.  **数据访问范围显式声明**：
    - 注册时必须显式指定 `data_start` 与 `data_length`；
    - `data_length` 不可超过最大窗口（通常为一个内存页）。
4.  **指令来源与限制**：
    - `code_start` 必须指向绑定节点自身的指令段；
    - `code_length` 不得超过上限（如 256 字节）；
    - 指令序列必须 **无调用、执行所需的权限不超过绑定节点的权限**。
5.  **只有具有有效指令段的节点可注册闭包**。
6.  **参数受限且无返回值**：
    - SDEC 可能的唯一输入参数为 **事件触发时的附加信息**，禁止通过其他方式传递复杂参数；
    - SDEC 执行的寄存器状态和缓存在到达指令流末尾之后直接废弃，无返回操作。

> *事件触发时的附加信息* 详见 §1.3.6.1 EDSOS Event：执行顺序屏障。

---

## **1.3.4 节点操作与生命周期**

### **1.3.4.1 原子操作语义**

#### **1.3.4.1.1 `push`：原子创建，无中间态**

- **语义**：创建新叶子节点，挂载为当前节点的子节点。
- **触发者**：当前 running 节点（如函数调用、线程 spawn）。
- **起始和目标状态**：空内容 → ready。
- **原子性保证**：
  - 内存子系统必须 **一次性分配所有 VPs**；
  - 若 VP 不足 → 返回错误，不创建部分节点；
  - 跨机 push：目标 PM 完成 VP 分配 + 祖先链验证后，才返回成功。
- **并行性**：多个核心可并发 push 不同父节点的子节点，无冲突。
- **无需中间态**。

#### **1.3.4.1.2 `active`：调度器驱动的上下文激活**

- **语义**：将 ready 节点调度到 CPU 核心执行。
- **触发者**：调度器。
- **起始和目标状态**：ready → running。
- **关键点**：
  - **不限于叶子节点**：父子可并行（如协程 + 主线程）；
  - **调度器责任**：
    1. 选择 ready 节点；
    2. 重建其祖先链（通过父指针遍历）；
    3. **增量 TLB 预热**（L1: 自身；L2: 祖先）；
    4. 加载寄存器上下文；
    5. 切换到 running。
- **失败处理**：
  - TLB 预热失败（罕见）→ 标记 error，尝试下一节点；
  - 无 ready 节点 → 进入 idle。
- **并行模型**：同片/跨片核心调度逻辑一致，仅 TLB 预热延迟不同。

#### **1.3.4.1.3 `wait`：阻塞与事件驱动唤醒**

- **语义**：节点主动放弃 CPU，等待某事件（I/O 完成、锁释放等）。
- **起始和目标状态**：running → block。
- **实现机制**：
  - 节点调用 `edsos_wait(event_id)`；
  - EDSOS 将节点加入 **event_id 的等待队列**；
  - 状态转为 block；
  - 当事件发生，**事件管理器**遍历队列，将节点状态转为 ready。
- **关键要求**：
  - **唤醒必须精准**：避免虚假唤醒（spurious wakeup）；
  - **无竞态**：wait 与 event_signal 需原子协调（通过内存子系统事件总线）。
- **并行性**：多个节点可 wait 同一事件，无冲突。

#### **1.3.4.1.4 `lift`：结构变更，需 pause**

- **语义**：将节点挂载到更高祖先下，改变其作用域。
- **起始和目标状态**：结构变更。
- **无需中间状态**，但需 **pause 所有子孙节点**。
- **详细流程**：
  1. 节点发起 lift(target_ancestor_gva)；
  2. EDSOS 验证 target_ancestor 在 TS 树中（通过 LVA 遍历）；
  3. **暂停所有子孙节点**（递推调用 `pause`）；
  4. 原子更新父指针；
  5. 子孙节点状态仍为 ready，但下次 active 时将使用新祖链。
- **为什么无需 lift_pending**？
  - 因为 **作用域变更对子孙不可见，直到它们被重新调度**；
  - TLB 是 per-core 的，旧核心的 TLB 仍有效，直到被刷新；
  - **安全由调度器保证**：子孙下次运行时，TLB 预热基于新父链。
- **跨机 lift**：跨机传输新挂载点，目标 PM 执行相同 pause + 更新；通过代理根节点（缓存本机子树的根节点之上的信息，接收其他 PM 发来的作用到本机子树根节点的直系祖节点链的结构变更消息）和代理子节点（缓存本机父树的从一个节点开始的子树在其他 PM 的信息，接收其他 PM 发来的从子树向上作用到这个节点的结构变更消息）来在每个 PM 中缓存信息，减少频繁跨机访问开销。

#### **1.3.4.1.5 `merge`：扩容操作，需 pause**

- **语义**：将自身与一个“扩展子节点”合并，扩大虚拟地址范围。
- **起始和目标状态**：结构变更。
- **流程**：
  1. 先 `push` 一个扩展子节点（仅数据段）；
  2. 调用 `merge(child_gva)`；
  3. **pause 自身**（若 running）；
  4. 内存子系统 **虚拟重映射**：将 child 的 VPs 合并到父的地址空间；
  5. 更新 `vbase/size`；
  6. 子节点标记为 zombie；
  7. 自身状态仍为 ready（若原为 ready）或 running（下次调度恢复）。
- **关键**：merge 期间，**自身不能执行**，故需 pause。
- **无需 merge_pending**：因操作在 pause 保护下原子完成。

---

#### **1.3.4.1.6 `divide`：分裂操作，需 pause**

- **语义**：将部分数据分裂为新子节点，用于回收或 Lift。
- **起始和目标状态**：结构变更。
- **流程**：
  1. **pause 自身**；
  2. 内存子系统分配新 VPs；
  3. 迁移数据；
  4. 创建新子节点（状态 ready）；
  5. 更新自身 `vbase/size`；
  6. 恢复状态。
- **典型场景**：冷数据回收 → divide → lift 到全局缓存池。

---

#### **1.3.4.1.7 `pause`：中断驱动的上下文冻结**

- **语义**：将 running 节点暂停，保存上下文，回到 ready。
- **起始和目标状态**：running → ready。
- **触发场景**：
  - 定时器中断（时间片到期）；
  - TLB 一致性中断（跨核 TLB 刷新）；
  - 调度器主动抢占；
  - lift/merge/divide 需要暂停。
- **操作**：
  - 保存寄存器；
  - 状态 → ready；
  - **不清空 TLB**（下次 active 可复用，除非祖链变更）。

---

#### **1.3.4.1.8 `finish`：正常退出**

- **语义**：节点执行完毕，进入 zombie。
- **触发**：`return` 或 `edsos_exit()`。
- **起始和目标状态**：running → zombie。
- **行为**：
  - 若有子节点，**递归 finish**（TS 子树语义）；
  - 父节点可 later `pop`。

---

#### **1.3.4.1.9 `pop`：父节点回收子节点**

- **语义**：父节点显式回收已 zombie 的子节点。
- **起始和目标状态**：zombie → 空内容。
- **原子性**：检查子节点状态 == zombie → 释放内存 → 删除父指针。
- **跨机 pop**：通过代理子节点向对应的远程 PM 中的代理根节点发送 `pop` 消息。

---

#### **1.3.4.1.10 `warn`：安全异常**

- **语义**：强制进入 error 状态，终止子树。
- **起始和目标状态**：any → error。
- **触发**：
  - Page Fault 验证失败（访问非法地址）；
  - 元数据 magic 错误；
  - 签名无效；
  - 其他异常和错误。
- **行为**：
  - 状态 → error；
  - **递归 error 所有子孙**；
  - 上报安全日志；
  - 父节点可选择 ignore 或 terminate。

---

### 1.3.4.2 安全异常处理：`warn` 与子树终止
<TODO>

---

## **1.3.5 硬件协同执行模型**

### **1.3.5.1 TLB 驱动的作用域访问控制**

在 EDSOS 中，内存安全与作用域隔离并非通过传统的、运行时的软件检查来实现，而是通过精心设计 **PTE（Page Table Entry）的权限位** 与 **MMU TLB 的层次化缓存行为**，将硬件的地址转换机制直接转化为作用域访问控制的执行引擎。其核心在于构建一种特定的 PTE 与 TLB 状态，使得 **合法的访问路径必然命中且通常高效，而非法的访问路径必然触发可被操作系统捕获的异常**。

#### **1.3.5.1.1 页表项（PTE）的二元状态设计**

EDSOS 的页表项被赋予两种截然不同的状态，这种状态直接映射了TS节点的作用域语义：

- **“作用域内”页（In-Scope Pages）**：
  - **权限**：PTE 被设置为 **可读/可写（或可执行）**。
  - **映射**：这些页对应于 **当前正在运行的TS节点自身** 及其 **所有直系祖先节点** 的数据段和指令段。
  - **目的**：允许节点无缝、高效地访问其作用域内的所有数据。

- **“作用域外”页（Out-of-Scope Pages）**：
  - **权限**：PTE 被显式地标记为 **“不存在”（Present = 0）** 或 **“无访问权限”（如 User=0, R/W=0）**。
  - **映射**：这些页对应于系统中**除当前节点及其祖先之外的所有其他内存**，包括兄弟节点、子孙节点、其他TS的节点等。
  - **目的**：任何对这些页的访问尝试，都将 **必然触发 Page Fault 异常**，从而被操作系统内核（Fault Handler）捕获并处理。

> 通过这种二元化的 PTE 状态设计，页表本身即成为作用域边界的 **静态、权威定义**。硬件 MMU 在进行地址转换时，自动遵循此定义。

#### **1.3.5.1.2 TLB 层次结构与作用域语义的精准对齐**

EDSOS 利用现代CPU的 **两级TLB（L1和L2）** 的不同特性，与TS树的层次结构进行精准对齐，以实现性能与安全的统一：

- **L1 TLB：当前节点的“安全缓存”**
  - **容量**：L1 TLB 容量较小（通常仅能容纳数十个条目）。
  - **映射内容**：在节点被调度执行前，其 **自身** 数据和指令所在内存页的所有条目会被 **主动预热** 至 L1 TLB。
  - **效果**：节点访问自身数据（如局部变量、函数代码）时，**100% L1 TLB命中**。这不仅带来了极致的性能（纳秒级访问）；更重要的是由于 L1 TLB 中 **只包含自身页**，任何试图通过 L1 TLB 访问非自身数据的行为在物理上就不可能发生，构成了第一道安全屏障。

- **L2 TLB：祖先链的“作用域缓存”**
  - **容量**：L2 TLB 容量远大于 L1（通常可容纳数百至上千个条目）。
  - **映射内容**：在节点被调度执行前，其 **所有直系祖先节点** 的“作用域内”页的翻译条目会被 **主动预热** 至 L2 TLB。
  - **效果**：当节点访问祖先数据（如全局配置、父上下文）时，会发生 **L1 TLB Miss**，但会紧接着在 **L2 TLB Hit**。整个过程 **不会触发 Page Walk**，保证了访问的合法性与高效性。L2 TLB 的大容量使其通常足以容纳整条祖先链，从而将作用域安全边界自然地扩展到继承链。
  - **路径可靠性**：虽然 L2 TLB Hit 之后硬件会自动将这个 PTE 缓存填入 L1 TLB，导致一个在 L1 TLB 中的节点内部页被挤出，但这个内部页的 PTE 通常也会在 L2 TLB 中缓存，再次访问这个内部页时仍会在 L2 TLB Hit。

> 这里描述的是理想情况，其实际的操作过程和达成结果见 §1.3.5.3.3 预热的顺序与策略。

#### **1.3.5.1.3 异常路径：Page Fault 作为安全哨兵**

- 当一个访问请求既未命中L1 TLB，也未命中L2 TLB时，硬件MMU将启动 **Page Walk** 过程，遍历页表以查找对应的 PTE：
  - **合法但未预热的冷数据**：
    - 如果该地址属于“作用域内”页（PTE 有效），Page Walk 会成功找到 PTE，并将新条目填入 TLB。这是一个 **容错路径**，虽然会带来性能抖动，但访问是被允许的。
  - **非法的越界访问**：
    - 如果该地址属于“作用域外”页（PTE 被标记为无效），Page Walk 会发现一个无效的 PTE，硬件将立即 **触发 Page Fault 异常**。
    - **OS行为**：EDSOS 的 Fault Handler 会捕获此异常，检查故障地址。由于页表已明确标记了作用域边界，Handler 可以 **瞬间判定此次访问为非法**，并立即对当前 TS 节点（及其子树）执行 `warn` 操作，将其状态置为 `error` 并触发全局终止。这个过程开销极低，因为判断逻辑极其简单。

> 因此，**Page Fault 在 EDSOS 中不再是模糊的“内存错误”，而是清晰、精准的“作用域越界”信号**。

#### **1.3.5.1.4 边界情况与兼容性：节点过大的优雅退化**

上述理想模型依赖于一个关键前提：**单个TS节点的大小 + 其整条祖先链的大小，必须能被 L1 + L2 TLB 的总容量所容纳**。EDSOS 对此有明确的工程约束和优雅的退化策略：

- **约束**：EDSOS 开发者手册会强烈建议原生代码控制单个节点的大小，确保其在绝大多数场景下能满足TLB容量要求；EDSOS 标准的编译器扩展也会遵循这一点。
- **退化场景**：
  - **节点自身过大**：如果单个节点的页数超过了 L1 TLB 容量，其部分页将无法被预热到 L1。访问这些“冷”页时会发生 **L1 Miss + L2 Hit**（如果祖先链不长，L2仍有空间）或 **L1/L2 Miss + Page Walk**。性能会下降，但只要 PTE 状态正确就可以成功访问，**安全性不受影响**。
  - **祖先链过长**：如果祖先链总大小超过了 L2 TLB 容量，部分祖先页无法被预热。访问这些页同样会触发 Page Walk。为了保证 **功能正确性**，这些页的 PTE **必须保持有效**，以避免合法的祖先访问被误判为非法。此时，性能会退化到与传统操作系统相当的水平（依赖 Page Walk 和 DRAM 访问），**失去确定性高性能的优势，但依然保持内存安全**。

> 这种设计确保了 EDSOS 模型的 **鲁棒性**：在满足约束的理想情况下，它能提供硬件级的、确定性的高性能与安全；在其他情况下，它也能优雅地退化，保证功能正确和基本安全。

---

### 1.3.5.2 PCID 作为祖链指纹
<TODO>
- TLB 刷新最小化
- 结构变更的缓存一致性

### **1.3.5.3 调度预热机制**

在 EDSOS 的硬件协同执行模型中，调度器不仅是执行流的仲裁者，更是 TLB 状态的主动管理者。每一次上下文切换都伴随着一次精心编排的 **TLB 预热（TLB Preheating）** 过程，目标是在新节点开始执行前都尽可能达到上一节的理想情况。

#### **1.3.5.3.1 预热目标的确定：基于最小公共祖先（LCA）的增量计算**

调度器并非每次都从零开始遍历整棵 TS 树来确定预热范围，而是通过 **增量比较** 新旧执行节点的祖先链，找出 **最小公共祖先（Lowest Common Ancestor, LCA）**，从而仅对发生变化的部分进行 TLB 状态更新。

- **RNDAC 优化机制**：每个 CPU 核心的调度器维护一个轻量级缓存 **Running Node Direct Ancestors Cache (RNDAC)**。它是一个有序数组，按从 **根到当前运行节点** 的顺序，存储当前正在执行节点的 **所有直系祖先节点指针**（包括自身）。
  - **缓存信息**：当一个节点 `N_old` 被调度出时，调度器的 RNDAC 即缓存了 `N_old` 的完整祖先链。
  - **快速 LCA 查找**：当调度器决定切换到新节点 `N_new` 时，它从 `N_new` 开始向上递归其父节点。对于每个访问到的祖先 `A`，调度器检查 `A` 是否存在于当前的 RNDAC 中。
    - 第一个命中的 `A` 即为 `N_old` 与 `N_new` 的 **LCA**。
    - 由于 RNDAC 是有序的，可以使用哈希表或直接线性扫描（因链通常不长）实现快速查找。(<TODO>分析、研究这里最高效的匹配算法)
  - **增量更新**：一旦找到 LCA，调度器即可：
    - 保留 RNDAC 中从根到 LCA 的部分；
    - 将 `N_new` 从 LCA 的子节点开始，一直到 `N_new` 自身的路径，**替换原有项** 或 **追加到 RNDAC 末尾**。
    - 此过程仅需 O(D) 时间，其中 D 是 `N_new` 到 LCA 的距离。由于调度器会进行亲和性调度，因此 D 通常很小。
  -  通过 RNDAC，调度器能以极低的开销确定预热范围：**需要预热的页面 = (N_new 到 LCA 的路径上的所有节点) + (N_new 自身)**。

> 此外，当直系祖链上发生了节点结构变化或拓扑变化（如一个节点被 lift 到了直系祖链范围内），调度器会主动触发 pause 以使得受影响的节点可以重新构建执行环境，并且会主动修改 RNDAC 的对应部分（因为此时的 RNDAC 正是对应于当前的 Running 节点，可以直接确定需要修改的项）。

#### **1.3.5.3.2 页表项（PTE）的同步更新**

- 在预热 TLB 之前，调度器必须确保页表的状态与新的作用域边界完全一致：
  - **冻结旧作用域**：确保所有无关 PTE 处于无效（Present=0）；
  - **激活新作用域**：遍历 `N_new` 及其到根的完整祖先链，将这些节点所映射的所有页的 PTE **设置为有效（Present=1, 权限正确）**。
- 此步骤确保了：**任何在预热后发生的内存访问，其合法性完全由页表状态决定**，为后续的 TLB 预热和安全异常处理奠定了基础。

#### **1.3.5.3.3 预热的顺序与策略**

- 预热顺序直接影响 TLB 的缓存局部性。EDSOS 采用 **“从根到叶”** 的顺序进行预热：
  - **从 LCA 的子节点开始，向下至 `N_new`**，依次预热 LCA 之后路径上的每一个节点。
- 现代 CPU 的 TLB 通常采用伪 LRU 等替换策略。后预热的条目在 TLB 中“更新鲜”，更不易被替换。程序最频繁访问的是**当前节点自身**的数据，其次是**直接父节点**。将它们最后预热，能最大程度保证它们在容量有限的 L1 TLB 中驻留。

> 这个流程可以 *尽可能* 实现理想的 TLB 内容布局，但不能也不会强求严格保证精确对应。

#### **1.3.5.3.4 预热的实现方式：异步、非阻塞的硬件触发**

- 调度器对需要预热的每个页面，**执行一次虚拟地址的读取操作**（例如，读取该页的第一个字节）。
- 此读取会触发 MMU 的地址转换。由于 PTE 已被设为有效，硬件会成功完成 Page Walk（如果 TLB 中尚无条目），并将新的翻译条目 **安装到 L1/L2 TLB 中**。
- 调度器**不需要、也不应该等待**该页的数据从内存（或缓存）中实际加载完成。
- **优化**：可以通过预取（prefetch）指令来进一步提示硬件，并且利用硬件的自动 prefetch 来减少需要显式触发的读指令量。

---

## **1.3.6 同步机制**

### **1.3.6.1 EDSOS Event：执行顺序屏障**

在 EDSOS 中，**EDSOS Event** 并非传统操作系统中的内核对象（如 Windows 的 Event HANDLE 或 Linux 的 futex），而是一种**纯粹的概念性执行顺序屏障**。其核心设计目标是将并发同步从“共享状态上的竞争”转变为“由调度器驱动的、显式的执行流编排”。这一机制完全契合 EDSOS “调度器驱动”和“无隐式共享”的核心原则。

#### **1.3.6.1.1 本质：无实体的调度器提示信息**

EDSOS Event 的本质可以概括为以下几点：

- **无实体性**：Event 本身**不占用任何内存**，没有内核数据结构，也没有用户态的句柄（HANDLE）。它仅仅是一个由**名称**（`event_name`）唯一标识的逻辑概念。
- **存在性由调度器维护**：一个 Event 仅在至少有一个节点在等待它时，才在**某个调度器的 Wait Graph 中拥有一个逻辑槽位**。一旦所有等待者都被唤醒且没有新的等待者，该槽位即被回收，Event 在系统中“消失”。
- **作用是屏障**：Event 的唯一作用是作为一个**执行顺序的屏障**。调用 `wait(event_name)` 的节点会主动放弃 CPU，并将自己注册到该 Event 的等待队列中，其执行流在此处暂停。只有当另一个执行上下文调用 `signal(event_name)` 时，调度器才会将等待者从屏障后释放，使其重新进入就绪状态。
- **生命周期自动管理**：Event 的生命周期完全由其使用情况动态决定，无需显式的创建（`CreateEvent`）或销毁（`CloseHandle`）操作，极大地简化了编程模型并避免了资源泄漏。

#### **1.3.6.1.2 同核同步：O(1) 的调度器内操作**

当 `wait` 和 `signal` 操作发生在**同一个 CPU 核心**上时，EDSOS Event 的处理是极其高效的，其开销几乎为零。

- **核心数据结构**：每个 Per-Core 调度器维护一个 **Wait Graph**（详见 §1.3.3.3 就绪结构：Distributed Ready Forest）。

- **`wait` 操作流程**：
  - 节点执行 `edsos_wait(event_name)`。
  - 调度器桩函数将当前节点的状态从 `running` 改为 `block`。
  - 调度器在本地 Wait Graph 中查找或创建对应 `event_name` 的槽位，并将该节点加入其 FIFO 队列。
  - 调度器执行调度主逻辑，从 Ready Linked List 中选择下一个就绪节点执行。

- **`signal` 操作流程**：
  - 节点执行 `edsos_signal(event_name)`。
  - 调度器桩函数在本地 Wait Graph 中查找 `event_name` 对应的队列。
  - 如果队列非空，调度器将队列头部（或全部，取决于语义）的节点 **逐个插入本地 Ready Linked List**，并将其状态改为 `ready`。

> 这种同核处理模式确保了在最常见的同步场景下，EDSOS 能提供确定性的、极低延迟的同步性能。

#### **1.3.6.1.3 跨核同步：协商式唤醒（Negotiated Wakeup）**

当 `signal` 操作发生在与 `wait` 节点**不同的 CPU 核心**上时，情况变得复杂。EDSOS 采用一种**去中心化的协商式唤醒**机制来解决此问题，避免了全局锁或集中式事件管理器。

- **QUERY/RESPONSE/GRANT 三步协商流程**：
  - **QUERY（查询）**：Giver Core 的调度器通过 **链式消息总线** 广播一条 `QUERY(event_name)` 消息。
  - **RESPONSE（响应）**：每个收到 `QUERY` 消息的调度器会检查自己的本地 Wait Graph。如果发现有该 Event 的等待者，它会回复一条 `RESPONSE` 消息。该消息包含：
    - 本 Core 的 ID。
    - 一个用于排序的时间戳（通常基于 `RDTSC`），以近似实现跨核的 FIFO 公平性。
    - 等待者队列的长度等元数据。
  - **GRANT（授权）**：Giver Core 收集所有 `RESPONSE` 后，根据预定义策略（如选择时间戳最早的核心以保证公平性）**选出一个获胜者**（Winner Core），然后向获胜者发送 `GRANT(event_name)` 消息。
  - **本地唤醒**：Winner Core 收到 `GRANT` 消息后，在其**本地**执行与同核 `signal` 完全相同的操作：将等待者从 Wait Graph 移至 Ready Linked List。

- **关键优势**：
  - **去中心化**：没有单点瓶颈。
  - **按需通信**：只有存在等待者的 Core 才会参与协商，通信开销与实际需求成正比。
  - **近似 FIFO**：通过 `(core_id, rdtsc_tsc)` 对进行全序排序，可以在跨核场景下提供可接受的公平性保证，不严格要求全局同步。不同物理核心的 TSC 由硬件互联层的操作进行校准。
  - **与调度器深度集成**：整个协商过程通过链式消息总线异步完成，并由调度器的 `check()` 机制定期处理（详见 §1.3.3.4 链式消息总线），不会阻塞正常的调度流程。

---

### 1.3.6.2 标准同步接口的实现
<TODO>
- Mutex / RWLock / Semaphore / Condition Variable 等
- 用户态状态 + Event 同步分离

---

## **1.3.7 系统调用模型**

### **1.3.7.1 第一类 syscall 架构**

#### **1.3.7.1.1 定义与核心特征**

第一类 syscall 是 **由调度器在本地 CPU 核心上直接、同步地完成处理**，且操作的影响范围限定在 **当前 TS 节点自身、其子树内部，或仅涉及只读的元数据查询** 的系统调用。

第一类 syscall 由进程代码按照桩函数形式调用，由 EDSOS Loader 在加载期静态内联展开为机器指令，直接操作调度器本地数据结构（如 Ready Forest, Wait Graph）或读取本地元数据。

#### **1.3.7.1.2 接口分类与具体实现**

第一类 syscall 可分为两大类：**属于调度器 NOI（Normal Open Interface）的** 和 **本地元数据查询接口**。

##### **1.3.7.1.2.1 调度器 NOI 中可被用户代码直接调用的接口**

这些接口是用户代码 **主动控制自身执行流和结构** 的主要手段。

- **节点操作 (Node Operations)**：
  - 即 §1.3.4 中定义的节点状态机操作。
- **事件操作 (Event Operations)**：
  - 即 §1.3.6 中定义的事件操作。
- **调度请求 (Scheduling Hints & Control)**：
  - **主动让出/延长**：`edsos_yield()` (触发立即调度) / `edsos_extend_quota(...)` (请求延长当前时间片，调度器根据策略决定是否批准)。
  - **CPU 亲和性与隔离**：`edsos_set_affinity(core_mask)` / `edsos_request_isolation()`。这些调用**不直接绑定**，而是向调度器的负载均衡模块（Work-Stealing++）提供**策略权重提示**，可能影响子树迁移决策。
  - **优先级提示**：`edsos_set_priority_hint(level)`。影响节点在 Ready Linked List 中的**智能顺位插入**位置（见 §1.3.3.3.1）。
- **时钟请求 (Clock Access)**：
  - **低精度时钟**：`edsos_get_coarse_time()`。返回一个模糊化、低分辨率的时钟值，用于非精确计时。
  - **不信任高精度时钟**：`edsos_get_untrusted_time()`。返回基于调度器记录的节点执行时间，并经过**模糊化和偏移处理**的值，旨在**防御时序侧信道攻击**。
  - **信任高精度时钟**：`edsos_get_trusted_time()`。**仅当进程的 Capability 政策允许**时，Loader 才会将此桩函数内联为直接读取硬件 TSC 的指令，否则会默认内联成为不信任高精度时钟。
- **性能查询 (Performance Profiling)**：
  - `edsos_get_execution_time()`: 返回调度器为当前节点记录的**累计 CPU 执行时间**。
  - `edsos_get_cache/TLB_miss_stats()`: 返回由硬件性能计数器（PMC）采样并由调度器聚合的**缓存/TLB Miss 估计值**。这些数据由调度器在上下文切换时采样获得。

##### **1.3.7.1.2.2 本地元数据查询接口（非 NOI，但属第一类）**

这些接口不改变调度器状态，仅用于 **查询** 根节点或 KPN 中的 **应用只读信息**。

- **硬件信息查询 (Hardware Info)**：
  - 查询记录在 KPN 中的硬件注册信息。此数据由 Capability 政策决定，并可在运行时随符合政策的硬件连接/断开而即时更新。此信息 **可能被根权限篡改**，但系统会同时提供一个 `is_info_tampered` 标志位告知应用程序。
- **进程与线程信息查询 (Process/Thread Info)**：
  - `edsos_get_ts_id()`: 返回当前 TS 的全局唯一标识符（PID），该值存储在根节点中。
  - 在兼容模式下，为模拟传统线程模型，`edsos_get_thread_id()` 会返回其所属 TS 的 ID，因为 EDSOS 中不存在独立的“线程”概念。
- **权限查询 (Permission/Capability Query)**：
  - 查询存储在 KPN 中的 **系统服务访问入口**。例如：`edsos_can_access_network()`。

---

### **1.3.7.2 第二类 syscall 架构**

**第二类 syscall** 构成了 EDSOS 系统调用模型中处理 **复杂、敏感、耗时或跨作用域操作** 的核心机制。与第一类 syscall 的本地、同步特性不同，第二类 syscall 采用 **异步委托** 模式，将请求交由专用的 **系统服务组件 TS** 处理，并通过结构化的共享通道 KPN 实现安全、高效的数据交换。这类调用涵盖了除第一类之外的所有系统功能，是 EDSOS 实现完整操作系统能力的关键。

#### **1.3.7.2.1 第二类 syscall 的范围界定**

第二类 syscall 的判定标准基于 **执行语义与安全影响**：

- **典型场景**：
  - 所有 **I/O 操作**（文件读写、网络通信、设备控制）。
  - **资源创建与管理**（如创建 CTRN 和全局 Event 对象）。
  - **高权限或全局状态操作**（如修改系统时间、加载动态模块）。
  - **需要复杂协议或状态机处理**的操作（如数据库事务）。
- **兼容模式下的特殊处理**：
  - 在启用 **POSIX/Win32 兼容模式** 且配置为 **弱安全选项** 时，某些本可映射为第一类的调用（如 `getpid()`、`clock_gettime()`）会被 **强制降级为第二类**。
  - 目的：确保兼容层的行为与传统 OS 完全一致，避免因 EDSOS 的语义差异（如模糊时钟、TS ID 代替 PID）导致遗留应用逻辑错误。即使存在对应的 EDSOS Standard ABI 桩函数，Loader 也不会进行内联替换。

简言之，**任何“希望 OS 代表我做某件事并返回结果”的请求，都属于第二类 syscall**。

> 此外，第二类 syscall 的执行流程也可以用于应用 TS 之间的处理协作，可以通过向注册表服务组件来注册相关接口及其访问的安全检查回调，来使得接口被其他应用 TS 可见，但需对方主动查询和注入，不会主动在其注册时注入其他应用 TS 的 KPN。

#### **1.3.7.2.2 利用 KPN 实现高效信道**

##### **1.3.7.2.2.1 KPN 的定义与作用**

KPN（Kernel Proxy Node） 是一个 **可注入指令段** 的特殊 CTRN，其主要特征包括：

- **参数与返回值缓冲区**：为第二类 syscall 提供一个调用者 TS 与系统服务组件共享的、结构化的内存区域，用于传递调用参数、接收返回结果。
- **事件协调器**：关联一对同步 Event（请求 Event 与响应 Event），用于实现调用者与服务组件之间的阻塞-唤醒语义。
- **桩函数载体**：在 TS 加载期，EDSOS Loader 会根据 Manifest 中声明的 Capability 策略，**将第二类 syscall 的桩函数逻辑内联展开并注入 KPN 的指令段中**。应用代码对 syscall 的调用被展开为对 KPN 指令段的直接跳转。

##### **1.3.7.2.2.2 KPN 的挂载拓扑**

KPN 的挂载位置严格遵循 EDSOS 的局部性原则：

- 每个 TS 在每个 CPU Core 上拥有的**本地子树根**（Local Subtree Root, LSR）——无论是 TS 真根还是由调度器创建的代理根节点（Proxy Root Node）——均会挂载一个 KPN 作为其 **直接子节点**。
- 该 KPN 的 **作用域** 被限定为 **仅对本地子树节点可见**。这意味着，分配到不同 Core 的同 TS 节点，访问的是不同的 KPN 实例，从根本上消除了跨核并发访问冲突。

##### **1.3.7.2.2.3 KPN 的使用方式**

KPN 对应用开发者完全透明：

- 在编译期，应用代码调用标准 syscall 接口。
- 在加载期，EDSOS Loader 根据 Capability 策略，**注入应被 TS 可见的系统服务组件的调用格式布局指令，并将对应的桩函数逻辑内联展开为到对应指令段的跳转**。例如，查看相关寄存器值是否合法，然后根据对应系统服务组件的内部接口约定在 KPN 数据段相应部分按须写入所需值。
- 运行时，调用者节点执行该内联代码，写入参数、signal 请求 Event、wait 响应 Event，整个过程表现为一次“看似同步”的函数调用，实则为高效的事件驱动异步交互。

##### **1.3.7.2.2.4 KPN 的安全子节点与数据访问机制**

对于涉及**大块数据**（如文件内容、网络包、GPU 纹理） 的第二类 syscall，KPN 本身不直接存储数据，而是通过 **安全子节点**（Secure Child Node） 机制进行扩展：

- **安全子节点** 是由系统服务组件 TS 创建的、挂载于 KPN 下的特殊 CTRN。它们同样 **允许注入指令段**，用于实现数据处理逻辑。
- 安全子节点 **不在调用者节点的直系祖链上**，因此不可被直接访问，具备隔离性。
- 为高效访问安全子节点中的数据，EDSOS 提供两种受控机制：
  1.  **只读映射窗口**（Read-Only Mapping Window）：调度器在构建调用者页表时，将安全子节点的数据页以 **只读权限** 当成 KPN 的页面预热，调用者可直接读取；
  2.  **SDEC 安全访问指令**（Scheduler-Driven Event Closure）：服务组件可将对安全子节点的 **读写操作封装为 SDEC**。当调用者触发访问请求 Event 时，调度器直接执行该 SDEC，由其完成数据拷贝或更新。此方式 **同时支持安全读与安全写**，但服务组件需自行处理跨 TS 的并发竞态。SDEC 详见 §1.3.3.5 面向调度器的事件闭包。

#### **1.3.7.2.3 执行流程：从调用到返回的完整生命周期**

第二类 syscall 的执行是一个典型的**异步生产者-消费者**模型，由调度器和 EDSOS Event 协同驱动：

- **用户端发起调用**：
  - 用户代码调用一个第二类 syscall 桩函数（例如 `edsos_file_read(fd, buf, size)`）。
  - **加载期展开**：EDSOS Loader 已将此桩函数展开为一段 **写入 KPN 特定槽位** 的指令序列。调用时，调用参数信息被写入 KPN 的预定义缓冲区中。
  - **自动阻塞**：桩函数的最后一步是调用 `edsos_wait(kpn_event)`，将当前用户节点置为 **`block`** 状态，并加入 KPN 关联的 Wait Graph。

- **触发服务端处理**：
  - 桩函数在阻塞前，会 **自动触发一个调度器 NOI**（如 `edsos_signal(service_event)` 或通过链式消息总线发送通知）。
  - 这个信号会 **唤醒** 对应的系统服务组件 TS。

- **服务端处理与响应**：
  - 当服务组件 TS 在其所属 CPU Core 上被调度执行时，它会 **读取 KPN 对应内容**，解析其中的请求参数。
  - 服务组件执行实际操作逻辑。
  - 服务组件返回数据：
    - **返回小数据**：若返回数据量小（如错误码、文件大小），直接写入 KPN 的**返回值槽位**。
    - **返回大数据**：
      - 服务组件 `push` 一个新的 CTRN 子节点， **同时包含指令段和数据段**。
      - 该子节点被 **共享挂载为 KPN 的子节点**。
      - 服务组件将**关键元数据**（如数据长度、子节点句柄）写入 KPN 的返回槽位。

- **用户端唤醒与数据消费**：
  - 服务组件在完成处理后，调用 `edsos_signal(kpn_event)`，**唤醒** 之前阻塞的用户节点。
  - 用户节点恢复执行，从 KPN 的返回槽位读取结果。

#### **1.3.7.2.4 安全与性能优势**

- **安全**：通过 KPN 的结构化设计实现了 **最小权限原则**。用户代码无法越权访问服务组件的内部状态，服务组件也无法随意篡改用户内存。
- **性能**：**零拷贝** 的数据传递、**异步非阻塞** 的执行模型、以及 **局部性优化**，使得第二类 syscall 的性能远超传统 *陷入-返回* 系统调用模型。
- **可组合性**：KPN 作为标准化接口，使得系统服务可以像微服务一样独立开发、部署和更新，提升了系统的模块化和可维护性。

---

### 1.3.7.3 兼容模式支持（POSIX / Win32 / Native）

<TODO>

---

## **1.3.8 负载均衡**

### **1.3.8.1 负载均衡的策略与触发**

#### **1.3.8.1.1 Work-Stealing++**

EDSOS 的负载均衡机制称为 **Work-Stealing++**，它是对传统 Work-Stealing 的深度扩展。
核心思想是：**均衡的目标不是“每个 Core 的就绪节点数相等”，而是“最大化结构亲和、最小化上下文切换与跨 Core 通信”**。
该机制完全去中心化，依赖链式消息总线实现 Core 间协商，以 **TS 子树为迁移单位**，并引入 **代理节点缓存** 降低调度开销。

#### **1.3.8.1.2 触发跨 Core 调度的典型场景**

调度器持续监控本地负载状态，以下情况将触发 Work-Stealing++：

- **子树规模失衡**：  
  某 TS 子树节点数超过阈值（如 1024），导致单 Core 调度延迟上升；
- **TS 分散度过高**：  
  同一 TS 的子树分布在 ≥3 个 Core 上，引发频繁跨 Core `signal` 与 TLB 冷启动；
- **异常负载波动**：  
  通过链式消息总线或 IPI 收到邻居 Core 的 **过载告警**（如队列长度 > HIGH_WATERMARK）；
- **性能敏感 TS 争用**：  
  多个高优先级 TS（如云游戏主逻辑、实时音视频）共享同一 Core，导致调度抖动。

> 注意：EDSOS **不追求绝对负载均匀**。若一个 Core 运行 10 个低优先级 TS，另一个运行 1 个高优先级 TS，系统可能认为这是“良好均衡”。

---

### **1.3.8.2 去中心化协商：基于链式消息总线的请求-响应**

Work-Stealing++ 采用 **主动协商模型**，无全局协调者。

#### **1.3.8.2.1 触发主动请求的情况**

- **过载 Core**：  
  当 `ready_queue_length > HIGH_WATERMARK`，广播 `TASK_GIVE_REQUEST` 消息，附带：
  - 可迁移子树列表（按亲和性排序）；
  - 性能敏感度标签。

- **空闲 Core**：  
  当 `ready_queue_length < LOW_WATERMARK` 且未收到 `TASK_GIVE_REQUEST` 消息，广播 `TASK_GET_REQUEST` 消息，附带：
  - 自身空闲容量；
  - 优先提供的 TS 类型。

#### **1.3.8.2.2 响应和迁移触发**

- **主动响应**：
  收到 `STEAL_REQUEST` 后，如果认为是自己可以接收的，回复 `STEAL_OFFER`，包含：
  - 自身空闲容量；
  - 自身当前调度的 TS 数量；
  - 其他用于说明“为什么选我”的信息。

- **决策与迁移**：  
  Giver 收集所有 `STEAL_OFFER`，按以下策略选择 Taker：
  1. 优先选择 **同 NUMA 域** 的 Core；
  2. 若为性能敏感 TS，选择 **当前 TS 数量少且无其他关键 TS** 的 Core；
  3. 否则选择 **空闲容量最大** 的 Core。  
  确定后，发送 `MIGRATE_SUBTREE(subtree_root_gva, target_core)` 指令。

> 整个过程通过链式消息总线异步完成，**不阻塞本地调度**。

---

### **1.3.8.3 子树级迁移**

#### **1.3.8.3.1 迁移保持结构完整性**

迁移单位必须是 **完整的 TS 子树**（以某节点为根的全部后代），不允许发生“断枝”（子孙节点转移出去而侄甥节点保留）的情况，原因如下：

- **TLB 亲和性**：子树共享祖先链，迁移后可在目标 Core 预热完整 TLB 上下文；
- **事件局部性**：若子树内多个节点等待同一事件，拆分将导致跨 Core 唤醒开销。

#### **1.3.8.3.2 迁移流程**

1. Giver 调度器 `pause` 整棵子树；
2. 通过链式消息总线传输子树的所有节点指针（LVA）；
3. 更新子树中所有节点的 `subtree_home_core`；
4. Taker 将子树加入自己的 Ready Linked List。

> **数据零拷贝**：节点本身不需复制，仅调度上下文迁移。

---

### **1.3.8.4 代理节点机制：减少跨 Core 通信**

为降低同一 TS 分散在多 Core 时的协调开销，EDSOS 引入两类 **代理节点缓存**：

#### **1.3.8.4.1 代理根节点（Proxy Root）**

当一个 Core 上运行的子树的上游位于其他 Core，缓存子树的祖先链（直至 TS 根）。

- 用于 TLB 预热、Capability 验证；
- 接收来自上游节点的结构变更消息。

#### **1.3.8.4.2 代理子节点（Proxy Child）**

当一个 Core 上运行的 TS 将一个子树拆分给其他 Core，缓存该 TS 在其他 Core 上的子树信息。

- 用于跨 Core `signal` 路由；
- 接收来自下游节点的结构变更消息。

> 代理机制不仅用于跨 PM，也用于同一 CPU 的不同 Core，避免每次祖先链预热和事件触发都需全局查询。

---

### **1.3.8.5 TS 感知的均衡策略**

EDSOS 将 TS 分为两类，采用差异化调度策略：

| TS 类型 | 示例 | 调度策略 |
|--------|------|----------|
| **性能敏感型** | 云游戏主逻辑、实时控制 | 尽量独占 Core，避免与其他 TS 共享 |
| **性能不敏感型** | Capability 鉴权、注册表服务 | 多个可共用同一 Core，主动承担上下文切换开销 |

调度器通过节点中的 `perf_sensitivity` 标签识别类型，并在 Work-Stealing++ 决策中加权。

---

## **1.3.9 工程特性与范式总结**
- 性能：百万级节点、百纳秒调度、零拷贝 IPC
- 安全：作用域隔离、Capability 固化、无用户态竞态
- 可扩展：同核 → 多核 → 跨 CPU → 跨 PM 无缝扩展
- 调试性：全状态可见（Wait Graph、CTRN 拓扑、消息流）
- 范式创新：
  - 并发 = 执行顺序的显式编排
  - 进程 = 动态演化的 TS 实例
  - 操作系统 = 超TS 结构的活体系统
