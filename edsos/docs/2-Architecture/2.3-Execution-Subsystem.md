<!--
SPDX-FileCopyrightText: © 2025-2026 Bib Guake
SPDX-License-Identifier: LGPL-3.0-or-later
-->

# 2.3 Execution Subsystem

---

> v0.1.3

---

- [2.3 Execution Subsystem](#23-execution-subsystem)
  - [2.3.0 Design Vision and Core Principles](#230-design-vision-and-core-principles)
  - [2.3.1 Execution Model Implementation](#231-execution-model-implementation)
    - [2.3.1.1 Arbor Strux (AS) Representation](#2311-arbor-strux-as-representation)
      - [2.3.1.1.1 AS Construction: Role of the EDSOS Loader](#23111-as-construction-role-of-the-edsos-loader)
      - [2.3.1.1.2 AS Lifecycle Management](#23112-as-lifecycle-management)
      - [2.3.1.1.3 AS Instance Identification and Naming](#23113-as-instance-identification-and-naming)
    - [2.3.1.2 Cross-AS Communication via CARN](#2312-cross-as-communication-via-carn)
      - [2.3.1.2.1 CARN Definition](#23121-carn-definition)
      - [2.3.1.2.2 CARN Operational Workflow](#23122-carn-operational-workflow)
    - [2.3.1.3 System Boot and Initialization: Building the Meta-AS from Local to Global](#2313-system-boot-and-initialization-building-the-meta-as-from-local-to-global)
      - [2.3.1.3.1 Boot Constraints and Address Space Design](#23131-boot-constraints-and-address-space-design)
      - [2.3.1.3.2 Phase 1: EDSOS Early Local View](#23132-phase-1-edsos-early-local-view)
      - [2.3.1.3.3 Phase 2: Global Meta-AS Construction](#23133-phase-2-global-meta-as-construction)
      - [2.3.1.3.4 Additional Notes](#23134-additional-notes)
  - [2.3.2 Node Scheduler Architecture](#232-node-scheduler-architecture)
    - [2.3.2.1 Per-Core Distributed Scheduler](#2321-per-core-distributed-scheduler)
      - [2.3.2.1.1 Scheduler Composition](#23211-scheduler-composition)
      - [2.3.2.1.2 Decentralized Architecture](#23212-decentralized-architecture)
      - [2.3.2.1.3 Core Scheduler Responsibilities](#23213-core-scheduler-responsibilities)
    - [2.3.2.2 Scheduling Trigger Mechanisms](#2322-scheduling-trigger-mechanisms)
      - [2.3.2.2.1 Cooperative Scheduling: Stub-Driven](#23221-cooperative-scheduling-stub-driven)
        - [(1) Node Boundary Stubs (Entry/Exit)](#1-node-boundary-stubs-entryexit)
        - [(2) State-Mutation Stubs](#2-state-mutation-stubs)
      - [2.3.2.2.2 Preemptive Scheduling: Hardware Interrupt-Driven](#23222-preemptive-scheduling-hardware-interrupt-driven)
      - [2.3.2.2.3 Chained Message Bus Coordination Requirement](#23223-chained-message-bus-coordination-requirement)
    - [2.3.2.3 Ready Structure: Distributed Ready Forest](#2323-ready-structure-distributed-ready-forest)
      - [2.3.2.3.1 Per-Core Ready Linked List](#23231-per-core-ready-linked-list)
      - [2.3.2.3.2 Per-Core Wait Graph](#23232-per-core-wait-graph)
    - [2.3.2.4 Chained Message Bus for Scheduler](#2324-chained-message-bus-for-scheduler)
      - [2.3.2.4.1 Chain-Based, Scalable Topology](#23241-chain-based-scalable-topology)
      - [2.3.2.4.2 Message Format](#23242-message-format)
      - [2.3.2.4.3 Implementation and Integration](#23243-implementation-and-integration)
  - [2.3.3 Synchronization Mechanisms](#233-synchronization-mechanisms)
    - [2.3.3.1 EDSOS Semaphore: An Execution Ordering Barrier](#2331-edsos-semaphore-an-execution-ordering-barrier)
      - [2.3.3.1.1 The Nature of an EDSOS Semaphore: A Scheduler Hint Without Physical Representation](#23311-the-nature-of-an-edsos-semaphore-a-scheduler-hint-without-physical-representation)
      - [2.3.3.1.2 Intra-Core Synchronization: O(1) Scheduler-Local Operations](#23312-intra-core-synchronization-o1-scheduler-local-operations)
      - [2.3.3.1.3 Inter-Core Synchronization: Negotiated Wakeup](#23313-inter-core-synchronization-negotiated-wakeup)
    - [2.3.3.2 Scheduler-Driven Semaphore Closure (SDSC)](#2332-scheduler-driven-semaphore-closure-sdsc)
      - [2.3.3.2.1 Definition and Lifecycle](#23321-definition-and-lifecycle)
      - [2.3.3.2.2 Memory Layout and Execution Model](#23322-memory-layout-and-execution-model)
      - [2.3.3.2.3 Security Constraints](#23323-security-constraints)
  - [2.3.4 Load Balancing](#234-load-balancing)
    - [2.3.4.1 Load-Balancing Policy and Trigger Conditions](#2341-load-balancing-policy-and-trigger-conditions)
    - [2.3.4.2 Decentralized Negotiation: Request–Response over the Chained Message Bus](#2342-decentralized-negotiation-requestresponse-over-the-chained-message-bus)
      - [2.3.4.2.1 Conditions for Initiating a Negotiation](#23421-conditions-for-initiating-a-negotiation)
      - [2.3.4.2.2 Response and Migration Decision](#23422-response-and-migration-decision)
    - [2.3.4.3 Subtree-Level Migration](#2343-subtree-level-migration)
    - [2.3.4.4 Proxy Node Mechanism: Reducing Cross-Core Coordination](#2344-proxy-node-mechanism-reducing-cross-core-coordination)
      - [2.3.4.4.1 Proxy Root Node](#23441-proxy-root-node)
      - [2.3.4.4.2 Proxy Child Node](#23442-proxy-child-node)
    - [2.3.4.5 AS-Aware Balancing Policies](#2345-as-aware-balancing-policies)


---

## 2.3.0 Design Vision and Core Principles

The Execution Subsystem embodies EDSOS’s structural approach to concurrency and distribution, centered on five principles:

- **No Implicit Sharing**: All inter-node communication occurs through explicit structural references (e.g., CARNs); shared state is never assumed.
- **Structure Determines Visibility**: A node’s access scope is defined solely by its position in the Arbor Strux tree—no global namespaces or ambient contexts.
- **Scheduler-Driven Execution**: The per-core scheduler is the sole authority for node lifecycle transitions (`push`, `pop`, `pivot`, `merge`) and migration decisions.
- **Decentralized, Locally Self-Organizing**: Load balancing, synchronization, and coordination emerge from peer-to-peer interactions via the chained message bus—no global coordinator.
- **Hardware-Cooperative Design**: Execution semantics align with modern hardware traits (e.g., PCID-based TLB isolation, NUMA topology), enabling zero-copy migration and affinity-aware scheduling.

Unlike traditional process/thread models, the Execution Subsystem is grounded in the formal Arbor Strux model, offering strong structural invariants, verifiable safety properties, and direct hardware affinity—making it both lower-level and more expressive for heterogeneous, distributed environments.

---

## 2.3.1 Execution Model Implementation

### 2.3.1.1 Arbor Strux (AS) Representation

#### 2.3.1.1.1 AS Construction: Role of the EDSOS Loader

The initial topology of an AS instance is constructed by the **EDSOS Loader** during the loading phase. The process proceeds as follows:

1. **ELF and Manifest Parsing**  
   The loader extracts the entry function, library dependencies, and declared capabilities from the binary manifest. It scans for system call instructions and inlines valid syscall stubs into native x86-64 sequences. Invalid syscalls are either rejected (default policy) or replaced with compatibility-layer shims in degraded mode. Library dependencies conform to the Arxil `.arxlib` specification.

2. **Virtual Address and Metadata Allocation**  
   The loader requests the Memory Subsystem to allocate contiguous **Local Virtual Addresses (LVAs)** for each node. A metadata segment (`.edsos_node_meta`) is generated per node, containing fields such as `vbase`, `size`, and `ancestor_path_hint`.

3. **Initial Tree Topology Assembly**  
   A root node is created and injected with its capability set and a view of accessible system resources. The main entry function is attached as a direct child of the root. Static constructs are organized into subtrees according to lexical scope relationships.

The output of the loader is a fully formed AS instance, rooted and ready for immediate scheduling.

#### 2.3.1.1.2 AS Lifecycle Management

In EDSOS, “process creation” is equivalent to constructing a new AS instance and integrating its root node into the logical structure of the Meta-AS.

**Creation proceeds as follows**:
1. A parent AS instance invokes `edsos_spawn()`;
2. The scheduler triggers the **EDSOS Loader**, which parses the target ELF binary and its associated Capability manifest;
3. The Loader constructs the initial AS instance topology;
4. The root node of the new AS instance is attached as a child to a designated management node (analogous to Unix process groups) and registered in the global namespace;
5. All nodes of the new AS instance are marked as `ready` and become eligible for scheduling.

**Destruction proceeds as follows**:
1. The user calls `exit()` or an unrecoverable exception occurs;
2. The scheduler recursively reclaims the entire AS instance;
3. When the reference count reaches zero, any associated **CARNs** (Cross-AS Referring Nodes) are automatically deallocated, ensuring no memory leaks.

The entire lifecycle is implicitly managed through AS structural references and reference counting—**no global process table is required**.

#### 2.3.1.1.3 AS Instance Identification and Naming

Each AS instance is assigned a globally unique **64-bit ASID** (Arbor Strux Identifier), stored in its root node. ASIDs are allocated by a dedicated **System Resource Management Service** and recorded in a distributed registry. Human-readable aliases may be bound to ASIDs (e.g., `/user/chrome_instance_01`).

Capability strategy and compatibility-mode metadata are embedded into the root node during loading and remain **immutable for the lifetime of the AS**.

---

### 2.3.1.2 Cross-AS Communication via CARN

EDSOS replaces traditional IPC with a structured shared-memory mechanism based on **CARNs (Cross-AS Referring Nodes)**. Crucially, CARNs provide only a **structured reference channel**; all concurrency control must be explicitly orchestrated using **EDSOS Semaphores** or other synchronization primitives.

#### 2.3.1.2.1 CARN Definition

A CARN is a degenerate AS node with special semantics, enabling explicit data sharing across multiple AS instances. Its key properties are:

- In each referencing AS instance, the CARN appears as an ordinary node and adheres to that AS’s structural constraints;
- No AS can traverse a CARN to access the direct ancestors of another AS’s CARN, preserving **lexical scoping and isolation**;
- CARNs carry no built-in concurrency semantics. Race conditions must be prevented via explicit synchronization such as EDSOS Semaphores.

#### 2.3.1.2.2 CARN Operational Workflow

- **Creation**: Node A executes `push(CARN_X)`, making CARN_X its child;
- **Registration**: CARN_X is published to a global naming table (implemented as a distributed hash table);
- **Scope Elevation (optional)**: A executes `pivot(CARN_X)` to promote it into an ancestor, enabling self-access;
- **Cross-AS Binding**:
  - Node B performs `lookup("CARN_X")` to obtain its virtual address;
  - B executes `push(CARN_X_VA, 'as_ctrn')` followed by `pivot(CARN_X)`, integrating the CARN into its own ancestry;
- **Disconnection**: Any participant may call `pop` on the CARN, decrementing its reference count. When the count reaches zero, the CARN is automatically reclaimed.

This mechanism guarantees structural integrity and memory safety in shared data access.

---

### 2.3.1.3 System Boot and Initialization: Building the Meta-AS from Local to Global

EDSOS boot is a **distributed, two-phase, structure-driven process** that constructs a unified Meta-AS spanning multiple physical machines (PMs). It achieves this while maintaining full compatibility with commodity hardware (CPUs, GPUs, chipsets), leveraging **dedicated DPUs** (Data Processing Units) for global coordination.

The process consists of two stages:  
1. **EDSOS Early Local View**  
2. **Global Meta-AS Construction**

#### 2.3.1.3.1 Boot Constraints and Address Space Design

EDSOS runs on unmodified commodity hardware, with DPU-assisted extensions for distribution. To reconcile these requirements, the system defines two virtual address spaces:

- **LVA (Local Virtual Address)**: Standard 64-bit virtual addresses used by CPUs and DMA-capable devices. Fully compatible with existing hardware ecosystems.
- **GVA (Global Virtual Address)**: 128-bit extended addresses **interpreted and routed exclusively by DPUs**. GVAs are transparent to general-purpose CPUs and enable uniform cross-PM resource access.

The memory subsystem (see §2.1) provides a well-formed LVA space for execution, while GVA resolution is delegated to DPU-managed infrastructure.

This design allows EDSOS to present a **globally coherent abstraction** without requiring hardware modifications.

#### 2.3.1.3.2 Phase 1: EDSOS Early Local View

Each PM first boots a **transient transitional image**—the *Early Local View*. This single-machine stage performs:

1. **Hardware Discovery**: Parses ACPI tables from UEFI/BIOS to construct a local physical resource tree (PCI devices, MMIO regions, interrupt lines, etc.);
2. **DPU Driver Initialization**: Loads and initializes the DPU driver as an AS instance;
3. **Declarative Configuration**: Reads system configuration from a designated path, determining roles, services, security policies, and cluster membership;
4. **Global Coordination Preparation**: Uses the DPU driver to communicate with peer DPUs and participate in distributed leader election.

> The term “local” here refers to the **hardware scope of a single PM**—what would traditionally be considered “global” in a monolithic OS, but is merely a subset in EDSOS semantics.

#### 2.3.1.3.3 Phase 2: Global Meta-AS Construction

Once the DPU fabric has assigned host identifiers and elected a Meta-AS root, the system transitions to the global phase:

1. **Global Resource Aggregation**: The System Resource Management Service collects local resource trees from all PMs via the DPU network and merges them into a **unified global hardware view**;
2. **Host ID Assignment**: Physical machine identifiers are allocated through a DPU-coordinated voting protocol. These IDs are used only for DPU routing without dependence on CPU;
3. **Driver AS Instantiation**: Device drivers are launched as independent AS instances and register for the System Resource Management Service.

#### 2.3.1.3.4 Additional Notes

The boot process deeply integrates core execution subsystem mechanisms:
- **Drivers as AS Instances**: Every driver runs as a standalone AS, aligning with microkernel principles;
- **Declarative Initialization**: Inspired by NixOS, system configuration is expressed declaratively, enabling reproducible, version-controlled initialization—not only for the whole system, but also for individual AS instances at launch time.

---

## 2.3.2 Node Scheduler Architecture

### 2.3.2.1 Per-Core Distributed Scheduler

The EDSOS scheduler is not implemented as a kernel thread or a standalone system process, as in conventional operating systems. Instead, it is a runtime mechanism distributed across every CPU core. It consists of **inline instruction sequences** and **core-local data structures**, occupies no Arbor Strux (AS) node, and is never itself a schedulable execution entity. The scheduler manifests as a set of tightly integrated software components operating on each physical core.

#### 2.3.2.1.1 Scheduler Composition

The EDSOS scheduler comprises two orthogonal components:

- **Inline Stubs**:  
  During AS instance loading, the EDSOS Loader statically inlines scheduler interfaces—such as `edsos_wait`, `edsos_push`, and `edsos_yield`—as immutable machine code at designated entry or exit points of user functions. These stubs directly manipulate the local scheduler state of the current core and cannot be bypassed or modified by user code.

- **Per-Core Local Data Structures**:  
  Each CPU core maintains an independent set of scheduling metadata, including:
  - A **Ready Linked List**: an MPSC (multi-producer, single-consumer) queue of ready AS nodes;
  - A **Wait Graph**: a mapping from event identifiers to lists of blocked nodes;
  - **PCID and TLB context caches** for efficient address-space switching;
  - **Load statistics**, such as ready queue length and subtree affinity hints.

Scheduler execution consists solely of atomic operations performed by stubs on these local data structures. Consistency is guaranteed through the memory model and the scheduler’s lock-free design.

#### 2.3.2.1.2 Decentralized Architecture

The EDSOS scheduler adopts a fully decentralized architecture, characterized by:

- No global ready queue, central dispatcher thread, or arbitration service;
- All scheduling decisions (except cross-core load balancing) are made locally on the executing core;
- Operations such as node creation (`push`), blocking (`wait`), and wakeup (`signal`) affect only the local core’s data structures;
- Cross-core coordination is achieved asynchronously via the **chained message bus**, without blocking the local scheduling path.

This design enables natural horizontal scalability: adding CPU cores linearly increases scheduling throughput, with no contention on global locks.

#### 2.3.2.1.3 Core Scheduler Responsibilities

The EDSOS scheduler is strictly limited to **execution flow control** and **AS structural maintenance**. Its responsibilities include:

- **Ready Structure Management**: Maintaining the ready linked list with support for FIFO ordering, head insertion (for high-priority services), and dependency-aware queuing. Scheduling order is determined at enqueue time; context switching is an O(1) operation.
- **Signal-Driven Synchronization**: Managing the wait graph to implement O(1) `wait(event)` and `signal(event)` semantics.
- **Node Activation and Context Switching**: Performing the `active` operation—reconstructing the ancestor chain, pre-warming the TLB, switching the CR3 register (with PCID), and jumping to the node’s entry point.
- **AS Structural Mutation**: Executing atomic operations such as `push`, `pop`, `pivot`, `merge`, and `detach`, ensuring both intermediate and final AS topology states remain well-defined and causally consistent.
- **Chained Message Bus Endpoint**: Acting as a message bus endpoint to trigger and process cross-core signals (e.g., migration requests, remote wakeups).

The scheduler contains no application logic. Tasks such as path resolution, capability validation, and global resource allocation are delegated to dedicated system-service AS instances (e.g., VFS, authentication service).

---

### 2.3.2.2 Scheduling Trigger Mechanisms

EDSOS scheduling is triggered through two orthogonal mechanisms: **cooperative scheduling** and **preemptive scheduling**. Both are realized via instruction sequences statically inlined by the EDSOS Loader during the loading phase. Although they differ in timing and context, they share the same underlying scheduling logic. Critically, **all scheduling paths integrate processing of the chained message bus** to preserve system-wide coordination under decentralization. Together, these interfaces constitute the scheduler’s **Normal Open Interface (NOI)**.

#### 2.3.2.2.1 Cooperative Scheduling: Stub-Driven

Cooperative scheduling is implemented by inlining stubs into user code during AS loading. It includes two categories:

##### (1) Node Boundary Stubs (Entry/Exit)

- **Trigger Timing**: Automatically invoked before node execution begins (entry) and after it completes (exit). These calls are implicit and cannot be skipped or directly invoked by user code.
- **Entry Stub Functions**:
  - Recursively pre-warm the TLB by accessing hot pages of the current node and its direct ancestors;
  - Initialize CPU time quota based on the node’s ASC deadline;
  - Restore register context (if resuming);
  - Invoke `check()` to process inbound messages from the chained message bus.
- **Exit Stub Functions**:
  - Save register state (if the node may resume later);
  - Notify the scheduler that the node has terminated;
  - Trigger the main scheduling logic:
    1. Select the next ready node from the head of the local ready list;
    2. Validate the integrity of its ancestor chain;
    3. Switch CR3 (including PCID);
    4. Jump to the selected node’s entry address;
  - Invoke `check()` to process inbound messages.

Node boundary stubs form the critical path for context switching, ensuring TLB pre-warming and cross-core message handling occur on every switch.

##### (2) State-Mutation Stubs

- **Trigger Timing**: Explicitly callable at any point within a node’s execution flow (e.g., via `edsos_wait()`, `edsos_push()`, or `edsos_pivot()`).
- **Behavior**: In addition to performing the requested operation, these stubs invoke `check()` to process the message bus.
- **Interface Classification**: These stubs are part of the scheduler’s NOI and are directly invocable by user code (see §2.3.4.1).

#### 2.3.2.2.2 Preemptive Scheduling: Hardware Interrupt-Driven

When cooperative scheduling is insufficient to guarantee fairness or real-time responsiveness, the scheduler intervenes via hardware interrupts.

- **Interrupt Sources** include:
  - Local timer interrupts (CPU time slice expiration);
  - Inter-processor interrupts (IPIs), such as load-balancing requests or remote wakeups;
  - TLB coherence interrupts (remote TLB invalidation requests);
  - Security exception interrupts (e.g., page fault validation failure, metadata corruption).

- **Interrupt Handling Flow**:
  1. The interrupt vector transfers control to the scheduler’s interrupt stub;
  2. The full context of the currently `running` node is saved;
  3. Actions are taken based on interrupt type:
     - **Time slice expiry**: Transition the current node to `ready` and re-enqueue it at the tail of the local ready list;
     - **IPI wakeup**: Move the target node from the wait graph to the ready list;
     - **Security exception**: Mark the node as `error` and recursively terminate its subtree;
  4. Invoke `check()` to process inbound messages;
  5. Trigger the main scheduling logic to switch to the next ready node.

Preemptive scheduling ensures liveness and safety even in the presence of uncooperative or faulty nodes.

#### 2.3.2.2.3 Chained Message Bus Coordination Requirement

All scheduling trigger paths—cooperative or preemptive—**mandatorily invoke `check()`**. This requirement arises because:

- The chained message bus employs a decentralized SPSC (single-producer, single-consumer) ring buffer per core pair;
- Message reliability depends on receivers frequently consuming their inbound queues;
- Prolonged omission of `check()` risks queue overflow and loss of cross-core coordination.

Thus, EDSOS deeply integrates message processing into every scheduling path, ensuring timely execution of load balancing, remote signaling, and other distributed operations—even under high computational load.

---

### 2.3.2.3 Ready Structure: Distributed Ready Forest

The EDSOS scheduler employs a **Distributed Ready Forest** as its core ready-state management abstraction. This structure is not a monolithic global data structure; rather, it is the logical union of per-core **Ready Linked Lists**, each maintained independently by a CPU core. These local lists coordinate via the **Chained Message Bus** (see §2.3.2.4) to enable flexible scheduling policies and structural affinity optimizations, collectively forming a decentralized, highly scalable task dispatching mechanism.

#### 2.3.2.3.1 Per-Core Ready Linked List

Each CPU core maintains a local Ready Linked List. Since scheduling operations on a single core are inherently sequential, insertions and removals from this list require no locks or atomic instructions.

1. **List Entry Format**  
   Each entry holds the Local Virtual Address (LVA) of an Arbor Strux (AS) node. Optionally, an entry may include a metadata pointer carrying scheduling hints—such as a `priority_hint` or a `scheduling_policy` tag—to guide policy-aware dispatch.

2. **Multi-Policy Insertion Semantics**  
   The scheduler supports three insertion strategies, selected based on calling context or node metadata:
   - **Tail Insertion**: The default strategy, preserving FIFO fairness.
   - **Head Insertion**: Reserved for high-priority system-service nodes (e.g., interrupt callbacks or driver completion routines) to ensure low-latency response.
   - **Smart Positioning**: Dynamically determines insertion position using:
     - Compiler-provided annotations (e.g., `__edsos_depends_on(node_X)` expressing explicit dependencies);
     - Heuristic metadata parsed by the EDSOS Loader (e.g., hot-function markers or I/O-binding hints);
     - Runtime feedback from optimization agents (e.g., outputs from ML models trained on execution intervals or cache hit rates).  
     Nodes are placed immediately after their dependency predecessors or adjacent to structurally affine peers to maximize TLB and cache reuse.

3. **Implicit Home-Core Affinity**  
   Newly created AS nodes are instantiated via a `push` operation by the current core’s scheduler and are automatically enqueued into that core’s Ready List. Consequently, the entire subtree executes by default on its home core, ensuring strong locality. Node migration is an explicit operation triggered only by load-balancing mechanisms (see §2.3.4).

#### 2.3.2.3.2 Per-Core Wait Graph

The Wait Graph implements the `wait(event)` and `signal(event)` synchronization primitives in EDSOS. Designed for performance, memory efficiency, and namespace isolation, it consists of three per-core components:

1. **Block Slot Table**  
   - Type: A dynamically sized array `BlockSlot slots[MAX_SLOTS]`, initially allocated with `N₀` slots (e.g., 64).
   - Each `BlockSlot` contains:
     - `Node* blocked_nodes[MAX_NODES_PER_SLOT]`: A FIFO queue (initial capacity `M₀`, e.g., 8);
     - `uint32_t count`: Current number of blocked nodes;
     - `bool is_global`: Indicates whether the slot corresponds to a global semaphore.
   - **Lifecycle Management**:
     - Slot IDs are reference-counted and reusable.
     - Dynamic resizing follows these rules:
       - If `count == MAX_NODES_PER_SLOT`, allocate a larger buffer (e.g., double size) and migrate pointers;
       - If `count < M₀/4` for `T` consecutive scheduling cycles, shrink the buffer;
       - If total slot utilization exceeds 90%, increment `MAX_SLOTS` in fixed steps (e.g., +64);
       - If idle slots exceed 50% over a sustained period, reduce `MAX_SLOTS`.

2. **Local Semaphore Name Hash Map**  
   - Type: Hash map mapping `local_event_name → local_event_id`.
   - `local_event_name` is scoped to the current AS instance (e.g., `"mutex_lock_0x1234"`);
   - `local_event_id` is drawn from a per-core ID space (e.g., `[0, N₀)` );
   - Names are unique within the core and do not require global registration;
   - Entries are created on first `wait()` and reclaimed when the last waiter departs and no pending signals remain.

3. **Global Semaphore Name Hash Map**  
   - Type: Hash map mapping `global_event_name → local_event_id`.
   - `global_event_name` is globally unique (e.g., `"io_complete_disk0_blk123"`);
   - `local_event_id` is assigned from a reserved high-ID range (e.g., ≥ 2³¹), with the MSB set to distinguish global IDs;
   - On first lookup, the name is resolved against a distributed hash table (DHT)-backed global registry and cached locally;
   - Lifecycle management mirrors that of the local map.

The ID spaces for local and global semaphores are disjoint (`local_id < 2³¹`, `global_id ≥ 2³¹`), eliminating naming conflicts.

---

### 2.3.2.4 Chained Message Bus for Scheduler

The **Chained Message Bus for Scheduler** is the foundational communication substrate enabling **decentralized, cross-core coordination** in the EDSOS scheduler. It is lock-free, centerless, and driven by localized structures and periodic consumption.

#### 2.3.2.4.1 Chain-Based, Scalable Topology

Logically, the bus forms a directed graph. Each node maintains a set of **Single-Producer Single-Consumer (SPSC) lock-free queues**—one per direct neighbor—as inbound message buffers. The node acts as the consumer; its immediate predecessor(s) in the topology act as producer(s).

Each node holds references to the *outbound* message buffers of its neighbors, enabling direct writes during transmission: **messages are enqueued directly into the recipient’s buffer** without intermediate routing.

This topology aligns natively with both:
- **On-chip interconnects** in modern multi-core CPUs (for intra-machine coordination), and
- **DPU-accelerated fabric networks** across physical machines (for distributed deployment).

Thus, the same abstraction seamlessly spans single-node and multi-node EDSOS instances.

#### 2.3.2.4.2 Message Format

Messages consist of a fixed-size header and a variable-length payload. The header includes:
- A type tag,
- Propagation direction,
- Time-to-live (TTL),
- Payload pointer and length.

> *[TODO: Define complete message header structure in Appendix or referenced spec.]*

#### 2.3.2.4.3 Implementation and Integration

The Chained Message Bus itself is implemented as a standalone, MIT-licensed library with a modular architecture supporting customization of:
- Message packet formats,
- Node topologies,
- Routing and propagation policies.

Its API is minimal and efficient:
- `check()`: Invoked frequently (e.g., at scheduler entry/exit) to consume pending messages;
- `send()`: Used to enqueue outbound messages;
- `init()` / `destroy()`: For lifecycle management.

As a critical EDSOS component, the bus is initialized during system boot and torn down at shutdown.

---

## 2.3.3 Synchronization Mechanisms

### 2.3.3.1 EDSOS Semaphore: An Execution Ordering Barrier

In EDSOS, the **EDSOS Semaphore** is not a kernel-managed synchronization object such as a Windows `HANDLE` or a Linux `futex`, but rather a lightweight, conceptual **execution ordering barrier**. Its design shifts concurrency coordination away from “contention over shared state” toward **scheduler-driven orchestration of execution flows**, aligning with EDSOS’s core principles of *scheduler-centric control* and *no implicit sharing*.

#### 2.3.3.1.1 The Nature of an EDSOS Semaphore: A Scheduler Hint Without Physical Representation

An EDSOS Semaphore exhibits the following key characteristics:

- **No physical representation**: The semaphore does not occupy memory, correspond to any in-kernel or user-space data structure, or expose a handle. It is identified solely by a globally unique name (`event_name`).
- **Dynamically maintained existence**: A logical slot for the semaphore exists only within the **Wait Graph** of a CPU core’s scheduler if at least one Arbor Strux (AS) node is actively waiting on it. Once all waiters are awakened and no new registrations occur, the slot is reclaimed, and the semaphore ceases to exist logically in the system.
- **Acts as an execution barrier**: When a node invokes `wait(event_name)`, it voluntarily yields the CPU and registers itself in the associated wait queue. Its execution flow halts until another execution context calls `signal(event_name)`, at which point the scheduler transitions the node to the `ready` state.
- **Automatic lifecycle management**: Semaphores require no explicit creation (e.g., `CreateSemaphore`) or destruction (e.g., `CloseHandle`). Their lifetime is entirely usage-driven, eliminating resource leaks by design.

#### 2.3.3.1.2 Intra-Core Synchronization: O(1) Scheduler-Local Operations

When both `wait` and `signal` operations occur on the same CPU core, EDSOS Semaphores deliver deterministic, low-latency synchronization with near-constant overhead.

- **Core data structure**: Each per-core scheduler maintains a local **Wait Graph** (see §2.3.2.3.2).
- **`wait` operation flow**:
  1. The node executes `edsos_wait(event_name)`.
  2. The inline scheduler stub transitions the node’s state from `running` to `blocked`.
  3. The scheduler locates or creates a slot for `event_name` in its local Wait Graph and enqueues the node into the associated FIFO queue.
  4. The scheduler triggers its main dispatch logic to select the next ready node from the local Ready List.
- **`signal` operation flow**:
  1. The node executes `edsos_signal(event_name)`.
  2. The inline stub queries the local Wait Graph for the queue associated with `event_name`.
  3. If the queue is non-empty, the scheduler moves the head node (or all nodes, depending on semantics) to the local Ready List and sets their state to `ready`.

This mechanism ensures efficient and predictable synchronization for intra-core coordination.

#### 2.3.3.1.3 Inter-Core Synchronization: Negotiated Wakeup

When `wait` and `signal` occur on different CPU cores, EDSOS employs a **Negotiated Wakeup** protocol to achieve decentralized coordination—avoiding global locks or centralized semaphore managers.

The protocol follows a three-phase **QUERY/RESPONSE/GRANT** handshake:

1. **QUERY**:  
   The signaling core (the *Giver Core*) broadcasts a `QUERY(event_name)` message via the **Chained Message Bus**.

2. **RESPONSE**:  
   Each receiving core inspects its local Wait Graph. If it hosts waiters for `event_name`, it replies with a `RESPONSE` containing:
   - Its core ID,
   - A timestamp (typically derived from `RDTSC`) to approximate cross-core FIFO fairness,
   - Metadata such as queue length.

3. **GRANT**:  
   The Giver Core aggregates all `RESPONSE` messages and selects a *Winner Core* based on a predefined policy (e.g., earliest timestamp). It then sends a `GRANT(event_name)` message exclusively to that core.

4. **Local Wakeup**:  
   Upon receiving `GRANT`, the Winner Core performs the same actions as in an intra-core `signal`: it dequeues the waiting node(s) from its local Wait Graph and enqueues them into its Ready List, transitioning their state to `ready`.

This approach offers several advantages:
- **Decentralization**: No single point of contention or failure.
- **Demand-driven communication**: Only cores with active waiters participate, keeping messaging overhead proportional to actual need.
- **Approximate FIFO fairness**: Global ordering is approximated using `(core_id, rdtsc)` tuples, providing acceptable fairness across cores.
- **Deep scheduler integration**: The negotiation proceeds asynchronously over the Chained Message Bus and is processed periodically by the scheduler’s `check()` routine (see §2.3.2.4), without blocking normal execution.

---

### 2.3.3.2 Scheduler-Driven Semaphore Closure (SDSC)

A **Scheduler-Driven Semaphore Closure (SDSC)** is a lightweight, scheduler-executed response mechanism provided by the EDSOS execution subsystem. It enables an Arbor Strux (AS) node with appropriate capability to register a static instruction fragment that the per-core scheduler executes directly upon signaling of an associated **EDSOS Semaphore**—bypassing full node wake-up and context switch overhead. SDSCs are designed for high-frequency, low-latency signal handling while strictly adhering to lexical scoping and the principle of least privilege.

#### 2.3.3.2.1 Definition and Lifecycle

An SDSC is explicitly created by its **host node** and bound exclusively to that node. Its key properties are:

- **Trigger**: Bound to a single EDSOS Semaphore. When the semaphore is signaled, the scheduler immediately dispatches the SDSC.
- **Parameters and Return**:
  - Accepts optional payload data provided at signal time as input;
  - Has no return value; all register state and transient memory are discarded upon completion.
- **Execution Constraints**: Must not invoke `wait` or block on any other synchronization primitive.
- **Lifetime**: Tied directly to the host node. All SDSCs are automatically deallocated when the host node performs a `finish` operation.

#### 2.3.3.2.2 Memory Layout and Execution Model

An SDSC is stored as a descriptor structure within the address space of its host node. Key fields include:

| Field | Description |
|------|-------------|
| `type_tag` | Enum value `NODE_TYPE_CLOSURE`, distinguishing it from regular AS nodes |
| `if_param` | Boolean indicating whether a signal payload is expected |
| `host_node` | Pointer to the owning AS node |
| `code_start`, `code_length` | Byte offset and length of the instruction segment within the node |
| `data_start`, `data_length` | Byte offset and length of the accessible data segment |

The SDSC uses a compact, ephemeral execution region: the data segment resides at the base address, followed immediately by the instruction segment. At registration time, instruction addresses are specified as **relative offsets**; during execution, the scheduler relocates them to absolute addresses using `data_start` as the base.

A pointer to this SDSC descriptor is inserted into the **Wake Slot** of the corresponding semaphore in the Wait Graph—**ahead of all full AS nodes**. Upon signaling, the scheduler executes the SDSC as follows:

1. Inspects the Wake Slot and confirms the entry’s `type_tag == NODE_TYPE_CLOSURE`;
2. Validates memory safety:
   - `code_length` and `data_length` must not exceed system-defined limits;
   - The combined region `[data_start, data_start + data_length + code_length)` must lie entirely within the host node’s address space;
3. If valid, the scheduler transfers control to the instruction range `[code_start, code_start + code_length)`, passing the signal payload (if any) as input;
4. After execution, the scheduler proceeds to process any remaining listeners in the Wake Slot.

#### 2.3.3.2.3 Security Constraints

To preserve system integrity, SDSC usage is subject to strict constraints:

- **Registration Capability**: Only nodes holding the `CAP_REGISTER_CLOSURE` capability may invoke `edsos_register_closure(...)`. This capability is not granted by default. A per-node limit caps the number of registered SDSCs.
- **No Nesting or Self-Signaling**:
  - SDSCs must not call `wait` or block on any semaphore;
  - A node must not signal the same semaphore its own SDSC is listening on.
- **Explicit Data Scope**:
  - `data_start` and `data_length` must be declared at registration;
  - `data_length` is capped at one page (typically 4 KiB).
- **Code Origin and Size**:
  - `code_start` must point into the host node’s own executable segment;
  - `code_length` ≤ 256 bytes;
  - The instruction sequence must contain **no function calls**, and its effective privilege level cannot exceed that of the host node.
- **I/O Model**:
  - Input is limited to the optional signal payload;
  - No return value or persistent side effects beyond declared data writes.

> **Note**: Only AS nodes containing a valid executable segment may register SDSCs. The format and semantics of signal payloads are defined in §2.3.2.4.2.

---

## 2.3.4 Load Balancing

### 2.3.4.1 Load-Balancing Policy and Trigger Conditions

EDSOS employs a **Work-Stealing++** mechanism for load balancing—an extension of the classic work-stealing model, reoriented around structural semantics rather than numeric workload equality. The primary objective is not to equalize the number of ready nodes across CPU cores, but to **maximize structural affinity**, **minimize context-switch overhead**, and **reduce inter-core communication costs**.

Work-Stealing++ is fully decentralized and operates at the granularity of **Arbor Strux (AS) subtrees**. Coordination between cores is mediated via the **chained message bus**, with **proxy nodes** used to cache remote structural metadata and amortize coordination latency.

The per-core scheduler continuously monitors local load conditions. A Work-Stealing++ negotiation is triggered when any of the following conditions is met:

- **Subtree Size Imbalance**: An AS subtree exceeds a configurable node-count threshold (e.g., 1024 nodes), increasing local scheduling latency.
- **Excessive AS Fragmentation**: Nodes from the same AS are distributed across three or more cores, causing frequent inter-core wakeups and TLB cold misses.
- **Anomalous Load Spikes**: An overload alert is received via the chained message bus or an inter-processor interrupt (IPI)—for example, when a neighboring core’s ready queue exceeds its high-water mark.
- **Contention Among Performance-Sensitive ASes**: Multiple performance-sensitive ASes (e.g., cloud gaming logic, real-time audio/video pipelines) contend on the same core, amplifying scheduling jitter.

EDSOS does **not enforce numeric load balance**. For instance, a configuration where one core runs multiple low-priority ASes while another exclusively hosts a single high-priority AS may be considered optimal under the system’s structural scheduling goals.

---

### 2.3.4.2 Decentralized Negotiation: Request–Response over the Chained Message Bus

#### 2.3.4.2.1 Conditions for Initiating a Negotiation

Each core continuously evaluates its ready queue length and AS distribution. It initiates a load-balancing negotiation when either of the following holds:

1. **Overloaded State**:  
   The local ready queue exceeds the `HIGH_WATERMARK`. The core broadcasts a `TASK_GIVE_REQUEST` message containing:
   - A list of migratable AS subtrees, sorted by structural affinity;
   - Performance sensitivity tags (e.g., `"realtime"`, `"batch"`).

2. **Underutilized State**:  
   The local ready queue falls below the `LOW_WATERMARK`, and no `TASK_GIVE_REQUEST` has been recently received. The core broadcasts a `TASK_GET_REQUEST` message containing:
   - Its available scheduling capacity;
   - Preferred AS types for migration (e.g., `"compute-bound"`, `"I/O-bound"`).

All messages are transmitted asynchronously over the chained message bus and do not block the local scheduler.

#### 2.3.4.2.2 Response and Migration Decision

1. **Offer Generation**  
   Upon receiving a `TASK_GIVE_REQUEST` or `TASK_GET_REQUEST`, a core that can accept or donate work replies with a `STEAL_OFFER` message, including:
   - Current idle capacity;
   - Number of locally scheduled ASes;
   - Auxiliary metadata (e.g., NUMA domain, running AS types).

2. **Target Selection**  
   The requesting core (the *Giver*) collects all `STEAL_OFFER`s and selects a target (*Taker*) using the following priority order:
   - Prefer cores within the same NUMA domain;
   - For performance-sensitive subtrees, prefer cores with fewer ASes and no other critical workloads;
   - Otherwise, select the core with the highest reported idle capacity.

3. **Migration Initiation**  
   The Giver sends a `MIGRATE_SUBTREE(subtree_root_gva, target_core)` command via the chained message bus to initiate the transfer.

The entire negotiation is non-blocking and asynchronous, preserving scheduler responsiveness.

---

### 2.3.4.3 Subtree-Level Migration

EDSOS performs load balancing at the granularity of **complete AS subtrees**—i.e., a migration always includes a root node and **all** its descendants. Partial (“pruned”) migrations are explicitly prohibited to preserve structural integrity.

This design is motivated by two key constraints:

1. **TLB Affinity Preservation**:  
   All nodes in a subtree share a common ancestor chain. Migrating the entire subtree allows the Taker to preheat the full TLB path in one operation, ensuring efficient and secure memory access within the new execution context.

2. **Signal Locality**:  
   If multiple nodes within a subtree await the same EDSOS semaphore, splitting the subtree would force cross-core signal delivery, increasing message-bus traffic and degrading synchronization performance.

Before migration, the scheduler validates subtree integrity to ensure no semantic violations occur.

The migration protocol proceeds as follows:

1. **Subtree Suspension**:  
   The Giver yields the entire subtree, setting all node states to `ready` and saving execution contexts.

2. **Metadata Transfer**:  
   The Giver transmits the subtree root’s Local Virtual Address (LVA) and topology descriptor to the Taker via the chained message bus.

3. **Home-Core Update**:  
   The `subtree_home_core` field of every node in the subtree is updated to the Taker’s core ID.

4. **Enqueue on Target**:  
   The Taker inserts the subtree root into its local ready list; subsequent scheduling proceeds normally.

Critically, **no node data is copied** during migration—only scheduling metadata and core ownership are transferred. Virtual addresses remain unchanged. Page-table mappings are lazily reconstructed by the **Silicon Interposer** during the first cross-core dispatch.

---

### 2.3.4.4 Proxy Node Mechanism: Reducing Cross-Core Coordination

To mitigate coordination overhead when an AS spans multiple cores, EDSOS introduces a **proxy node caching mechanism**. Proxy nodes store lightweight structural metadata about remote ancestors or descendants, eliminating the need for frequent global lookups or synchronous cross-core queries. This applies both intra-socket (across cores) and inter-node (across physical machines).

#### 2.3.4.4.1 Proxy Root Node

When a core executes a subtree whose upstream ancestors reside on another core, the local scheduler creates a **Proxy Root Node**. This node caches the ancestor chain from the local subtree root up to the global AS root.

Use cases include:
- Enabling complete TLB preheating during context activation;
- Providing structural context for capability validation;
- Receiving structural mutation notifications (e.g., `pivot` or `merge`) that affect the ancestor chain.

Proxy root nodes contain **no executable code**—they exist solely as structural metadata caches.

#### 2.3.4.4.2 Proxy Child Node

After migrating a subtree to a remote core, the Giver creates a **Proxy Child Node** to track the migrated subtree’s remote state.

Use cases include:
- Routing `signal` operations to the correct remote subtree;
- Receiving downstream structural updates (e.g., `detach` or `finish` events).

Like proxy roots, proxy children are **non-executable** and excluded from scheduling.

---

### 2.3.4.5 AS-Aware Balancing Policies

EDSOS classifies ASes into two categories and applies differentiated scheduling policies:

| AS Type                | Examples                          | Scheduling Policy |
|------------------------|-----------------------------------|-------------------|
| **Performance-Sensitive** | Cloud gaming logic, real-time control | Prefer exclusive core assignment; avoid co-scheduling with other ASes |
| **Performance-Insensitive** | Capability validation, registry services | Allow co-location; tolerate context-switch overhead |

The `perf_sensitivity` label embedded in each node guides the Work-Stealing++ decision engine, enabling affinity-aware load distribution that respects application semantics.

---

*End of Document.*