<!--
SPDX-FileCopyrightText: © 2025 Bib Guake
SPDX-License-Identifier: LGPL-3.0-or-later
-->

# 1.2 Memory Subsystem

---

> v0.2.1-drafting

---

- [1.2 Memory Subsystem](#12-memory-subsystem)
  - [1.2.0 引言](#120-引言)
    - [1.2.0.1 设计愿景](#1201-设计愿景)
  - [1.2.1 Physical Layer](#121-physical-layer)
    - [1.2.1.1 Ontology of the Physical Page (PP)](#1211-ontology-of-the-physical-page-pp)
    - [1.2.1.2 PP Near-Storage Metadata](#1212-pp-near-storage-metadata)
    - [1.2.1.3 Segmented Storage Layout](#1213-segmented-storage-layout)
      - [1.2.1.3.1 DRAM Segments](#12131-dram-segments)
      - [1.2.1.3.2 SSD Segments](#12132-ssd-segments)
      - [1.2.1.3.3 Tail Segments](#12133-tail-segments)
    - [1.2.1.4 Metadata Reliability and Fault Tolerance](#1214-metadata-reliability-and-fault-tolerance)
    - [1.2.1.5 Physical Memory Management Policy](#1215-physical-memory-management-policy)
  - [1.2.2 Virtual Address Space Model](#122-virtual-address-space-model)
    - [1.2.2.1 Global Virtual Address (GVA)](#1221-global-virtual-address-gva)
      - [1.2.2.1.1 GVA Structure and Allocation Policy](#12211-gva-structure-and-allocation-policy)
      - [1.2.2.1.2 GVA NSME: Self-Bootstrapping Distributed Metadata](#12212-gva-nsme-self-bootstrapping-distributed-metadata)
        - [Conservative Approximate Jump Pointers](#conservative-approximate-jump-pointers)
        - [Self-Bootstrapping Access Protocol](#self-bootstrapping-access-protocol)
      - [1.2.2.1.3 Local Continuity Between GVA and LVA](#12213-local-continuity-between-gva-and-lva)
      - [1.2.2.1.4 GVA Lifecycle and Fault Recovery](#12214-gva-lifecycle-and-fault-recovery)
    - [1.2.2.2 Local Virtual Address (LVA)](#1222-local-virtual-address-lva)
      - [1.2.2.2.1 LVA Semantics and Constraints](#12221-lva-semantics-and-constraints)
      - [1.2.2.2.2 LVA Allocation and Free-Space Management](#12222-lva-allocation-and-free-space-management)
      - [1.2.2.2.3 LVA NSME: Metadata for Address Intervals](#12223-lva-nsme-metadata-for-address-intervals)
        - [Key Design Principles:](#key-design-principles)
      - [1.2.2.2.4 Coordination Between LVA and Page Table (PTE)](#12224-coordination-between-lva-and-page-table-pte)
    - [1.2.2.3 Ontology of Virtual Pages (VP)](#1223-ontology-of-virtual-pages-vp)
      - [1.2.2.3.1 Essence of VP: Abstract Object for Data](#12231-essence-of-vp-abstract-object-for-data)
      - [1.2.2.3.2 Dual Embedding Views: LVA vs. GVA](#12232-dual-embedding-views-lva-vs-gva)
      - [1.2.2.3.3 Containment Relationship: VP and PP](#12233-containment-relationship-vp-and-pp)
        - [Migration Semantics](#migration-semantics)
      - [1.2.2.3.4 Copy Mechanism and Consistency](#12234-copy-mechanism-and-consistency)
      - [1.2.2.3.5 Structural Relationships Between VP, AS, and Node](#12235-structural-relationships-between-vp-as-and-node)
      - [1.2.2.3.6 State Machine of VP](#12236-state-machine-of-vp)
  - [1.2.3 协议层](#123-协议层)
    - [1.2.3.1 路径缓存协议](#1231-路径缓存协议)
    - [1.2.3.2 所有权协议](#1232-所有权协议)
  - [1.2.4 逻辑层](#124-逻辑层)
    - [1.2.4.1 AS 生命周期](#1241-as-生命周期)
    - [1.2.4.2 节点语义](#1242-节点语义)
    - [1.2.4.3 共享机制](#1243-共享机制)
      - [1.2.4.3.1 CARN（Cross AS Referring Node）](#12431-carncross-as-referring-node)
      - [1.2.4.3.2 共享代码段（Shared Code Node, SCN）](#12432-共享代码段shared-code-node-scn)
    - [1.2.4.4 CoW（Copy-on-Write）机制](#1244-cowcopy-on-write机制)
  - [1.2.5 容错与错误处理](#125-容错与错误处理)
    - [1.2.5.1 故障恢复](#1251-故障恢复)
    - [1.2.5.2 ECC 与数据完整性](#1252-ecc-与数据完整性)
  - [1.2.6 内存压力管理](#126-内存压力管理)
    - [1.2.6.1 驱逐策略](#1261-驱逐策略)
    - [1.2.6.2 转移存储](#1262-转移存储)
    - [1.2.6.3 资源硬边界](#1263-资源硬边界)
  - [1.2.7 调试与可观测性](#127-调试与可观测性)
    - [1.2.7.1 PP 层接口](#1271-pp-层接口)
    - [1.2.7.2 VP 层接口](#1272-vp-层接口)
    - [1.2.7.3 AS/Node 层接口](#1273-asnode-层接口)
    - [1.2.7.4 全局接口](#1274-全局接口)
    - [1.2.7.5 安全验证](#1275-安全验证)
  - [1.2.8 硬件协同与兼容性](#128-硬件协同与兼容性)
    - [1.2.8.1 DPU 角色](#1281-dpu-角色)
    - [1.2.8.2 CPU 兼容](#1282-cpu-兼容)
    - [1.2.8.3 显式协同模型](#1283-显式协同模型)
    - [1.2.8.4 未来定制 CPU](#1284-未来定制-cpu)


---

## 1.2.0 引言

### 1.2.0.1 设计愿景
- 显式分布式（Explicitly Distributed）
- 局部自组织，全局稳态

---

## 1.2.1 Physical Layer

The physical layer constitutes the foundational substrate of the EDSOS memory subsystem, responsible for managing raw storage units on physical devices—such as DDR DRAM modules and NVMe SSDs. Its primary objective is to **deliver scalable, reliable, and hardware-friendly physical page management with minimal overhead**, while enabling the *containment* relationship that underpins virtual pages (VPs) in the upper layers.

### 1.2.1.1 Ontology of the Physical Page (PP)

A **Physical Page (PP)** is the atomic unit of the physical layer, defined as a fixed-size, contiguous block of storage on a single physical device. Crucially, a PP is **stateless, unowned, and semantically empty**—it functions solely as a *container* that may hold one VP at a time.

- **No intrinsic content management**: The state and semantics of a PP are entirely externalized via metadata.
- **Containment semantics**: In steady state, a VP resides in exactly one PP. During migration, it may transiently exist in two PPs simultaneously; external consistency is guaranteed by the coherence protocol.
- **Device binding**: Each PP is strictly bound to a single physical storage device (e.g., one DDR5 DIMM or one SSD). **Segment boundaries never span across distinct physical devices**.

> **Design philosophy**: By being “headless,” the PP becomes a pure resource pool. This enables the scheduler to freely allocate, migrate, or reclaim PPs without modifying the PP itself—aligning with EDSOS’s principle of *explicit distribution* and *loading-time centrality*.

### 1.2.1.2 PP Near-Storage Metadata

Each PP is described by a **PP Near-Storage Metadata Entry (PP NSME)**, an 8-byte (64-bit) compact structure encoding its state and topological attributes:

| Field               | Width | Description |
|---------------------|-------|-------------|
| `state`             | 6 bits | Page state: `FREE=0`, `LOCKED=1`, `CORRUPTED=2`, `RELIABLE=3`, `RESERVED=4`, … |
| `next_free_log2`    | 6 bits | Logarithmic hint for free-list jump distance (base-2 scale) |
| `jump_hint_prev`    | 19 bits | Conservative approximation of predecessor offset in hierarchical free list |
| `jump_hint_next`    | 19 bits | Conservative approximation of successor offset in hierarchical free list |
| `ded`               | 1 bit  | Parity bit for fast hardware integrity check of the PP NSME |
| `zone_token`        | 4 bits | Zone identifier for NUMA or storage-type (DRAM/SSD) optimization |
| `reserved`          | 9 bits | Reserved for future extensions (e.g., security tags, compression state, ECC mode) |

The PP NSME supports an O(log d) allocation/deallocation algorithm based on *Conservative Logarithmic Jump Hints*. The jump pointers are conservative approximations (computed position ≤ true position), ensuring correctness through unidirectional linear probing during lookup.

### 1.2.1.3 Segmented Storage Layout

To organize PP NSMEs efficiently and eliminate pointer overhead, physical memory is partitioned into **Segments**:

- Segment size: Logically **2 GiB** ($2^{31}$ bytes).
- Metadata region: The first **4 MiB** of each segment, composed of two 2-MiB **metadata pages**.
- Data region: The remaining $2\,\text{GiB} - 4\,\text{MiB}$, divided into PPs according to device-native page granularity.
- Indexing: The PP NSME for the $i$-th PP within a segment resides at byte offset $i \times 8$ in the metadata pages—**no explicit address pointers are stored**.

#### 1.2.1.3.1 DRAM Segments
- PP size: **4 KB**, aligned with CPU page size.
- Number of PPs: $(2\,\text{GiB} - 4\,\text{MiB}) / 4\,\text{KiB} = 523{,}264$.
- Metadata capacity: A 2-MiB metadata page holds $2^{19} = 524{,}288$ PP NSMEs, leaving ~1,024 entries reserved for:
  - Segment-level metadata (e.g., lock, usage tag, hardware info);
  - Cross-segment indices (e.g., free-segment lists);
  - Alignment padding to accommodate vendor-specific capacity reporting (e.g., base-1000 vs. base-1024).

#### 1.2.1.3.2 SSD Segments
- **PP size**: **16 KB**, matching typical SSD write granularities.
- **Fewer PPs per segment**, with **more reserved PP NSME slots**.
- **Write-amplification mitigation**: Since 16-KB updates underutilize the 2-MB metadata page, PP NSME writes are **rotated across subregions** of the metadata page to distribute wear and extend SSD lifetime.

#### 1.2.1.3.3 Tail Segments
- If a physical device’s capacity is not an exact multiple of 2 GB, the remainder forms a **tail partial segment**.
- Tail segments use simplified metadata management:
  - Metadata pages are proportionally smaller;
  - Free-list traversal degrades to **linear scan**, which remains acceptable due to small scale.

> **Important note**: Segments are purely organizational units for PP NSMEs and do **not constrain higher-layer allocations**. For example, a 1-GiB large page may span two segments, requiring coordinated updates to PP NSMEs in both. This incurs only negligible additional memory latency and preserves correctness.

### 1.2.1.4 Metadata Reliability and Fault Tolerance

- **Hardware ECC as primary defense**: Both PP NSMEs and PP data rely on device-native error correction (e.g., DDR5 On-Die ECC, SSD LDPC).
- Software ECC as optional fallback: Disabled by default. If enabled, it requires **dedicated physical regions** for parity storage—no space is reserved by default.
- **Metadata page backup**:
  - Critical segments may configure backup copies of their metadata pages;
  - Backups can be co-located (e.g., in a dedicated metadata zone) or randomly dispersed (to improve fault isolation);
  - A backup is accessed via a pointer stored in the segment header and is only used upon failure of the primary metadata page.

### 1.2.1.5 Physical Memory Management Policy

- Allocation/deallocation: Driven exclusively by the scheduler, leveraging the hierarchical jump-list in PP NSMEs for O(log d) complexity.
- **No global garbage collector**: The content clearance and space reuse of PP occurs only through explicit scheduling decisions, avoiding unpredictable latency from implicit collection.
- **Hard PPSUC boundary**: Each physical machine (PM) enforces a hard limit on physical capacity—**Physical Paged Storage Unit Capacity (PPSUC)**. **Cross-PM PP borrowing is prohibited**, ensuring locality and performance predictability.
- Fault handling: A PP marked as `CORRUPTED` (due to uncorrectable ECC errors or metadata checksum failure) is permanently isolated and excluded from future allocations.

---

## 1.2.2 Virtual Address Space Model

### 1.2.2.1 Global Virtual Address (GVA)

The **Global Virtual Address (GVA)** is a 128-bit logical identifier used in EDSOS to uniquely name a **Virtual Page (VP) that is accessible across physical machines (PMs)** within the cluster. Aligned with the principle of *explicit distribution*, a GVA is allocated **only when a VP becomes visible beyond its local PM**—for example, upon remote reference, sharing, or migration. VPs that remain strictly private to a single PM do not require a GVA, thereby avoiding unnecessary global naming overhead.

GVA and Local Virtual Address (LVA) together form EDSOS’s two-level address model:
- **LVA** is a 64-bit virtual address used internally by an Arbor Strux (AS) for CPU execution and pointer arithmetic. It is always bound to a specific PM.
- **GVA** serves as a cluster-wide logical ID, enabling DPU-driven coherence protocols, remote access, and fault recovery.

These two address spaces are linked via **local continuity**: for any given VP, the offset between its base and any internal pointer is identical in both LVA and GVA. Formally,
$$
\text{GVA}_{\text{ptr}} = \text{GVA}_{\text{base}} + (\text{LVA}_{\text{ptr}} - \text{LVA}_{\text{base}})
$$
This invariant ensures that applications can safely convert local LVA-based pointers into GVAs for DPU communication without violating program semantics. Critically, the lifetimes and allocation policies of GVA and LVA are fully decoupled: LVAs are allocated at AS creation time and remain stable, whereas GVAs are allocated lazily on first cross-PM use.

#### 1.2.2.1.1 GVA Structure and Allocation Policy

A GVA is structured as follows:

```
[ RefIdx:16 ][ Free Bits:112 ]
```

- **`RefIdx` (16 bits)**: Identifies the replica index of the referenced VP. `RefIdx = 0` denotes the canonical copy; `RefIdx > 0` indicates a ref copy. This field is assigned by the DPU during a `ref` operation and **does not encode physical location**.
- **`Free Bits` (112 bits)**: Encodes the logical identity of the VP. While uninterpreted by hardware, this field is *guideline-partitioned* as:
  ```
  [ initPMID:16 ][ LocalGVA:96 ]
  ```
  - **`initPMID`**: A hint for the initial owner PM ID, used solely to **reduce contention during GVA allocation**. The allocator preferentially attempts allocation within the current PM’s `initPMID` range; if unsuccessful, it may search globally. **This field does not determine the VP’s actual location.**
  - **`LocalGVA`**: A high-order logical offset, typically aligned with the upper bits of the corresponding LVA to support local continuity.

> **Important Note**: The GVA bit layout **encodes no physical location information**. The true owner PM of any GVA is dynamically indicated by the `pmid` field in its associated **GVA Namespace Metadata Entry (GVA NSME)**. This complete decoupling of naming from placement enables arbitrary VP migration without GVA modification.

#### 1.2.2.1.2 GVA NSME: Self-Bootstrapping Distributed Metadata

Each GVA is associated with a **GVA NSME**, which stores metadata about the VP’s state, size, ownership, and allocation topology. Crucially, GVA NSMEs themselves reside in the GVA address space, forming a **self-bootstrapping metadata structure**.

The GVA NSME format is defined as:

| Field                        | Width | Description |
|-----------------------------|-------|-------------|
| `state`                     | 4     | VP state: `FREE` / `ALLOCATED` / `RESERVED` / `OTHER` |
| `pmid`                      | 16    | ID of the PM currently holding the VP’s page table entry (PTE) |
| `next_diftype_log2`         | 7     | Logarithmic step size hint for primary-space traversal |
| `diftype_jump_hint_prev`    | 14    | Conservative backward jump delta for primary-space skip list |
| `diftype_jump_hint_next`    | 14    | Conservative forward jump delta for primary-space skip list |
| `frefidx_jump_hint_prev`    | 3     | Conservative backward jump delta for free RefIdx list |
| `frefidx_jump_hint_next`    | 3     | Conservative forward jump delta for free RefIdx list |
| `lock`                      | 3     | Lightweight spin lock (1 bit) + contention flags (2 bits) |

##### Conservative Approximate Jump Pointers

To enable efficient allocation and deallocation, GVA NSMEs employ **conservative approximate jump pointers**:
- **Primary-space jumps (14 bits)**: Encoded as `(k:11, t:3)`, yielding an approximate offset of $(2^k) \times 2^{(2^t)}$, indexing into a GVA NSM Page (which contains $2^{19}$ VP entries).
- **Free RefIdx jumps (3 bits)**: Encoded as $s$, yielding an approximate offset of $4^{(2^s)}$.

All jumps represent **deltas**, and all approximations are **conservative**: the computed position is guaranteed to be ≤ the true target. Consequently, lookups require only **forward linear probing** with never backward traversal, ensuring correctness and implementation simplicity.

##### Self-Bootstrapping Access Protocol

At boot time, the system pre-allocates a small set of GVA NSM Pages (metadata pages), whose GVAs and initial `pmid`s are hard-coded by the bootloader. To access the GVA NSME of any GVA:
1. Compute the GVA of its containing NSM Page using high-order bits;
2. Recursively resolve GVA NSMEs of the NSM Page to obtain its `pmid`;
3. Issue a DPU read request to the PM identified by `pmid`;
4. The target PM’s DPU returns the specific GVA NSME entry.

This process is fully distributed, requires no central directory, and supports lazy GPA mapping—the PTE for an GVA NSME may be marked as absent, with the scheduler populating it on demand.

#### 1.2.2.1.3 Local Continuity Between GVA and LVA

To preserve application semantics, EDSOS enforces a strict **local continuity constraint**: within any contiguous region of VPs, the offset between base and pointer must be identical in both GVA and LVA. This ensures that:
- Compiler-generated pointers (based on LVA) can be safely converted to GVAs at runtime;
- DPU instrumentation code can directly compute remote data locations as `(GVA_base + offset)`;
- Applications require no source-level modifications to operate correctly in a distributed setting.

This invariant is jointly maintained by the loader and scheduler:
- The loader allocates contiguous LVA regions for each AS;
- The scheduler aligns GVA allocation to the LVA base on first `ref`;
- Subsequent operations (e.g., `fork`, `divide`) preserve LVA layout stability, thereby maintaining GVA–LVA mapping consistency.

#### 1.2.2.1.4 GVA Lifecycle and Fault Recovery

- **Allocation**: Triggered by the scheduler when a VP is first referenced across PMs, via allocation from the GVA NSME free list.
- **Deallocation**: Occurs when all `ref` references are released (`unref`) and the VP is destroyed; the GVA NSME state is set to `FREE` and returned to the free list.
- **Fault Recovery**: If the PM indicated by `pmid` fails, the DPU detects the failure via heartbeat timeout and initiates:
  1. Cleanup of stale paths (`dpu_cleanup_pm_refs`);
  2. Election of a new owner PM among existing replicas;
  3. Update of the GVA NSME’s `pmid` field by the new owner, completing metadata handover;
  4. The GVA itself remains unchanged during failure, ensuring **reference transparency**.

---

### 1.2.2.2 Local Virtual Address (LVA)

The **Local Virtual Address (LVA)** constitutes the **sole address space visible to CPU execution** in EDSOS. It adheres to the standard 64-bit virtual addressing format. It is translated by the local MMU into Local Physical Addresses (LPAs) via conventional multi-level page tables (PTEs). This design ensures full compatibility with existing x86-64 and ARM64 address translation mechanisms. The LVA model preserves familiar OS programming semantics while enabling key Arbor Strux (AS) features: node-scoped isolation, structured concurrency, and subtree-aware execution.

#### 1.2.2.2.1 LVA Semantics and Constraints

- **AS Privacy**: Each AS (analogous to a process in classical OS models) owns a private LVA space, rooted at a dedicated page table whose base is loaded into the CR3 register. LVAs from distinct ASes are semantically disjoint—even if numerically identical, they refer to unrelated memory regions. Switching between ASes is accomplished by updating CR3.

- **Node Contiguity**: All Virtual Pages (VPs) belonging to a single Node—comprising metadata (R), executable code (RX), and mutable data (RW)—must reside within a **contiguous LVA region**. This requirement enables:
  - Compiler-generated relative jumps and data offsets;
  - Pointer validity and transitivity within the AS;
  - Efficient subtree scoping via PTE Present-bit manipulation.

- **Address Stability**: Once allocated, an LVA remains valid for the entire lifetime of its AS. Addresses are **never reclaimed or reused**, ensuring long-lived application pointers remain safe.

- **Address Width**: The usable virtual address width is up to 57 bits, providing ample headroom for foreseeable workloads.

> The LVA represents the *execution view* of memory. It exists independently of the Global Virtual Address (GVA); a GVA is assigned only when a VP must be referenced across physical machines (PMs).

#### 1.2.2.2.2 LVA Allocation and Free-Space Management

LVA allocation is triggered by the scheduler during AS creation or Node operations. The allocator must satisfy the following constraints:
- Align allocations to node granularity (typically ≥ 4 KB);
- Guarantee contiguity of the R/RX/RW segments within each Node;
- Insert at least one 4 KB guard page between adjacent Nodes to prevent spatial overruns.

To efficiently manage the vast 48–57 bit virtual address space, EDSOS introduces **LVA NSMEs** as lightweight descriptors enabling O(log N) free-block search and coalescing, which is different from Linux’s VMA-based red-black trees.

#### 1.2.2.2.3 LVA NSME: Metadata for Address Intervals

Each LVA NSME is a compact **64-bit (8-byte)** record that describes the lifetime and topological attributes of an LVA interval. It complements—but does not duplicate—the information in PTEs:
- **PTEs** describe *pages*: their LPA mapping, permissions, huge-page status, and SSD residency.
- **NSMEs** describe *address ranges*: whether they are allocated, lockable, or eligible for sharing.

The LVA NSME layout is defined as follows:

| Field                | Width   | Description |
|----------------------|---------|-------------|
| `state`              | 4 bits  | Interval state:<br>`0=FREE`, `1=LOCKED` (allocation in progress), `2=CORRUPTED`, `3=RELIABLE`, `4=RESERVED` |
| `next_diftype_log2`  | 6 bits  | Primary jump-step hint for hierarchical free-list traversal |
| `jump_hint_prev`     | 19 bits | Conservative backward offset to predecessor in jump list (≤ true distance) |
| `jump_hint_next`     | 19 bits | Conservative forward offset to successor in jump list (≤ true distance) |
| `lock`               | 2 bits  | Lightweight spinlock for concurrent allocation on multi-core PMs |
| `flags`              | 6 bits  | Extended flags (stored here due to limited OS-reserved bits in PTEs):<br>• `IS_NODE_BOUNDARY`<br>• `SSD_BACKED_HINT`<br>• `NO_REF` (disallows cross-PM referencing) |
| `counter`            | 8 bits  | Performance counter (e.g., access frequency, page-fault count) for scheduler-driven eviction |

##### Key Design Principles:
1. **No Page-Size Encoding**: Page size (4K/2M/1G) is solely the responsibility of the PTE. The NSME tracks only whether an address range is *in use*, not how it is paged.
2. **Conservative Jump Hints**: The `jump_hint_prev/next` fields store *conservative under-approximations* of pointer offsets. This guarantees that free-space searches require only unidirectional linear probing.
3. **Multi-Core Safety**: The `lock` field serializes concurrent modifications to the same NSME, supporting parallel AS execution across cores.
4. **Observability for Scheduling**: The `counter` field enables heat-based eviction and migration policies by the scheduler.

#### 1.2.2.2.4 Coordination Between LVA and Page Table (PTE)

Physical mapping of LVAs is handled entirely through standard PTEs. EDSOS leverages **OS-reserved bits** in the PTE (e.g., bits 58–62 on x86-64) to encode extended semantics:
- `IN_SSD`: Indicates the LPA resides on SSD, requiring special I/O handling on access;
- `SUBTREE_ROOT`: Marks the PTE as the root of a subtree, enabling incremental page-table updates during scope transitions;
- `CARN_REF`: Denotes that the VP participates in a CARN (Cross-AS Reference Namespace), permitting shared writable access across ASes.

These annotations are interpreted by software upon page faults and do not rely on hardware support.

> **Note**: PTEs describe *pages*, whereas LVA NSMEs describe *address intervals*. For example, a single 2 MB huge page corresponds to one PTE but spans multiple LVA NSME entries, each covering a 4 KB-aligned sub-interval of the larger region.

---

### 1.2.2.3 Ontology of Virtual Pages (VP)

In the EDSOS memory subsystem, a **Virtual Page (VP)** serves as the **fundamental logical abstraction unit** for data encapsulation. It is not a physical entity but an **abstract object** with distinct identity, state, and behavioral semantics. A VP's existence is independent of its physical container (PP) or indexing method (LVA or GVA), yet it must be associated with both to be instantiated and accessed.

#### 1.2.2.3.1 Essence of VP: Abstract Object for Data

The core definition of VP is as follows:

> **A VP is a fixed-size (4K/2M/1G) abstract object with globally recognizable identity. It carries no execution semantics and functions purely as a data abstraction.**

Key characteristics:
- **Unique Identity**: A VP is uniquely identified by `{ASID, LVA}` within its AS context; if cross-PM visibility is required, it also possesses a GVA.
- **No Built-in Reference Counting**: Unlike traditional systems, VPs do not maintain reference counts. Their existence is binary:
  - **Non-existent/unallocated**;
  - **Exist and unique** (allocated and owned by a Node).
- **Bound to a Single AS**: Each VP strictly belongs to one AS and cannot be shared across ASes unless explicitly duplicated via the **CARN (Cross-AS Referring Node)** mechanism.

> **Philosophical Clarification**: VPs are considered "objects" rather than "resources". Their usage is achieved through creating copies or obtaining access rights to the canonical copy instead of incrementing refcounts.

#### 1.2.2.3.2 Dual Embedding Views: LVA vs. GVA

While VPs themselves have no inherent address, they can be "embedded" into address spaces through two orthogonal views:

| View | User | Purpose | Mandatory |
|------|------|---------|-----------|
| **LVA** | Local CPU | Index for local read/write operations in the private virtual address space | Yes (all VPs must have an LVA) |
| **GVA** | DPU | Index for remote access in the cluster-wide logical namespace | No (only assigned when cross-PM visibility is needed) |

- **LVA View**: Embeds the VP into the AS's private virtual address space, forming contiguous node layouts;
- **GVA View**: Embeds the VP into a cluster-level logical namespace, supporting path caching and consistency protocols.

These views **do not bind the VP itself but index the same VP instance from different dimensions**, akin to embedding an abstract manifold into various coordinate systems ($\mathbb{R}^n$). The VP represents the manifold, while LVA and GVA serve as coordinate charts.

> **Critical Implication**: Modifying LVA mappings (e.g., remapping to SSD) or GVA routing (e.g., migrating owner_pm) **does not alter the VP itself**, only changes how it is accessed.

#### 1.2.2.3.3 Containment Relationship: VP and PP

For a VP to be instantiated and accessible, it must be contained within a physical entity, referred to as **Containment**:

- **PP (Physical Page)** is the smallest unit of physical storage (a continuous segment of DDR or SSD);
- **At any given time, a VP resides in exactly one PP** (except during transient migration phases);
- **Copies of a VP reside in separate PPs**, ensuring physical isolation.

##### Migration Semantics
"Migration of VP" involves switching its containment relationship from one PP to another, accomplished through remapping of GVA and LVA:
1. Source PM transfers VP data to target PM;
2. Update `pmid` field in GVA NSME;
3. Mark source PM’s LVA PTE as page fault;
4. Establish new LVA → LPA mapping on target PM;
5. Update DPU *Path Cache* (DPU cache of communication path description of VPs) via `PCEReq/PCEAck`.

> **Boundary Case**: During migration, the VP may temporarily exist in both source and target PPs ("dual containment"), but consistency protocols ensure external observers see consistent states.

#### 1.2.2.3.4 Copy Mechanism and Consistency

VP supports the creation of **copies (ref copy)** for cross-PM sharing or load migration. Key attributes of copies include:
- **Dependence on Canonical Copy**: Validity of copies relies on the canonical copy (RefIdx=0);
- **Independent Identities**: Each copy has its own LVA (within the target AS) and GVA (RefIdx > 0);
- Data Consistency: Two orthogonal protocols are designed:
  - **Path-Caching Consistency Protocol**: Eventual consistency, supporting optimistic reads/writes;
  - **Ownership Protocol**: Strong consistency, used for locking or migration scenarios.

> **Important Note**: Copies are not references but **new VP instances**, whose contents synchronize with the canonical copy through data consistency protocols.

#### 1.2.2.3.5 Structural Relationships Between VP, AS, and Node

VPs act as **atomic building blocks** for constructing AS address spaces, with higher-level structures providing their semantic context:

| Structure | Relation to VP |
|-----------|----------------|
| **Node** | Organizational unit for VPs; contains several **contiguous LVA VPs** (R/RX/RW segments) |
| Special Node **CARN (Cross-AS Referring Node)** | Allows cross-AS sharing; its VPs can be referenced by other ASes to create copies |

- **VPs lack structural semantics**: Individual VPs do not know which Node they belong to or whether they are part of a CARN;
- **Structural semantics maintained by scheduler**: Node metadata records VP ranges, types, and sharing policies;
- **VP lifecycle is part of Node lifecycle**: Node destruction leads to all its VPs being marked FREE and LVA NSME recycling.

> **Design Principle**: VPs are "headless" data pages whose structure is defined by Nodes and managed by DPUs.

#### 1.2.2.3.6 State Machine of VP

Throughout its lifecycle, a VP transitions through the following states (described by GVA NSME.state or LVA NSME.state, which should remain consistent):

```mermaid
stateDiagram-v2
    [*] --> FREE
    FREE --> ALLOCATED: Scheduler allocation (Node creation)
    ALLOCATED --> MIGRATING: OwnReq/ref triggers migration
    MIGRATING --> ALLOCATED: Migration completion
    ALLOCATED --> FREE: Node destruction
    ALLOCATED --> CORRUPTED: ECC errors/fault detection
```

- **ALLOCATED**: VP exists, contained in a PP, and is accessible;
- **MIGRATING**: Transient state where data is being copied, potentially blocking or redirecting accesses;
- **CORRUPTED**: Irrecoverable error requiring upper-layer handling.

---

## 1.2.3 协议层

### 1.2.3.1 路径缓存协议

- 适用场景: 高频读优化、乐观并发共享、页范围原子操作
- 机制: 
  - 控制流的分支迁移到另一个 PM, 随之带来进程空间的扩张
  - 数据流伴随控制流迁移, 新 PM 触发 `ref` 并拷贝获取整个 VP, 在 `ref` 建立过程中建立路径缓存
  - 路径缓存的 MidPoint 缓存 (保存在沿路的 DPU 中) 包括 VP 的正本 GVA, 归属的副本 bit_map, HostPoint 方向最近一跳, EndPoint 方向最近一跳; HostPoint 缓存额外包括归属副本 bit_map 所对应的 PM; EndPoint 缓存额外包括正本所在的 PM
  - 分布式自动路由: 完全通过路径缓存记录下对应于 VP 的一致性消息的路由方向, 由路径缓存解析器 (PCAU, Path Caching Analysis Unit) 负责解析, 不需中心目录, 只需 GVA 即可自动导向正确的 PM
  - 版本与确认: `RefReq` / `RefAck` 及其他成对指令对应携带相同的由新 PM 生成 (在新 PM 范围内单调增) 的Transaction ID, 旧 PM 在 `RefReq` 时缓存, 在 `RefAck` 时删除; 同时指令携带 TTL, 超时废弃并报告
  - 新 PM (非 owner_pm) 触发写操作时, `SWReq (Submit Write)` 指令伴随 Write-Update 沿缓存路径传播到达 owner_pm, owner_pm 触发 `WUReq` (对于小数据) 或 `WIReq` (对于大数据) 携带触发者的 bit_map, 由 PCAU 自动向其他副本转发 & 向触发者返回 `SWAck`, 仍由 PCAU 自动缓存 Transaction ID 并在其他副本全部返回 `WUAck` / `WIAck` 时向主机返回 `WUAck` / `WIAck`
  - 旧 PM (owner_pm) 触发写操作时, 直接触发 `WUReq` (对于小数据) 或 `WIReq` (对于大数据); 这里实际上意味着 owner_pm 类似于 MESIF 中的 Forward
  - 任何非 owner_pm 触发读操作时需要检查是否有 `Invalid` 标记, 如果是则触发 `ReadReq` 从 owner_pm 处读取得到*已知最新*的版本; 不对数据 (VP) 本身记录版本, 只对一致性协议消息记录版本
  - 当控制流退出一个 PM 时, 进程空间随之收缩, 触发 `UnrefReq` 自动清理路径, 但必须等待 `UnrefAck` 才能安全退出

---

### 1.2.3.2 所有权协议

- 适用场景: 实现强一致性读写 / 分布式锁 / 迁移
- 机制: 
  - 请求: 在建立了基于路径缓存的 `ref` 之后, 控制流可以主动触发 `OwnReq`; 当旧 owner_pm 发现自己不再是 owner_pm 时, 也可以根据对应的 Config Tag 寄存器在要求 "始终按照所有者身份读写" 时自动触发 `OwnReq`; 触发之后请求者需要锁定本地的副本 VP, 拒绝新的 store
  - 仲裁: `OwnReq` 到达当前 owner_pm 后触发仲裁流程, 仲裁主要基于指令传播到达的先后顺序（包括自竞争指令）; 只有当多个指令在同一个时钟周期同时到达时, 根据可交换标识 & 租期状态 / 优先级 / 公平防饿 / 端口序号仲裁出归属; 不论是否获得所有权, 都会*立即*（固定的时钟周期数量）返回一个响应 (`OwnAck` / `OwnFail`); 仲裁发生在当前 owner_pm, 它拥有最高的权限, 可以将一个指定的寄存器置高来声明处于"不可被换出状态", 但是这个状态会在DPU内硬件计时, 不可超出上限, "超出上限主机未响应"是一个触发报告主机故障的场景
  - 所有权与数据迁移: 当 owner_pm 的仲裁器给出仲裁允许转移和请求方收到 `OwnAck` 时, 各自向 PCAU 提交一个 `Stall`, 一切在此期间到达的 VP 一致性消息都临时写入缓存等待; 同时 owner_pm 并行地在主机本地对 LVA 标记缺页; 请求方在收到 `OwnAck` 时消息附加完整的正本内容, 直接 DMA 写入原副本 VP 对应的 PP中; 接收完成 (接收字节量 & ECC 校验) 后请求方返回 `ROAck (Receive Owning)`, 附带完整的旧副本元数据
  - 元数据确认与路径缓存更新: 原 owner_pm 收到 `ROAck` 后更新本机的 GVA 和 LVA 页表项, 并重置与 owner_pm 相关的寄存器, 完成后发送一个特殊的 `PCEReq (Path Caching Exchange)` 要求 MidPoint 中的路径缓存交换方向并附带 HostPoint 所需的额外数据; 请求方收到 `PCEReq` 说明路径缓存的 MidPoint 更新完成, 触发 `PCEAck` 附带 EndPoint 的额外数据, 交换双方的 HostPoint / EndPoint 标记, 消除 PCAU 的 `Stall`, 恢复运行
- 约束：
  - 防止未完成写或本机原子操作期间转移
  - 防止转移期间写
  - 防止租期内转移

---

## 1.2.4 逻辑层

### 1.2.4.1 AS 生命周期
- 创建：初始化 LVA 空间元数据（页表基址）
- 销毁：释放 LVA 元数据，触发所有 VP unref

---

### 1.2.4.2 节点语义
- **三段结构**：元数据（R）、指令（RX）、数据（RW）
- **定义域**：可访问的 VP 集合（自身 + 祖先）
- **作用域**：自身 VP 可被哪些节点访问
- **引用计数**：仅 CARN/SCN 具备

---

### 1.2.4.3 共享机制

#### 1.2.4.3.1 CARN（Cross AS Referring Node）
- **唯一可跨 AS 共享的节点类型**
- **引用计数**：由 CARN 元数据维护
- **一致性**：
  - 同 PM：链式消息总线 或 硬件原子锁（显式调用）
  - 跨 PM：DPU 路径缓存 / 所有权协议（显式桩函数）

#### 1.2.4.3.2 共享代码段（Shared Code Node, SCN）
- **退化 CARN**：仅含只读指令段
- **复用 CARN 引用计数与 PTE 配置机制**
- **不可迁移、不可写**

---

### 1.2.4.4 CoW（Copy-on-Write）机制
- **触发条件**：fork 后写入共享 VP
- **PTE 标记**：只读 + CoW 标志位（OS 保留位）
- **缺页处理**：
  - 调度器分配新 PP
  - 复制原内容
  - 更新写入 AS 的 VP → 新 PP
- **CARN 特例**：
  - fork 时 **引用计数 +1**，AS B 正常共享 CARN
  - 写入时 **取消 CoW，允许直接写**（因 AS B 合法持有引用）
  - 若 AS B 不需要，应显式取消引用

---

## 1.2.5 容错与错误处理

### 1.2.5.1 故障恢复
- PM 失效检测：DPU 心跳/超时
- 所有权接管：各个副本自我票选新正本
- 路径自动清理：`dpu_cleanup_pm_refs`

---

### 1.2.5.2 ECC 与数据完整性
- **硬件 ECC**：DPU 传输校验 + CPU 内存 ECC（默认）
- **软件 ECC**：应急接口 `enable_software_ecc(vp)`（默认关闭）

---

## 1.2.6 内存压力管理

### 1.2.6.1 驱逐策略
- **单位**：节点（因 VP 绑定于节点）
- **热度预测**：
  - 就绪队列等待时间
  - 事件阻塞时长
  - AS 树拓扑邻近性
- **动作**：调度器主动解除 VP 对 PP 的盛装

---

### 1.2.6.2 转移存储
- **SSD PP**：PPSUC 包含高速磁盘可用
- **流程**：调度器将 VP 迁移到 SSD PP → 访问时按需重迁移

---

### 1.2.6.3 资源硬边界
- **抛出失败**：无空闲 PP 且迁移超时 → 返回“内存不足”
- **禁止跨 PM 分配**

---

## 1.2.7 调试与可观测性

### 1.2.7.1 PP 层接口
- `pp_status(pp_id)`
- `pp_snapshot(gpa_range)`

---

### 1.2.7.2 VP 层接口
- `vp_info(tsid, lva)`
- `vp_trace_start/stop(tsid, lva)`

---

### 1.2.7.3 AS/Node 层接口
- `ts_layout(tsid)`
- `ctrn_refs(ctrn_lva)`
- `node_scope(node_lva)`

---

### 1.2.7.4 全局接口
- `dpu_message_log(pm_id)`
- `scheduler_decision_log(core_id)`
- `chain_bus_stats()`

---

### 1.2.7.5 安全验证
- `check_lva_overflow(tsid)`
- `verify_vp_consistency(gva)`

---

## 1.2.8 硬件协同与兼容性

### 1.2.8.1 DPU 角色
- GVA ↔ LVA 映射
- 一致性协议执行引擎
- 故障检测与恢复代理

### 1.2.8.2 CPU 兼容
- 页表格式兼容 x86/ARM（PTE 扩展 OS 位）
- TLB 管理：同子树不切换 CR3
- 缺页异常：由调度器处理

### 1.2.8.3 显式协同模型
- **DPU 指令由应用主动触发**，提供相关编译器扩展或插桩器
- **无隐式监听**（无 MMU 旁路）

### 1.2.8.4 未来定制 CPU
- 实现对 EDSOS 的 Node-level 页表兼容
- 主动管理内存访问，适应 SDRAM 作为 L3 Cache 的连接