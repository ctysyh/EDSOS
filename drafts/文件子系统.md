# 文件子系统

> **“持久化数据是运行时数据的脱水收缩，运行时数据是持久化数据的加水泡开”**

---

## 一、L1：磁盘驱动层 —— “受控的第一次拷贝”与硬件协同

> DMA 与内存分配的协同机制

### 1. **职责再明确：谁发起、谁分配、谁执行？**

| 动作 | 发起者 | 执行者 | 数据载体 |
|------|--------|--------|----------|
| 请求物理页 | **L2 文件服务组件** | 内存子系统 | VP（虚拟页） |
| 构造 CTRN | **L2** | 内存子系统（通过 `edsos_ct_new`） | CTRN（含 GVA） |
| 共享 CTRN 给驱动 | **L2** | 内存子系统（`edsos_ct_share`） | CTRN 引用 |
| 触发 DMA | **磁盘驱动 TS / DPU** | DPU 硬件 | 物理页（PPSUC） |
| 中断通知 | **DPU** | L2 文件服务组件 | 事件邮箱 |

> **关键点**：内存子系统 只负责分配和映射，**不关心数据语义或长度**，长度信息完全由 L2 提供（来自元数据或用户写入缓冲区）。

### 2. **读操作流程（L2 → L1）**

1. L2 解析路径，查得文件大小 = N 字节；
2. L2 调用 `edsos_ct_new(size=N)` → 内存子系统分配 `⌈N/4096⌉` 个 VP，返回 CTRN（GVA）；
3. L2 调用 `edsos_ct_share(ctrn_gva, driver_ts_gva)` → 驱动获得只写权限；
4. L2 向磁盘驱动 TS 发送事件：`{cmd: READ, lba: X, ct_gva: Y, offset: 0, len: N}`；
5. 驱动 TS 调用 DPU 指令：`dma_write_phys(ctrn_phys_base, lba, len)`；
6. DPU 完成 DMA → 触发中断 → 驱动通知 L2；
7. L2 构造“成品 CTRN”（可能合并多个块）→ 共享给 L3 用户 TS。

> **此时 CTRN 已“泡开”**：原始磁盘数据已进入内存子系统管理的 VP，可被 lift。

### 3. **写操作流程（L3 → L2 → L1）**

1. 用户 TS 执行 `write(fd, buf, len)` → KPN 写入 `{buf_gva, len}`；
2. L3 验证 Capability → 转发给 L2；
3. L2 调用 `edsos_ct_new(len)` → 分配空 CTRN；
4. L2 调用 `edsos_ct_share(buf_gva, ct_gva)` → 将用户数据共享给系统；
5. L2 构造写请求：`{cmd: WRITE, lba: X, ct_gva: Y, len: len}`；
6. 驱动 TS 触发 DPU：`dma_read_phys(ctrn_phys_base, lba, len)`；
7. DPU 写入磁盘 → 完成。

> 写入时的“第一次拷贝”是从 CTRN → 硬件。

---

## 二、L2：文件服务组件 —— 元数据引擎与 Linux 代码的“语义容器”

> 元数据驱动的 CTRN 构造与 Linux 代码复用

### 1. **元数据即权威：L2 是唯一知道“文件是什么”的地方**

- 每个文件服务组件（如 ext4_ts、zfs_ts）管理一块存储设备；
- 它维护完整的 **inode 表、目录树、extent 映射、日志、快照元数据**；
- **元数据本身也存储于 VP 中**，空间由内存子系统管理，但数据语义由 L2 解释；
- 多个 L2 实例的并集 = 全局文件系统视图（支持跨设备统一命名空间）。

### 2. **CTRN 构造：从原始块到结构化对象**

L2 的核心工作是 **将原始 VP 数据（来自 L1）转化为语义 CTRN**：

- **读场景**：
  - 输入：硬件原始数据 CTRN（来自 L1 DMA）
  - 处理：解析 inode → 构建 metadata CTRN + data CTRN
  - 输出：成品 CTRN（含 size、mtime、权限、数据）

- **写场景**：
  - 输入：用户数据 CTRN
  - 处理：分配新 block → 更新 extent → 写日志 → 构造新 inode CTRN
  - 输出：更新后的 metadata CTRN（用于返回或缓存）

> **所有文件数据结构和解析逻辑可直接复用 Linux 代码**：
> - `ext4_map_blocks()` → 无需修改，仅将 `struct page*` 替换为 `VP*`；
> - `jbd2_journal_commit_transaction()` → 将日志写入专用 CTRN；
> - `btrfs_snapshot_create()` → 创建新 subvolume CTRN 树；
> - 这些都是固定形式的替换，可由编译器扩展直接完成。

### 3. **请求有效性检验 ≠ 访问控制**

L2 **只验证请求格式合法性**，例如：
- “fd 是否对应一个已打开的 CTRN？”
- “offset + len 是否超出文件大小？”
- “是否尝试写只读文件？”

但 **不验证“该 TS 是否有权访问此路径”** —— 这是 L3 的职责。

> **职责分离**：L2 是“语义执行者”，L3 是“策略执行者”。

---

## 三、L3：用户 TS 接口 —— 安全硬编码与零鉴权运行时

> KPN 指令段注入、安全硬编码与 ABI 保障

### 1. **KPN 指令段注入：安全策略的“固化”**

这是 EDSOS 安全模型的**基石**：

- 在 **EDSOS Loader 加载用户 ELF 时**，根据其声明的 Capability（如 manifest 或父 TS 授权），**静态注入 KPN 指令段**；
- 该指令段包含：
  - **允许访问的路径前缀列表**（如 `/home/user`, `/tmp`）；
  - **允许调用的文件服务功能**（如 `enable_snapshot`, `enable_encryption`）；
  - **目标文件服务 TS 的 ID 列表**（如 ext4_ts_id = 0x1234）；
- **注入后，整个用户 TS 所有节点的指令段设为只读**（硬件页表保护）；
- **用户代码无法修改 KPN 指令段，也无法生成非法 syscall**。

> **安全不是“检查”，而是“不可能”**。

### 2. **ABI 保障：编译器与 Loader 的协同**

- **编译器扩展**：仅生成 **EDSOS ABI 调用桩**（如 `call edsos_open`），**不展开内联调度逻辑**；
- **EDSOS Loader**：
  - 解析 ELF 中的 syscall 表；
  - 验证每个 syscall 是否在合法列表中；
  - 若尝试直接写 `mov rax, 0x101; syscall`（伪造 open），则：
    - **默认模式**：拒绝加载；
    - **兼容弱安全模式**：拦截并替换为 KPN 调用（性能损失，仅用于 legacy 二进制）。
- **结果**：用户 TS **永远无法执行未授权的指令**，无需运行时鉴权。

### 3. **syscall 的统一模型**

不仅文件子系统，**所有 syscall**（网络、IPC、注册与设置等）都遵循相同模式：
- syscall = KPN 写入 + 事件触发；
- 权限 = 指令段硬编码的 Capability；
- 实现 = 对应服务 TS（网络_ts、time_ts 等）；
- **无 ring0，无 trap，无上下文切换**。

> **零鉴权开销**：安全检查在加载时完成，运行时只有结构协作。

---

## 四、总结：EDSOS 文件子系统的范式优势

| 维度 | 传统 OS | EDSOS |
|------|--------|-------|
| **安全模型** | 运行时鉴权（DAC/MAC） | 加载时固化（Capability + 指令段只读） |
| **性能** | 多次拷贝（user ↔ kernel ↔ disk） | 一次拷贝（disk ↔ VP），共享即访问 |
| **可组合性** | 单一 VFS，难以替换 | 多文件服务 TS 并存 |
| **隔离性** | namespace + cgroup | TS 作用域 + Capability + CTRN |
| **代码复用** | 难（内核耦合） | 易（Linux 代码作为 TS 服务运行） |
| **分布式** | 需额外层（NFS, Ceph） | 原生支持（GVA 跨机，DPU 迁移） |

**EDSOS 文件子系统的三层架构，本质上是将“文件系统”从“内核特权”转化为“结构化服务”，并将安全、性能、兼容性统一于“TS + CTRN + Capability”这一核心范式之中**。