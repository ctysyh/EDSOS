# 网络子系统的发送流程

---

## 一、整体架构回顾（关键节点角色）

| 节点 | 所属 TS | 类型 | 作用 |
|------|--------|------|------|
| `user_buf_node` | `/user_proc/app` | **用户数据节点** | 用户 `send()` 的源缓冲区 |
| `user_kpn` | `/user_proc/sysagent/kpn` | **用户 KPN（CTRN）** | syscall 入口，桥接用户与内核 |
| `socket_send_node` | `/net/sockets/sock_0xABCD/send_q/0x88` | **发送队列项（CTRN）** | 待发送的应用数据 |
| `skb_node` | `/net/ipv4/tx/0x77` | **协议栈构造的 sk_buff 节点** | 封装 L2/L3/L4 头 + payload |
| `driver_tx_node` | `/drivers/net/e1000/tx_ring/0x66` | **驱动发送节点** | 最终写入网卡 DMA ring |

> 所有数据传递均为 **CTRN 共享挂载**，**无内存拷贝**。

---

## 二、详细流程：从用户 `send()` 到网卡发送

### 阶段 1：用户调用 `send()`，KPN 接收请求

1. 用户代码调用：
   ```c
   send(sockfd, buf, len, 0);
   ```
   - `buf` 是用户 TS 中某节点的数据段（如 `0xUSER_APP_1234_0000`）；
2. EDSRT 将其重定向到 **`user_kpn`**：
   - 因 `user_kpn` 是直系祖先，用户可直接调用其 stub；
3. `user_kpn` 的指令段执行：
   - **验证 `buf` GVA 属于当前 TS**（通过 TLB 上下文检查）；
   - **验证 `sockfd` 对应的 socket capability**（如是否已 connect）；
   - **调用 `edsos_ct_share(buf_gva, GVA_of_sock_node)`** → 将用户缓冲区 divide 成新的 CTRN `socket_send_node` 共享给 socket 服务。

---

### 阶段 2：Socket 服务接收共享，创建 `socket_send_node`

4. **`sock_node`（`/net/sockets/sock_0xABCD`）被调度**（因其监听 `send_req_event`）；
5. 它从 KPN 获取到用户创建的 CTRN `socket_send_node` 将其加入发送队列（作为 `sock_node` 的子节点），作为中转缓冲；
6. 通过 event signal 唤醒 TCP 发送逻辑节点（如 `tcp_tx_worker`）或其他协议类型的处理节点。

---

### 阶段 3：协议栈封装，创建 `skb_node`

8. `tcp_tx_worker` 被调度：
   - 通过标准 EDSOS IPC 调用共享挂载的  `socket_send_node` 并读取 payload；
   - 分配 **协议头空间**（通过 `edsos_push` 创建新节点并 `edsos_merge`）；
9. 执行封装逻辑，调用重用的 Linux 协议栈函数 `ip_queue_xmit(skb_gva)`，修改 `socket_send_node` 的数据内容，处理好的节点接下来*称为* `skb_node`；
   - 若需分段（TSO），则 divide 成多个 `skb_node`；
10. 完成后，通过 event signal 唤醒驱动。

---

### 阶段 4：驱动发送，完成 DMA

11. **驱动 TX 节点**（`/drivers/net/e1000/tx_worker`）被调度；
12. 通过标准 EDSOS IPC 调用共享挂载 `skb_node`：
    - 通过 TLB 验证 GVA 合法性；
    - 读取 L2/L3/L4 头；
    - 读取 payload；
13. 写入网卡 TX ring：
    - descriptor 指向 **头物理地址 + payload 物理地址**；
14. 触发网卡发送；
15. 发送完成后：
    - 驱动 `signal(send_complete_event)`；
    - `pop(skb_node)`；
    - 通知 `sock_node` → `pop(socket_send_node)`；
    - `sock_node` 通知 `user_kpn` → `signal(send_resp_event)` 并记录已发送字节数。

---

### 阶段 5：用户 `send()` 返回

16. 用户调度器收到 `send_resp_event`；
17. 唤醒原调用节点；
18. 从 `user_kpn` 读取已发送字节数。

---

## 三、TS 作用域与可见性（对偶于 recv）

| 访问者 | 目标 | 是否可见 | 原因 |
|--------|------|--------|------|
| `sock_node` | `user_buf_node` | 是 | 显式共享 |
| 协议栈 | `user_buf_node` | 是 | 显式共享 |
| 驱动 | `user_buf_node` | 是 | 显式共享 |
| 其他用户 TS | `user_buf_node` | 否 | 无共享，作用域隔离 |

> **对偶规则**：
> - `recv()`：内核 → 用户，KPN **拉取**数据；
> - `send()`：用户 → 内核，KPN **推送**共享。

---

## 四、零拷贝与安全的双重保障

### 4.1 零拷贝路径
- 用户数据：始终在原地（`user_buf_node`）；
- 协议栈：仅构造头；
- 驱动：直接 DMA 用户物理页。

### 4.2 安全边界
- 用户不能共享内核内存。
- 数据包对其他用户完全不可见。

### 4.3 异常处理
- 若用户 TS 在发送完成前退出：
  - 调度器 `lift` 所有共享 CTRN；
  - 驱动检测到无效 GVA，丢弃包；
  - 无内存泄漏（引用计数自动回收）。

---

## 五、与 recv() 的结构对偶总结

| 维度 | `recv()` | `send()` |
|------|--------|--------|
| **起点** | 驱动 TS | 用户 TS |
| **终点** | 用户 TS | 驱动 TS |
| **CTRN 方向** | 驱动 → 服务组件 → KPN → 用户 | 用户 → KPN → 服务组件 → 驱动 |
| **KPN 角色** | 拉取代理（暴露内核数据） | 推送代理（共享用户数据） |
| **零拷贝核心** | CTRN 共享 + 映射 | CTRN 共享 + DMA |

---

## 六、结论：对称、安全、零拷贝的双向网络栈

`send()` 路径完美体现了 EDSOS 的设计哲学：

- **结构对称性**：收发路径使用相同的 CTRN 共享机制；
- **最小信任**：每个 TS 仅访问被显式授权的内存；
- **最大复用**：Linux 协议栈代码几乎无需修改；
- **极致性能**：全程零拷贝，仅 GVA 传递与映射。

这不仅是一个网络子系统，更是 **EDSOS 如何将传统 I/O 模型重构为结构化协作范式** 的典范。下一步可考虑 **批量 send/recv 优化**（如 `io_uring` 风格的 CTRN 批处理），进一步提升吞吐。