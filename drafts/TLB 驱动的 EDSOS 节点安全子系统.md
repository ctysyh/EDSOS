# TLB 驱动的 EDSOS 节点安全子系统

---

## 总体概述

> **核心思想**：  
> **L1 TLB = 当前节点的“安全缓存”**  
> **L2 TLB = 当前节点 + 直系祖先的“作用域缓存”**  
> **Page Fault = 作用域外访问的唯一信号**  
> **调度器 = TLB 内容的主动管理者**

该子系统实现了：
- **性能确定性**：当前节点 100% L1 TLB 命中
- **安全精准性**：任何越界访问（几乎）必触发 Page Fault，且不可能做到蓄意的精确漏网（概率和单次随机猜对密钥一样低）
- **语义对齐**：硬件缓存行为 ≡ TS 树作用域语义
- **零额外开销**：合法路径无检查、无 trap、无 metadata 访问

---

## 详细处理过程：OS、硬件、用户节点三方协作

### 阶段 1：**TS 节点创建（编译时 + 加载时）**

| 角色 | 行为 | 目的 |
|------|------|------|
| **编译器** | - 将函数/数据打包为节点<br>- 静态检查节点大小 ≤ L1_DTLB/2 × PAGE_SIZE（如 ≤128KB）<br>- 生成 `.edsos_node_meta`（含 vbase, size, ancestor_path） | 确保节点满足 TLB 安全约束 |
| **EDSOS 加载器** | - 为每个节点分配连续虚拟地址<br>- 构建全局 `node_range_map`（地址 → 节点/GPN）<br>- 验证祖先链合法性 | 建立地址与作用域的权威映射 |

> **用户节点获得**：一个**大小受控、地址连续、作用域明确**的执行单元。

---

### 阶段 2：**调度器选择下一节点（运行时）**

| 角色 | 行为 | 目的 |
|------|------|------|
| **EDSOS 调度器** | 1. 从就绪队列选择候选节点<br>2. **优先选择与当前 CPU 活跃祖先路径匹配的节点**（路径亲和调度）<br>3. 若跨树或路径变化，标记需 TLB 预热 | 最大化 L2 TLB 重用，减少刷新 |
| **调度器（预热阶段）** | 在**目标 CPU 核心**上执行：<br>```c<br>for (page in current_node.hot_pages) touch(page);<br>for (page in ancestor_chain.hot_pages) touch(page);<br>``` | **主动填充 L1 + L2 TLB** |

> **硬件行为**：
> - `touch(addr)` → 触发 page walk
> - L1 TLB miss → 自动从页表加载 → 填充 L1
> - 同时填充 L2 TLB（容量大，可容纳整条祖先链）

> **用户节点获得**：**即将运行时，其自身 + 所有祖先数据已在 TLB 中就绪**。

---

### 阶段 3：**节点执行（正常路径）**

| 角色 | 行为 | 效果 |
|------|------|------|
| **用户 TS 节点** | 执行指令，访问：<br>- 自身数据（如局部变量、代码）<br>- 祖先数据（如全局配置、父上下文） | |
| **CPU 硬件** | - 访问自身数据 → **L1 TLB hit**（因预加载）<br>- 访问祖先数据 → **L1 miss → L2 TLB hit → 自动回填 L1（可选）** | **无 Page Fault，无性能抖动** |

> **用户节点获得**：
> - **确定性低延迟**（L1 hit）
> - **无缝祖先访问**（L2 hit）
> - **完全透明的作用域访问**（无需显式 Capability）

---

### 阶段 4：**非法访问发生（异常路径）**

| 场景 | 硬件行为 | OS 行为 | 结果 |
|------|--------|--------|------|
| **越界访问**（如访问兄弟节点） | L1 miss → L2 miss → **触发 Page Fault** | 1. 获取 fault_addr<br>2. 查询 `node_range_map`<br>3. 检查：`addr ∈ (current_node ∪ ancestors)`？<br>4. 若否 → **立即终止节点** | 安全拦截 |
| **访问未映射地址** | 同上 | 同上，检查失败 | 拦截 |
| **合法但未预热**（如栈增长） | Page Fault | 检查是否为合法栈扩展 → 是则分配页 | 容错 |

> **关键设计**：  
> **Page Fault 不直接等于非法，但“在 TLB-safe 节点中发生 Page Fault”极大概率是非法**，  
> **而 OS 的检查开销可接受（因极少发生）**。

---

### 阶段 5：**节点切换 / 回收**

| 事件 | OS 行为 | 硬件影响 |
|------|--------|--------|
| **节点 pop（正常退出）** | - 释放虚拟地址<br>- **遍历该节点所有页，执行 `invlpg`**<br>- 从 `node_range_map` 移除 | 清理 TLB 中残留条目，防止 UAF |
| **核间迁移** | - 选择共享 L2 cache 的目标核（如 Intel 同 CCX）<br>- 在目标核预热 TLB | 最小化 L2 TLB 冷启动 |

> **TS 栈语义保证**：**祖先节点不可能在子节点运行时被回收**，因此无需担心“祖先页失效”。

---

## 最终交付：用户 TS 节点获得什么？

| 维度 | 传统 OS（线程） | EDSOS TS 节点（本子系统） |
|------|----------------|--------------------------|
| **执行单元粒度** | 线程（MB 级栈） | 节点（KB 级，高内聚） |
| **切换开销** | 高（寄存器 + 栈 + cache 污染） | 极低（仅 GPN 切换 + TLB 已预热） |
| **内存安全** | 无（依赖 ASLR/Canary） | **硬件级作用域隔离** |
| **性能确定性** | 低（TLB/cache 随机 miss） | **高（L1 100% hit，L2 高命中）** |
| **开发者心智模型** | “整个地址空间可用” | “仅自己 + 祖先可用”（更安全） |

---

## 补充

### 提示与额外设计
- 可以通过监听CPU的L1_DTLB_MISS（类似物）事件来并行地侧监督。
- 利用Hardware Prefetcher来减少需要执行的主动预取操作量。
- 此方案对TS节点的大小有严格限制。

---

### 边界情况
- 对于允许的L1 TLB miss而L2 TLB hit访问（访问直系祖节点），哪怕会导致L1 TLB更新而且更新会导致原L1 TLB条目被彻底挤出，但被挤掉的L1 TLB条目必然也在L2 TLB里，这只会带来轻微的性能抖动，不会破坏安全模型。

---

## 结语：这不是对抗硬件，而是与硬件共舞

我们没有要求 CPU 增加“节点权限位”，没有引入新指令，没有牺牲兼容性。  
我们只是：

1. **理解了 TLB 的容量与层次**
2. **约束了节点的大小与作用域**
3. **让调度器成为 TLB 的“编舞者”**
4. **将 Page Fault 从“错误”变为“安全哨兵”**

> **结果**：  
> **在现有 x86/ARM 硬件上，实现了一个比传统 OS 更安全、更确定、更高效的任务模型**。


---


> 附录：方案诞生的思维过程

## 思维演进路线图：从约束到创造

### 第一步：**定义问题本质 —— “TS 节点需要什么？”**

> **核心问题**：  
> EDSOS 的 TS（Tree-Stacked）节点是比线程更细粒度的执行单元，  
> 它们需要：
> - **极低的切换开销**
> - **强内存隔离（防越界）**
> - **可组合的作用域（可访问祖先）**
> - **确定性性能（无缓存抖动）**

传统 OS 用页表 + 进程/线程模型解决，但开销大、粒度粗、作用域模糊。

**洞察 1**：  
> **TS 树 ≈ 进程，TS 节点 ≈ 超轻量协程**，但需要更精细的地址空间控制。

---

### 第二步：**观察硬件现实 —— “TLB 到底是什么？”**

> **关键观察**：  
> 现代 CPU 的 TLB 不是“黑盒”，而是：
> - **有限容量**（L1 ≈ 64 条）
> - **分层结构**（L1 快但小，L2 慢但大）
> - **行为可预测**（miss → walk → fill）

传统系统把 TLB miss 视为“性能噪声”，试图用大页、PCID 缓解。

**洞察 2**：  
> **如果我能控制 TLB 中的内容，TLB 就不再是噪声源，而是“作用域缓存”**。

---

### 第三步：**逆向思维 —— “能否让 TLB miss = 非法访问？”**

> **灵光一现**：  
> 如果强制每个节点的活跃页数 ≤ L1 TLB 容量的一半，  
> 并在切换前预加载，那么：
> - **合法访问 → 必 TLB hit**
> - **TLB miss → 必非法**

这相当于**用硬件缓存实现零开销边界检查**！

**洞察 3**：  
> **安全不必总是“主动检查”，也可以是“被动验证”** —— 让非法行为自然触发异常。

---

### 第四步：**处理现实复杂性 —— “祖先访问怎么办？”**

> **新问题浮现**：  
> TS 节点必须能访问祖先数据，但这会超出 L1 TLB 容量。

> **解决方案**：  
> - L1 TLB：只放当前节点（确保 100% hit）
> - L2 TLB：放“当前节点 + 所有祖先”（利用其大容量）
> - 调度器：尽量让共享祖先的节点在同一个核上运行（提升 L2 亲和性）

**洞察 4**：  
> **硬件层次结构 ≈ 语言作用域层次**：  
> L1 = 局部作用域，L2 = 闭包/继承链。

---

### 第五步：**验证边界与风险 —— “L1 替换会破坏一切吗？”**

> **深度质疑**：  
> L1 TLB 替换是随机的，会不会把合法页踢出，导致误判？

> **微架构深挖**：  
> - L1/L2 TLB 是 non-inclusive
> - L1 替换不影响 L2
> - L1 miss + L2 hit 是合法路径，不会触发 Page Fault

**洞察 5**：  
> **不必追求“完美缓存”，只需确保“合法路径永不 fault”**。  
> 安全边界由 OS 的 fault handler 最终兜底，而非依赖缓存状态。

---

### 第六步：**整合成系统 —— “OS、硬件、用户如何协作？”**

> **最终拼图**：
> - **编译器**：约束节点大小，生成元数据
> - **调度器**：路径亲和 + TLB 预热
> - **硬件**：自动 TLB fill，miss 触发 fault
> - **OS**：fault 时检查地址是否在 {self ∪ ancestors}
> - **用户**：获得确定性、安全、低开销的执行环境

**洞察 6**：  
> **伟大的系统设计，是让每一层都做它最擅长的事**：
> - 硬件做缓存
> - 编译器做静态约束
> - 调度器做动态优化
> - OS 做最终仲裁

---

## 启示：系统设计的六大原则

1. **从语义出发，而非从实现出发**  
   → 先明确“TS 节点是什么”，再想“怎么实现”。

2. **拥抱硬件，而非对抗硬件**  
   → TLB 不是敌人，是未被开发的资源。

3. **用约束换确定性**  
   → 限制节点大小，换来 100% TLB 命中和安全验证。

4. **分层对齐**  
   → 将语言作用域（TS 树）与硬件缓存层次（L1/L2 TLB）对齐。

5. **异常即信号**  
   → 不主动检查，而是让非法行为自然暴露。

6. **兜底思维**  
   → 即使缓存行为不确定，OS 的 fault handler 仍是安全最后防线。

---

## 结语：这不是“发明”，而是“发现”

我们并没有发明新硬件、新指令、新安全模型。  
我们只是：

- **看清了 TLB 的真实行为**
- **理解了 TS 的语义需求**
- **让两者在调度器的指挥下共舞**

> **最好的系统设计，往往不是“加法”，而是“重新诠释”**。  
> 在别人看到“性能噪声”的地方，我们看到了“安全信号”；  
> 在别人抱怨“缓存不可控”的地方，我们构建了“确定性执行”。